{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1980be4e-7bcb-43bb-b97a-401f101fe5df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install sagemaker -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618626b6-5263-49c9-8d17-57015f0989eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token XXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac41d34f-f530-4b1c-b3f1-c2934db75c28",
   "metadata": {},
   "source": [
    "## SageMaker Distributed Training using HuggingFace Managed Container Image\n",
    "\n",
    "### üöÄ SageMaker Warm Pools Overview\n",
    "\n",
    "SageMaker warm pools help reduce training job startup time by keeping instances \"warm\" and ready for reuse.\n",
    "\n",
    "### ‚ö†Ô∏è **IMPORTANT: Warm Pool Availability**\n",
    "\n",
    "**The warm pool is ONLY available AFTER your current job completes (succeeds or fails).**\n",
    "\n",
    "- ‚ùå **First job**: Always experiences cold start\n",
    "- ‚ùå **Concurrent jobs**: Cannot share warm pools - each gets cold start\n",
    "- ‚úÖ **Sequential jobs**: Second job onwards can use warm pool\n",
    "\n",
    "> **Don't create multiple jobs expecting immediate warm pool benefits!** The warm instance becomes available only after the current job finishes.\n",
    "\n",
    "### ‚ö° How Warm Pools Work\n",
    "\n",
    "**First Training Job (Cold Start):**\n",
    "- Instance provisioning: ~3-10 minutes\n",
    "- Container image download: ~2-3 minutes  \n",
    "- Environment setup: ~1-2 minutes\n",
    "- **Total overhead: 6-15 minutes**\n",
    "\n",
    "**Subsequent Jobs (Warm Start):**\n",
    "- Instance reuse: ~5 seconds\n",
    "- Cached container: ~0 seconds\n",
    "- Environment setup: ~15 seconds\n",
    "- **Total overhead: ~20 seconds** ‚ö°\n",
    "\n",
    "### üéØ Warm Pool Requirements\n",
    "\n",
    "For an instance to be reused, the following must **exactly match**:\n",
    "- ‚úÖ **Instance type** (e.g., `ml.g5.12xlarge`)\n",
    "- ‚úÖ **Instance count** (e.g., `1`)\n",
    "- ‚úÖ **Volume size** (e.g., `100 GB`)\n",
    "- ‚úÖ **Network configuration** (VPC, subnets, security groups)\n",
    "\n",
    "### üèóÔ∏è Container Image Caching\n",
    "\n",
    "**SageMaker Managed Images** (like HuggingFace):\n",
    "- ‚úÖ **Cached automatically** - no re-download needed\n",
    "- ‚úÖ **Faster startup** - image already available locally\n",
    "\n",
    "**Custom Docker Images**:\n",
    "- ‚ùå **Not cached** - must be pulled each time\n",
    "- ‚è±Ô∏è **Slower startup** - full image download required\n",
    "\n",
    "### üí° Best Practices\n",
    "\n",
    "1. **Configure warm pools** with `keep_alive_period_in_seconds`\n",
    "2. **Use consistent configurations** across related training jobs\n",
    "3. **Leverage SageMaker managed images** when possible\n",
    "4. **Plan job sequences** to maximize warm pool utilization\n",
    "\n",
    "### üîÑ Typical Workflow for Warm Pool Benefits\n",
    "\n",
    "```python\n",
    "# Job 1: Cold start (6-15 min overhead)\n",
    "job_1 = huggingface_estimator.fit(wait=True)  # First job - always cold start\n",
    "job_1.wait()  # Wait for completion\n",
    "\n",
    "# Job 2: Warm start (~20 sec overhead) ‚ö°\n",
    "job_2 = huggingface_estimator.fit(wait=True)  # Now uses warm pool!\n",
    "job_2.wait()\n",
    "\n",
    "# Job 3: Warm start (~20 sec overhead) ‚ö°\n",
    "job_3 = huggingface_estimator.fit(wait=True)  # Continues using warm pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48e32106-66fb-4922-a8a3-088d2e0664d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T20:58:02.153452Z",
     "iopub.status.busy": "2025-09-09T20:58:02.153065Z",
     "iopub.status.idle": "2025-09-09T20:58:02.217292Z",
     "shell.execute_reply": "2025-09-09T20:58:02.216409Z",
     "shell.execute_reply.started": "2025-09-09T20:58:02.153425Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "import time\n",
    "from sagemaker.tuner import ContinuousParameter, IntegerParameter, HyperparameterTuner, CategoricalParameter\n",
    "\n",
    "# Define Training Job Name with timestamp for uniqueness\n",
    "job_name = f'qwen-dpo-training-{int(time.time())}'\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Training metrics for SageMaker SDK to track\n",
    "metric_definitions = [\n",
    "\n",
    "    # Average training loss (logged at end)\n",
    "    {\n",
    "        \"Name\": \"train:loss\",\n",
    "        \"Regex\": r\"'train_loss':\\s*([0-9\\.]+)\"  # As printed out to stdout\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='dpo_train.py',              # train script\n",
    "    source_dir='code-hpo',                   # directory with all training files\n",
    "    instance_type='ml.g5.2xlarge',          # GPU instance\n",
    "    instance_count=1,                        # instance count\n",
    "    base_job_name=job_name,                  # base name for training job\n",
    "    role=role,                               # IAM role for AWS resources\n",
    "    volume_size=30,                         # EBS volume size in GB\n",
    "    transformers_version='4.49.0',           # transformers version\n",
    "    pytorch_version='2.5.1',                 # pytorch version\n",
    "    py_version='py311',                      # python version\n",
    "    disable_output_compression=True,         # faster training completion\n",
    "    output_path=\"s3://BUCKET NAME/PREFIX/\",      # S3 location where training output is stored\n",
    "\n",
    "    # Environment variables\n",
    "    environment={\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\",  # cache location\n",
    "        \"HF_TOKEN\": HfFolder.get_token(),        # Hf token id\n",
    "        \"TOKENIZERS_PARALLELISM\": \"false\",        # Prevent tokenizer warnings\n",
    "        \"PIP_CACHE_DIR\": \"/opt/ml/sagemaker/warmpoolcache/pip\"  # Persisitent dir across warm pool runs used to cache pip installs\n",
    "    },\n",
    "\n",
    "    # Hyperparameters - these override values in dpo_config.yaml\n",
    "    hyperparameters={\n",
    "        # Config file name\n",
    "        \"config_path\": \"dpo_config.yaml\",  # As saved in the `source_dir` parameter above\n",
    "        \n",
    "        # LoRA parameters\n",
    "        \"lora_r\": \"32\",  \n",
    "        \"lora_alpha\": 64,  \n",
    "        \"lora_dropout\": 0.05, \n",
    "\n",
    "        # Training parameters\n",
    "        \"learning_rate\": 1e-4,  \n",
    "        \"per_device_train_batch_size\": 4,  \n",
    "        \"num_train_epochs\": 3,  \n",
    "        \"gradient_accumulation_steps\": 2,  \n",
    "\n",
    "        # DPO parameters\n",
    "        \"dpo_beta\": 0.2,\n",
    "\n",
    "        # Data parameters\n",
    "        \"num_samples\": 1000, \n",
    "        \"data_processor_type\": \"anthropic_hh_rlhf\",\n",
    "    },\n",
    "\n",
    "    # Add metric definitions to capture to sagemaker default metrics chart\n",
    "    metric_definitions=metric_definitions,\n",
    "    \n",
    "    distribution={\n",
    "        \"torch_distributed\": {  # this uses torchrun command on your script with the following arguments\n",
    "            \"enabled\": True,\n",
    "        }\n",
    "    },\n",
    "    keep_alive_period_in_seconds=1800,       # 30 minutes warm pool, maximum of 60 mins\n",
    ")\n",
    "\n",
    "# No need to call the .fit method to start the training job since we are using SageMaker automatic model tuning below\n",
    "# huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9215a524-e0c3-4e8b-b2b5-f81a7f2d53d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T20:28:56.317026Z",
     "iopub.status.busy": "2025-09-09T20:28:56.316479Z",
     "iopub.status.idle": "2025-09-09T20:28:56.323662Z",
     "shell.execute_reply": "2025-09-09T20:28:56.322788Z",
     "shell.execute_reply.started": "2025-09-09T20:28:56.316994Z"
    }
   },
   "source": [
    "## üéØ Hyperparameter Optimization & Model Fine-tuning\n",
    "\n",
    "This section leverages **Amazon SageMaker's powerful ML infrastructure** to perform automated hyperparameter optimization (HPO) for fine-tuning our OSS models.\n",
    "\n",
    "**SageMaker Automatic Model Tuning (AMT)** eliminates manual hyperparameter guesswork:\n",
    "- üî¨ **Optimization strategies** select from `bayesian` `grid` `random` and `hyperband` strategies to explore hyperparameter space\n",
    "- üìä **Multi-metric tracking** monitors training progress in real-time\n",
    "- ‚ö° **Parallel experimentation** runs multiple configurations simultaneously\n",
    "- üí∞ **Cost-effective** by stopping underperforming jobs early\n",
    "- **Framework Agnostic** can work with any ML framework and/or custom images\n",
    "\n",
    "### üîç Automated Hyperparameter Search\n",
    "\n",
    "**SageMaker AMT** intelligently explores the parameter space:\n",
    "\n",
    "Hyperparameter Tuning ensures we find optimal hyperparameters efficiently while maintaining reproducibility and cost control for finetuning job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23217f2f-b506-4939-bdd7-b17db99e6ea1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T20:58:16.739116Z",
     "iopub.status.busy": "2025-09-09T20:58:16.738455Z",
     "iopub.status.idle": "2025-09-09T20:58:17.499055Z",
     "shell.execute_reply": "2025-09-09T20:58:17.498138Z",
     "shell.execute_reply.started": "2025-09-09T20:58:16.739080Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating hyperparameter tuning job with name: dpo-qwen-sft-250909-2058\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_ranges_qwen = {\n",
    "    \"lora_r\": CategoricalParameter([\"32\", \"64\"]),\n",
    "    \"lora_dropout\": ContinuousParameter(0, 0.1),\n",
    "    \"learning_rate\": ContinuousParameter(0.000005, 0.0002),\n",
    "    \"per_device_train_batch_size\": IntegerParameter(1,4)\n",
    "}\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "        base_tuning_job_name=\"dpo-qwen-sft\",\n",
    "        estimator=huggingface_estimator,\n",
    "        objective_metric_name=\"train:loss\",  # Name of a metric defined in the Estimator class above to track for hpo\n",
    "        hyperparameter_ranges=hyperparameter_ranges_qwen,\n",
    "        metric_definitions=metric_definitions,\n",
    "        max_jobs=4,  # Total number of jobs (candidates)\n",
    "        max_parallel_jobs=2,  # Number of jobs (candidates) to run in parallel\n",
    "        objective_type=\"Minimize\",  # Eithe `Maximize` or `Minimize` the objective metric\n",
    "        strategy=\"Bayesian\"  # Search strategy can be any of `Random` `Grid` `Hyperband`\n",
    "    )\n",
    "\n",
    "tuner.fit(wait=False  # Toggle to False so that the cell does not wait for the job to finish (default is True) \n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d297a12f-7155-4b60-bb8e-6a5ac8a61ce2",
   "metadata": {},
   "source": [
    "### You can check the status of the Tuning Job from the sagemaker console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d99a434-c9f1-47ea-8fd2-d2f7d10569b8",
   "metadata": {},
   "source": [
    "<img src=\"hpo-dpo.PNG\" width=\"1000\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
