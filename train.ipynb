{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618626b6-5263-49c9-8d17-57015f0989eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea22770-38bd-4fa3-b3f4-5c1a99d531ae",
   "metadata": {},
   "source": [
    "## SageMaker Distributed Training using HuggingFace Managed Container Image\n",
    "\n",
    "### 🚀 SageMaker Warm Pools Overview\n",
    "\n",
    "SageMaker warm pools help reduce training job startup time by keeping instances \"warm\" and ready for reuse.\n",
    "\n",
    "### ⚠️ **IMPORTANT: Warm Pool Availability**\n",
    "\n",
    "**The warm pool is ONLY available AFTER your current job completes (succeeds or fails).**\n",
    "\n",
    "- ❌ **First job**: Always experiences cold start\n",
    "- ❌ **Concurrent jobs**: Cannot share warm pools - each gets cold start\n",
    "- ✅ **Sequential jobs**: Second job onwards can use warm pool\n",
    "\n",
    "> **Don't create multiple jobs expecting immediate warm pool benefits!** The warm instance becomes available only after the current job finishes.\n",
    "\n",
    "### ⚡ How Warm Pools Work\n",
    "\n",
    "**First Training Job (Cold Start):**\n",
    "- Instance provisioning: ~3-10 minutes\n",
    "- Container image download: ~2-3 minutes  \n",
    "- Environment setup: ~1-2 minutes\n",
    "- **Total overhead: 6-15 minutes**\n",
    "\n",
    "**Subsequent Jobs (Warm Start):**\n",
    "- Instance reuse: ~5 seconds\n",
    "- Cached container: ~0 seconds\n",
    "- Environment setup: ~15 seconds\n",
    "- **Total overhead: ~20 seconds** ⚡\n",
    "\n",
    "### 🎯 Warm Pool Requirements\n",
    "\n",
    "For an instance to be reused, the following must **exactly match**:\n",
    "- ✅ **Instance type** (e.g., `ml.g5.12xlarge`)\n",
    "- ✅ **Instance count** (e.g., `1`)\n",
    "- ✅ **Volume size** (e.g., `100 GB`)\n",
    "- ✅ **Network configuration** (VPC, subnets, security groups)\n",
    "\n",
    "### 🏗️ Container Image Caching\n",
    "\n",
    "**SageMaker Managed Images** (like HuggingFace):\n",
    "- ✅ **Cached automatically** - no re-download needed\n",
    "- ✅ **Faster startup** - image already available locally\n",
    "\n",
    "**Custom Docker Images**:\n",
    "- ❌ **Not cached** - must be pulled each time\n",
    "- ⏱️ **Slower startup** - full image download required\n",
    "\n",
    "### 🔄 Typical Workflow for Warm Pool Benefits\n",
    "\n",
    "```python\n",
    "# Job 1: Cold start (6-15 min overhead)\n",
    "job_1 = huggingface_estimator.fit(wait=True)  # First job - always cold start\n",
    "job_1.wait()  # Wait for completion\n",
    "\n",
    "# Job 2: Warm start (~20 sec overhead) ⚡\n",
    "job_2 = huggingface_estimator.fit(wait=True)  # Now uses warm pool!\n",
    "job_2.wait()\n",
    "\n",
    "# Job 3: Warm start (~20 sec overhead) ⚡\n",
    "job_3 = huggingface_estimator.fit(wait=True)  # Continues using warm pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "48e32106-66fb-4922-a8a3-088d2e0664d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-09T17:54:17.156986Z",
     "iopub.status.busy": "2025-09-09T17:54:17.156689Z",
     "iopub.status.idle": "2025-09-09T18:31:33.714114Z",
     "shell.execute_reply": "2025-09-09T18:31:33.713405Z",
     "shell.execute_reply.started": "2025-09-09T17:54:17.156963Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.telemetry.telemetry_logging:SageMaker Python SDK will collect telemetry to help us better understand our user's needs, diagnose issues, and deliver additional features.\n",
      "To opt out of telemetry, please disable via TelemetryOptOut parameter in SDK defaults config. For more information, refer to https://sagemaker.readthedocs.io/en/stable/overview.html#configuring-and-using-defaults-with-the-sagemaker-python-sdk.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: qwen-dpo-training-1757440457-2025-09-09-17-54-17-220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-09 17:54:20 Starting - Found matching resource for reuse..\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35mCUDA compat package should be installed for NVIDIA driver smaller than 550.163.01\u001b[0m\n",
      "\u001b[35mCurrent installed NVIDIA driver version is 570.172.08\u001b[0m\n",
      "\u001b[35mSkipping CUDA compat setup as newer NVIDIA driver is installed\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:30,946 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:30,984 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:30,993 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:30,995 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:30,995 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:32,449 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[35mCollecting trl==0.22.2 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[35mUsing cached trl-0.22.2-py3-none-any.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[35mCollecting peft==0.17.1 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mUsing cached peft-0.17.1-py3-none-any.whl.metadata (14 kB)\u001b[0m\n",
      "\u001b[35mCollecting bitsandbytes==0.47.0 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[35mUsing cached bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[35mCollecting transformers==4.55.2 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[35mUsing cached transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34mCUDA compat package should be installed for NVIDIA driver smaller than 550.163.01\u001b[0m\n",
      "\u001b[34mCurrent installed NVIDIA driver version is 570.172.08\u001b[0m\n",
      "\u001b[34mSkipping CUDA compat setup as newer NVIDIA driver is installed\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:30,936 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:30,973 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:30,983 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:30,985 sagemaker_pytorch_container.training INFO     Invoking TorchDistributed...\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:30,985 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:32,447 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting trl==0.22.2 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mUsing cached trl-0.22.2-py3-none-any.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft==0.17.1 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mUsing cached peft-0.17.1-py3-none-any.whl.metadata (14 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes==0.47.0 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mUsing cached bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\u001b[0m\n",
      "\u001b[34mCollecting transformers==4.55.2 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mUsing cached transformers-4.55.2-py3-none-any.whl.metadata (41 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (1.26.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (2.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (4.66.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (14.0.0)\u001b[0m\n",
      "\u001b[34mCollecting deepspeed (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mUsing cached deepspeed-0.17.5-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: accelerate>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from trl==0.22.2->-r requirements.txt (line 3)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from trl==0.22.2->-r requirements.txt (line 3)) (3.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from peft==0.17.1->-r requirements.txt (line 4)) (24.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from peft==0.17.1->-r requirements.txt (line 4)) (7.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.11/site-packages (from peft==0.17.1->-r requirements.txt (line 4)) (2.5.1+cu124)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from peft==0.17.1->-r requirements.txt (line 4)) (0.5.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface_hub>=0.25.0 in /opt/conda/lib/python3.11/site-packages (from peft==0.17.1->-r requirements.txt (line 4)) (0.29.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers==4.55.2->-r requirements.txt (line 6)) (3.18.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface_hub>=0.25.0 (from peft==0.17.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mUsing cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers==4.55.2->-r requirements.txt (line 6)) (2024.11.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers==4.55.2->-r requirements.txt (line 6)) (2.32.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers==4.55.2->-r requirements.txt (line 6)) (0.21.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft==0.17.1->-r requirements.txt (line 4)) (2024.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft==0.17.1->-r requirements.txt (line 4)) (4.14.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft==0.17.1->-r requirements.txt (line 4)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.1->-r requirements.txt (line 4)) (3.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.1->-r requirements.txt (line 4)) (3.1.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.1->-r requirements.txt (line 4)) (1.13.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.17.1->-r requirements.txt (line 4)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 10)) (2.9.0.post0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 10)) (2025.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 10)) (2025.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->-r requirements.txt (line 13)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->-r requirements.txt (line 13)) (2.19.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.11/site-packages (from deepspeed->-r requirements.txt (line 16)) (0.8.1)\u001b[0m\n",
      "\u001b[34mCollecting hjson (from deepspeed->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mUsing cached hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: msgpack in /opt/conda/lib/python3.11/site-packages (from deepspeed->-r requirements.txt (line 16)) (1.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.11/site-packages (from deepspeed->-r requirements.txt (line 16)) (1.11.1.3)\u001b[0m\n",
      "\u001b[34mCollecting py-cpuinfo (from deepspeed->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mUsing cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from deepspeed->-r requirements.txt (line 16)) (2.11.7)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-ml-py (from deepspeed->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mUsing cached nvidia_ml_py-13.580.65-py3-none-any.whl.metadata (9.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (21.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (0.3.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (3.12.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (2.6.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (1.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (25.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (1.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (6.6.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (1.20.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (3.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 13)) (0.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 16)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 16)) (2.33.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 16)) (0.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 10)) (1.17.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.55.2->-r requirements.txt (line 6)) (3.4.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.55.2->-r requirements.txt (line 6)) (2.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.55.2->-r requirements.txt (line 6)) (2025.7.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft==0.17.1->-r requirements.txt (line 4)) (3.0.2)\u001b[0m\n",
      "\u001b[34mUsing cached trl-0.22.2-py3-none-any.whl (544 kB)\u001b[0m\n",
      "\u001b[34mUsing cached peft-0.17.1-py3-none-any.whl (504 kB)\u001b[0m\n",
      "\u001b[34mUsing cached bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\u001b[0m\n",
      "\u001b[34mUsing cached transformers-4.55.2-py3-none-any.whl (11.3 MB)\u001b[0m\n",
      "\u001b[34mUsing cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\u001b[0m\n",
      "\u001b[34mUsing cached hjson-3.1.0-py3-none-any.whl (54 kB)\u001b[0m\n",
      "\u001b[34mUsing cached nvidia_ml_py-13.580.65-py3-none-any.whl (48 kB)\u001b[0m\n",
      "\u001b[34mUsing cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: py-cpuinfo, nvidia-ml-py, hjson, huggingface_hub, deepspeed, bitsandbytes, transformers, trl, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface_hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.29.1\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.29.1:\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (1.26.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (2.3.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (6.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (4.66.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: rich in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (14.0.0)\u001b[0m\n",
      "\u001b[35mCollecting deepspeed (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[35mUsing cached deepspeed-0.17.5-py3-none-any.whl\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: accelerate>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from trl==0.22.2->-r requirements.txt (line 3)) (1.4.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: datasets>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from trl==0.22.2->-r requirements.txt (line 3)) (3.3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from peft==0.17.1->-r requirements.txt (line 4)) (24.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from peft==0.17.1->-r requirements.txt (line 4)) (7.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.11/site-packages (from peft==0.17.1->-r requirements.txt (line 4)) (2.5.1+cu124)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: safetensors in /opt/conda/lib/python3.11/site-packages (from peft==0.17.1->-r requirements.txt (line 4)) (0.5.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: huggingface_hub>=0.25.0 in /opt/conda/lib/python3.11/site-packages (from peft==0.17.1->-r requirements.txt (line 4)) (0.29.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers==4.55.2->-r requirements.txt (line 6)) (3.18.0)\u001b[0m\n",
      "\u001b[35mCollecting huggingface_hub>=0.25.0 (from peft==0.17.1->-r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[35mUsing cached huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers==4.55.2->-r requirements.txt (line 6)) (2024.11.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers==4.55.2->-r requirements.txt (line 6)) (2.32.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.11/site-packages (from transformers==4.55.2->-r requirements.txt (line 6)) (0.21.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft==0.17.1->-r requirements.txt (line 4)) (2024.12.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft==0.17.1->-r requirements.txt (line 4)) (4.14.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.11/site-packages (from huggingface_hub>=0.25.0->peft==0.17.1->-r requirements.txt (line 4)) (1.1.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.1->-r requirements.txt (line 4)) (3.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.1->-r requirements.txt (line 4)) (3.1.6)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch>=1.13.0->peft==0.17.1->-r requirements.txt (line 4)) (1.13.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.17.1->-r requirements.txt (line 4)) (1.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 10)) (2.9.0.post0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 10)) (2025.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 10)) (2025.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich->-r requirements.txt (line 13)) (3.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->-r requirements.txt (line 13)) (2.19.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: einops in /opt/conda/lib/python3.11/site-packages (from deepspeed->-r requirements.txt (line 16)) (0.8.1)\u001b[0m\n",
      "\u001b[35mCollecting hjson (from deepspeed->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[35mUsing cached hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: msgpack in /opt/conda/lib/python3.11/site-packages (from deepspeed->-r requirements.txt (line 16)) (1.1.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: ninja in /opt/conda/lib/python3.11/site-packages (from deepspeed->-r requirements.txt (line 16)) (1.11.1.3)\u001b[0m\n",
      "\u001b[35mCollecting py-cpuinfo (from deepspeed->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[35mUsing cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from deepspeed->-r requirements.txt (line 16)) (2.11.7)\u001b[0m\n",
      "\u001b[35mCollecting nvidia-ml-py (from deepspeed->-r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[35mUsing cached nvidia_ml_py-13.580.65-py3-none-any.whl.metadata (9.6 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (21.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (0.3.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (3.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (0.70.16)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (3.12.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (2.6.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: aiosignal>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (1.4.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (25.3.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (1.7.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (6.6.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (0.3.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (1.20.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.11/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets>=3.0.0->trl==0.22.2->-r requirements.txt (line 3)) (3.10)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->-r requirements.txt (line 13)) (0.1.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 16)) (0.7.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 16)) (2.33.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 16)) (0.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 10)) (1.17.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.55.2->-r requirements.txt (line 6)) (3.4.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.55.2->-r requirements.txt (line 6)) (2.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers==4.55.2->-r requirements.txt (line 6)) (2025.7.14)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft==0.17.1->-r requirements.txt (line 4)) (3.0.2)\u001b[0m\n",
      "\u001b[35mUsing cached trl-0.22.2-py3-none-any.whl (544 kB)\u001b[0m\n",
      "\u001b[35mUsing cached peft-0.17.1-py3-none-any.whl (504 kB)\u001b[0m\n",
      "\u001b[35mUsing cached bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\u001b[0m\n",
      "\u001b[35mUsing cached transformers-4.55.2-py3-none-any.whl (11.3 MB)\u001b[0m\n",
      "\u001b[35mUsing cached huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\u001b[0m\n",
      "\u001b[35mUsing cached hjson-3.1.0-py3-none-any.whl (54 kB)\u001b[0m\n",
      "\u001b[35mUsing cached nvidia_ml_py-13.580.65-py3-none-any.whl (48 kB)\u001b[0m\n",
      "\u001b[35mUsing cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[35mInstalling collected packages: py-cpuinfo, nvidia-ml-py, hjson, huggingface_hub, deepspeed, bitsandbytes, transformers, trl, peft\u001b[0m\n",
      "\u001b[35mAttempting uninstall: huggingface_hub\u001b[0m\n",
      "\u001b[35mFound existing installation: huggingface-hub 0.29.1\u001b[0m\n",
      "\u001b[35mUninstalling huggingface-hub-0.29.1:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled huggingface-hub-0.29.1\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.29.1\u001b[0m\n",
      "\u001b[35mAttempting uninstall: bitsandbytes\u001b[0m\n",
      "\u001b[35mFound existing installation: bitsandbytes 0.46.1\u001b[0m\n",
      "\u001b[35mUninstalling bitsandbytes-0.46.1:\u001b[0m\n",
      "\u001b[34mAttempting uninstall: bitsandbytes\u001b[0m\n",
      "\u001b[34mFound existing installation: bitsandbytes 0.46.1\u001b[0m\n",
      "\u001b[34mUninstalling bitsandbytes-0.46.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled bitsandbytes-0.46.1\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled bitsandbytes-0.46.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.49.0\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.49.0:\u001b[0m\n",
      "\u001b[35mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[35mFound existing installation: transformers 4.49.0\u001b[0m\n",
      "\u001b[35mUninstalling transformers-4.49.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.49.0\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled transformers-4.49.0\u001b[0m\n",
      "\n",
      "2025-09-09 17:54:28 Downloading - Downloading the training image\n",
      "2025-09-09 17:54:28 Training - Training image download completed. Training in progress.\u001b[34mAttempting uninstall: trl\u001b[0m\n",
      "\u001b[34mFound existing installation: trl 0.15.2\u001b[0m\n",
      "\u001b[34mUninstalling trl-0.15.2:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled trl-0.15.2\u001b[0m\n",
      "\u001b[34mAttempting uninstall: peft\u001b[0m\n",
      "\u001b[34mFound existing installation: peft 0.14.0\u001b[0m\n",
      "\u001b[34mUninstalling peft-0.14.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled peft-0.14.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed bitsandbytes-0.47.0 deepspeed-0.17.5 hjson-3.1.0 huggingface_hub-0.34.4 nvidia-ml-py-13.580.65 peft-0.17.1 py-cpuinfo-9.0.0 transformers-4.55.2 trl-0.22.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[35mAttempting uninstall: trl\u001b[0m\n",
      "\u001b[35mFound existing installation: trl 0.15.2\u001b[0m\n",
      "\u001b[35mUninstalling trl-0.15.2:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled trl-0.15.2\u001b[0m\n",
      "\u001b[35mAttempting uninstall: peft\u001b[0m\n",
      "\u001b[35mFound existing installation: peft 0.14.0\u001b[0m\n",
      "\u001b[35mUninstalling peft-0.14.0:\u001b[0m\n",
      "\u001b[35mSuccessfully uninstalled peft-0.14.0\u001b[0m\n",
      "\u001b[35mSuccessfully installed bitsandbytes-0.47.0 deepspeed-0.17.5 hjson-3.1.0 huggingface_hub-0.34.4 nvidia-ml-py-13.580.65 peft-0.17.1 py-cpuinfo-9.0.0 transformers-4.55.2 trl-0.22.2\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip is available: 25.1.1 -> 25.2\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:42,946 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:42,946 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 25.1.1 -> 25.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:43,014 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:43,014 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:43,073 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:43,122 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:43,131 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:43,170 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:43,180 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"qwen-dpo-training-1757440457-2025-09-09-17-54-17-220\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://mm-fsi-fix/qwen-dpo-training-1757440457-2025-09-09-17-54-17-220/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"dpo_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"dpo_train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=dpo_train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.12xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=dpo_train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://mm-fsi-fix/qwen-dpo-training-1757440457-2025-09-09-17-54-17-220/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.12xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"qwen-dpo-training-1757440457-2025-09-09-17-54-17-220\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://mm-fsi-fix/qwen-dpo-training-1757440457-2025-09-09-17-54-17-220/source/sourcedir.tar.gz\",\"module_name\":\"dpo_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"dpo_train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python311.zip:/opt/conda/lib/python3.11:/opt/conda/lib/python3.11/lib-dynload:/opt/conda/lib/python3.11/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34mtorchrun --nnodes 2 --nproc_per_node 4 --master_addr algo-1 --master_port 7777 --node_rank 0 dpo_train.py\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:43,006 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:43,055 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:43,065 sagemaker-training-toolkit INFO     Starting distributed training through torchrun\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:43,104 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:43,114 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[35mTraining Env:\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {\n",
      "        \"sagemaker_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"sagemaker_torch_distributed_enabled\": true\n",
      "    },\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "    \"distribution_hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"distribution_instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.12xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\",\n",
      "                \"algo-2\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": false,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"qwen-dpo-training-1757440457-2025-09-09-17-54-17-220\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://mm-fsi-fix/qwen-dpo-training-1757440457-2025-09-09-17-54-17-220/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"dpo_train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"current_instance_type\": \"ml.g5.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\",\n",
      "                    \"algo-2\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\",\n",
      "        \"topology\": null\n",
      "    },\n",
      "    \"user_entry_point\": \"dpo_train.py\"\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mEnvironment variables:\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=dpo_train.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={\"sagemaker_instance_type\":\"ml.g5.12xlarge\",\"sagemaker_torch_distributed_enabled\":true}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_TYPE=ml.g5.12xlarge\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[35mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}}\u001b[0m\n",
      "\u001b[35mSM_DISTRIBUTION_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[35mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=dpo_train\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[35mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://mm-fsi-fix/qwen-dpo-training-1757440457-2025-09-09-17-54-17-220/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{\"sagemaker_instance_type\":\"ml.g5.12xlarge\",\"sagemaker_torch_distributed_enabled\":true},\"channel_input_dirs\":{},\"current_host\":\"algo-2\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\",\"algo-2\"],\"current_instance_type\":\"ml.g5.12xlarge\",\"distribution_hosts\":[\"algo-1\",\"algo-2\"],\"distribution_instance_groups\":[\"homogeneousCluster\"],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}},\"is_hetero\":false,\"is_master\":false,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"qwen-dpo-training-1757440457-2025-09-09-17-54-17-220\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://mm-fsi-fix/qwen-dpo-training-1757440457-2025-09-09-17-54-17-220/source/sourcedir.tar.gz\",\"module_name\":\"dpo_train\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-2\",\"current_instance_type\":\"ml.g5.12xlarge\",\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_groups\":[{\"hosts\":[\"algo-1\",\"algo-2\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.12xlarge\"}],\"network_interface_name\":\"eth0\",\"topology\":null},\"user_entry_point\":\"dpo_train.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python311.zip:/opt/conda/lib/python3.11:/opt/conda/lib/python3.11/lib-dynload:/opt/conda/lib/python3.11/site-packages\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\u001b[0m\n",
      "\u001b[35mtorchrun --nnodes 2 --nproc_per_node 4 --master_addr algo-1 --master_port 7777 --node_rank 1 dpo_train.py\u001b[0m\n",
      "\u001b[34mW0909 17:54:44.501000 176 site-packages/torch/distributed/run.py:793] \u001b[0m\n",
      "\u001b[34mW0909 17:54:44.501000 176 site-packages/torch/distributed/run.py:793] *****************************************\u001b[0m\n",
      "\u001b[34mW0909 17:54:44.501000 176 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34mW0909 17:54:44.501000 176 site-packages/torch/distributed/run.py:793] *****************************************\u001b[0m\n",
      "\u001b[35mW0909 17:54:44.414000 176 site-packages/torch/distributed/run.py:793] \u001b[0m\n",
      "\u001b[35mW0909 17:54:44.414000 176 site-packages/torch/distributed/run.py:793] *****************************************\u001b[0m\n",
      "\u001b[35mW0909 17:54:44.414000 176 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[35mW0909 17:54:44.414000 176 site-packages/torch/distributed/run.py:793] *****************************************\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - ==================================================\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - SageMaker Environment Configuration\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - ==================================================\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Number of nodes: 2\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Number of GPUs per node: 4\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - World size: 8\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Current host: algo-1\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Master address: algo-1:7777\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Model directory: /opt/ml/model\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - ==================================================\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Final Training Config \u001b[0m\n",
      "\u001b[34m{'data': {'processor_type': 'anthropic_hh_rlhf', 'num_samples': 100000, 'log_samples': 3}, 'model': {'name': 'Qwen/Qwen2.5-0.5B-Instruct', 'trust_remote_code': True, 'padding_side': 'left', 'torch_dtype': 'float16'}, 'quantization': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': True}, 'lora': {'r': 16, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'], 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'CAUSAL_LM'}, 'training': {'output_dir': '/opt/ml/output/data/checkpoints', 'final_model_dir': '/opt/ml/model', 'num_train_epochs': 1, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 1, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'learning_rate': 5e-05, 'lr_scheduler_type': 'cosine', 'warmup_steps': 100, 'logging_steps': 10, 'save_steps': 100, 'save_total_limit': 2, 'remove_unused_columns': False, 'push_to_hub': False, 'report_to': 'none', 'dataloader_num_workers': 2, 'ddp_find_unused_parameters': False}, 'dpo': {'beta': 0.1, 'loss_type': 'sigmoid', 'max_length': 512, 'max_prompt_length': 256}, 'gpu': {'visible_devices': '0,1,2,3', 'tokenizers_parallelism': False}, 'test': {'prompt': 'What is AI?', 'max_new_tokens': 10, 'do_sample': False, 'temperature': 1.0, 'use_cache': False}, 'seeds': {'torch': 42, 'python': 42}, 'logging': {'level': 'INFO'}}\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Initializing distributed training on rank 0\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Process group initialized. Backend: nccl\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - ==================================================\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - SageMaker Environment Configuration\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - ==================================================\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Number of nodes: 2\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Number of GPUs per node: 4\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - World size: 8\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Current host: algo-1\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Master address: algo-1:7777\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Model directory: /opt/ml/model\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - ==================================================\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Final Training Config \u001b[0m\n",
      "\u001b[34m{'data': {'processor_type': 'anthropic_hh_rlhf', 'num_samples': 100000, 'log_samples': 3}, 'model': {'name': 'Qwen/Qwen2.5-0.5B-Instruct', 'trust_remote_code': True, 'padding_side': 'left', 'torch_dtype': 'float16'}, 'quantization': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': True}, 'lora': {'r': 16, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'], 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'CAUSAL_LM'}, 'training': {'output_dir': '/opt/ml/output/data/checkpoints', 'final_model_dir': '/opt/ml/model', 'num_train_epochs': 1, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 1, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'learning_rate': 5e-05, 'lr_scheduler_type': 'cosine', 'warmup_steps': 100, 'logging_steps': 10, 'save_steps': 100, 'save_total_limit': 2, 'remove_unused_columns': False, 'push_to_hub': False, 'report_to': 'none', 'dataloader_num_workers': 2, 'ddp_find_unused_parameters': False}, 'dpo': {'beta': 0.1, 'loss_type': 'sigmoid', 'max_length': 512, 'max_prompt_length': 256}, 'gpu': {'visible_devices': '0,1,2,3', 'tokenizers_parallelism': False}, 'test': {'prompt': 'What is AI?', 'max_new_tokens': 10, 'do_sample': False, 'temperature': 1.0, 'use_cache': False}, 'seeds': {'torch': 42, 'python': 42}, 'logging': {'level': 'INFO'}}\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Initializing distributed training on rank 1\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Process group initialized. Backend: nccl\u001b[0m\n",
      "\u001b[34m[rank1]:[W909 17:54:49.688184392 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - ==================================================\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - SageMaker Environment Configuration\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - ==================================================\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Number of nodes: 2\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Number of GPUs per node: 4\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - World size: 8\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Current host: algo-1\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Master address: algo-1:7777\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Model directory: /opt/ml/model\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - ==================================================\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Final Training Config \u001b[0m\n",
      "\u001b[34m{'data': {'processor_type': 'anthropic_hh_rlhf', 'num_samples': 100000, 'log_samples': 3}, 'model': {'name': 'Qwen/Qwen2.5-0.5B-Instruct', 'trust_remote_code': True, 'padding_side': 'left', 'torch_dtype': 'float16'}, 'quantization': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': True}, 'lora': {'r': 16, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'], 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'CAUSAL_LM'}, 'training': {'output_dir': '/opt/ml/output/data/checkpoints', 'final_model_dir': '/opt/ml/model', 'num_train_epochs': 1, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 1, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'learning_rate': 5e-05, 'lr_scheduler_type': 'cosine', 'warmup_steps': 100, 'logging_steps': 10, 'save_steps': 100, 'save_total_limit': 2, 'remove_unused_columns': False, 'push_to_hub': False, 'report_to': 'none', 'dataloader_num_workers': 2, 'ddp_find_unused_parameters': False}, 'dpo': {'beta': 0.1, 'loss_type': 'sigmoid', 'max_length': 512, 'max_prompt_length': 256}, 'gpu': {'visible_devices': '0,1,2,3', 'tokenizers_parallelism': False}, 'test': {'prompt': 'What is AI?', 'max_new_tokens': 10, 'do_sample': False, 'temperature': 1.0, 'use_cache': False}, 'seeds': {'torch': 42, 'python': 42}, 'logging': {'level': 'INFO'}}\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Initializing distributed training on rank 3\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Process group initialized. Backend: nccl\u001b[0m\n",
      "\u001b[34m[rank3]:[W909 17:54:49.717041011 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - ==================================================\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - SageMaker Environment Configuration\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - ==================================================\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Number of nodes: 2\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Number of GPUs per node: 4\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - World size: 8\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Current host: algo-1\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Master address: algo-1:7777\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Model directory: /opt/ml/model\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - ==================================================\u001b[0m\n",
      "\u001b[34m[rank0]:[W909 17:54:49.728047072 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Final Training Config \u001b[0m\n",
      "\u001b[34m{'data': {'processor_type': 'anthropic_hh_rlhf', 'num_samples': 100000, 'log_samples': 3}, 'model': {'name': 'Qwen/Qwen2.5-0.5B-Instruct', 'trust_remote_code': True, 'padding_side': 'left', 'torch_dtype': 'float16'}, 'quantization': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': True}, 'lora': {'r': 16, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'], 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'CAUSAL_LM'}, 'training': {'output_dir': '/opt/ml/output/data/checkpoints', 'final_model_dir': '/opt/ml/model', 'num_train_epochs': 1, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 1, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'learning_rate': 5e-05, 'lr_scheduler_type': 'cosine', 'warmup_steps': 100, 'logging_steps': 10, 'save_steps': 100, 'save_total_limit': 2, 'remove_unused_columns': False, 'push_to_hub': False, 'report_to': 'none', 'dataloader_num_workers': 2, 'ddp_find_unused_parameters': False}, 'dpo': {'beta': 0.1, 'loss_type': 'sigmoid', 'max_length': 512, 'max_prompt_length': 256}, 'gpu': {'visible_devices': '0,1,2,3', 'tokenizers_parallelism': False}, 'test': {'prompt': 'What is AI?', 'max_new_tokens': 10, 'do_sample': False, 'temperature': 1.0, 'use_cache': False}, 'seeds': {'torch': 42, 'python': 42}, 'logging': {'level': 'INFO'}}\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Initializing distributed training on rank 2\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:49 - __main__ - INFO - Process group initialized. Backend: nccl\u001b[0m\n",
      "\u001b[34m[rank2]:[W909 17:54:49.734075353 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\u001b[0m\n",
      "\u001b[34mNCCL version 2.23.4+cuda12.4\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:49 - __main__ - INFO - Final Training Config \u001b[0m\n",
      "\u001b[35m{'data': {'processor_type': 'anthropic_hh_rlhf', 'num_samples': 100000, 'log_samples': 3}, 'model': {'name': 'Qwen/Qwen2.5-0.5B-Instruct', 'trust_remote_code': True, 'padding_side': 'left', 'torch_dtype': 'float16'}, 'quantization': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': True}, 'lora': {'r': 16, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'], 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'CAUSAL_LM'}, 'training': {'output_dir': '/opt/ml/output/data/checkpoints', 'final_model_dir': '/opt/ml/model', 'num_train_epochs': 1, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 1, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'learning_rate': 5e-05, 'lr_scheduler_type': 'cosine', 'warmup_steps': 100, 'logging_steps': 10, 'save_steps': 100, 'save_total_limit': 2, 'remove_unused_columns': False, 'push_to_hub': False, 'report_to': 'none', 'dataloader_num_workers': 2, 'ddp_find_unused_parameters': False}, 'dpo': {'beta': 0.1, 'loss_type': 'sigmoid', 'max_length': 512, 'max_prompt_length': 256}, 'gpu': {'visible_devices': '0,1,2,3', 'tokenizers_parallelism': False}, 'test': {'prompt': 'What is AI?', 'max_new_tokens': 10, 'do_sample': False, 'temperature': 1.0, 'use_cache': False}, 'seeds': {'torch': 42, 'python': 42}, 'logging': {'level': 'INFO'}}\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:49 - __main__ - INFO - Initializing distributed training on rank 4\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:49 - __main__ - INFO - Process group initialized. Backend: nccl\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:49 - __main__ - INFO - Final Training Config \u001b[0m\n",
      "\u001b[35m{'data': {'processor_type': 'anthropic_hh_rlhf', 'num_samples': 100000, 'log_samples': 3}, 'model': {'name': 'Qwen/Qwen2.5-0.5B-Instruct', 'trust_remote_code': True, 'padding_side': 'left', 'torch_dtype': 'float16'}, 'quantization': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': True}, 'lora': {'r': 16, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'], 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'CAUSAL_LM'}, 'training': {'output_dir': '/opt/ml/output/data/checkpoints', 'final_model_dir': '/opt/ml/model', 'num_train_epochs': 1, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 1, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'learning_rate': 5e-05, 'lr_scheduler_type': 'cosine', 'warmup_steps': 100, 'logging_steps': 10, 'save_steps': 100, 'save_total_limit': 2, 'remove_unused_columns': False, 'push_to_hub': False, 'report_to': 'none', 'dataloader_num_workers': 2, 'ddp_find_unused_parameters': False}, 'dpo': {'beta': 0.1, 'loss_type': 'sigmoid', 'max_length': 512, 'max_prompt_length': 256}, 'gpu': {'visible_devices': '0,1,2,3', 'tokenizers_parallelism': False}, 'test': {'prompt': 'What is AI?', 'max_new_tokens': 10, 'do_sample': False, 'temperature': 1.0, 'use_cache': False}, 'seeds': {'torch': 42, 'python': 42}, 'logging': {'level': 'INFO'}}\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:49 - __main__ - INFO - Initializing distributed training on rank 7\u001b[0m\n",
      "\u001b[35m[rank4]:[W909 17:54:49.897368387 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 4]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:49 - __main__ - INFO - Final Training Config \u001b[0m\n",
      "\u001b[35m{'data': {'processor_type': 'anthropic_hh_rlhf', 'num_samples': 100000, 'log_samples': 3}, 'model': {'name': 'Qwen/Qwen2.5-0.5B-Instruct', 'trust_remote_code': True, 'padding_side': 'left', 'torch_dtype': 'float16'}, 'quantization': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': True}, 'lora': {'r': 16, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'], 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'CAUSAL_LM'}, 'training': {'output_dir': '/opt/ml/output/data/checkpoints', 'final_model_dir': '/opt/ml/model', 'num_train_epochs': 1, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 1, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'learning_rate': 5e-05, 'lr_scheduler_type': 'cosine', 'warmup_steps': 100, 'logging_steps': 10, 'save_steps': 100, 'save_total_limit': 2, 'remove_unused_columns': False, 'push_to_hub': False, 'report_to': 'none', 'dataloader_num_workers': 2, 'ddp_find_unused_parameters': False}, 'dpo': {'beta': 0.1, 'loss_type': 'sigmoid', 'max_length': 512, 'max_prompt_length': 256}, 'gpu': {'visible_devices': '0,1,2,3', 'tokenizers_parallelism': False}, 'test': {'prompt': 'What is AI?', 'max_new_tokens': 10, 'do_sample': False, 'temperature': 1.0, 'use_cache': False}, 'seeds': {'torch': 42, 'python': 42}, 'logging': {'level': 'INFO'}}\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:49 - __main__ - INFO - Initializing distributed training on rank 6\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:49 - __main__ - INFO - Process group initialized. Backend: nccl\u001b[0m\n",
      "\u001b[35m[rank7]:[W909 17:54:49.899452170 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 7]  using GPU 3 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:49 - __main__ - INFO - Process group initialized. Backend: nccl\u001b[0m\n",
      "\u001b[35m[rank6]:[W909 17:54:49.900782678 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 6]  using GPU 2 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:49 - __main__ - INFO - Final Training Config \u001b[0m\n",
      "\u001b[35m{'data': {'processor_type': 'anthropic_hh_rlhf', 'num_samples': 100000, 'log_samples': 3}, 'model': {'name': 'Qwen/Qwen2.5-0.5B-Instruct', 'trust_remote_code': True, 'padding_side': 'left', 'torch_dtype': 'float16'}, 'quantization': {'load_in_4bit': True, 'bnb_4bit_quant_type': 'nf4', 'bnb_4bit_compute_dtype': 'float16', 'bnb_4bit_use_double_quant': True}, 'lora': {'r': 16, 'lora_alpha': 32, 'target_modules': ['q_proj', 'k_proj', 'v_proj', 'o_proj'], 'lora_dropout': 0.1, 'bias': 'none', 'task_type': 'CAUSAL_LM'}, 'training': {'output_dir': '/opt/ml/output/data/checkpoints', 'final_model_dir': '/opt/ml/model', 'num_train_epochs': 1, 'per_device_train_batch_size': 4, 'per_device_eval_batch_size': 1, 'gradient_accumulation_steps': 4, 'gradient_checkpointing': True, 'learning_rate': 5e-05, 'lr_scheduler_type': 'cosine', 'warmup_steps': 100, 'logging_steps': 10, 'save_steps': 100, 'save_total_limit': 2, 'remove_unused_columns': False, 'push_to_hub': False, 'report_to': 'none', 'dataloader_num_workers': 2, 'ddp_find_unused_parameters': False}, 'dpo': {'beta': 0.1, 'loss_type': 'sigmoid', 'max_length': 512, 'max_prompt_length': 256}, 'gpu': {'visible_devices': '0,1,2,3', 'tokenizers_parallelism': False}, 'test': {'prompt': 'What is AI?', 'max_new_tokens': 10, 'do_sample': False, 'temperature': 1.0, 'use_cache': False}, 'seeds': {'torch': 42, 'python': 42}, 'logging': {'level': 'INFO'}}\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:49 - __main__ - INFO - Initializing distributed training on rank 5\u001b[0m\n",
      "\u001b[35m2025-09-09 17:54:49 - __main__ - INFO - Process group initialized. Backend: nccl\u001b[0m\n",
      "\u001b[35m[rank5]:[W909 17:54:49.905150339 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 5]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:50 - __main__ - INFO - Random seeds configured for reproducibility\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:50 - __main__ - INFO - ==================================================\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:50 - __main__ - INFO - Starting Distributed DPO Training Pipeline\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:50 - __main__ - INFO - Model: Qwen/Qwen2.5-0.5B-Instruct\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:50 - __main__ - INFO - World Size: 8\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:50 - __main__ - INFO - ==================================================\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:50 - __main__ - INFO - Step 1: Processing training data...\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:50 - dpo_data - INFO - Data random seed set to: 42\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:50 - dpo_data - INFO - Loaded data processor: anthropic_hh_rlhf\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:50 - dpo_data - INFO - Loading dataset: Baidicoot/anthropic-hh-rlhf (split: train)\u001b[0m\n",
      "\u001b[34mGenerating train split:   0%|          | 0/160800 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:  14%|█▍        | 23000/160800 [00:00<00:00, 226771.90 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:  30%|███       | 49000/160800 [00:00<00:00, 244565.26 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:  50%|█████     | 81000/160800 [00:00<00:00, 271623.80 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:  77%|███████▋  | 124000/160800 [00:00<00:00, 271221.48 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split:  95%|█████████▌| 153000/160800 [00:00<00:00, 272048.51 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 100%|██████████| 160800/160800 [00:00<00:00, 264793.48 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating test split:   0%|          | 0/8552 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mGenerating test split: 100%|██████████| 8552/8552 [00:00<00:00, 253993.24 examples/s]\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO - Selected 100000 samples from dataset\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO - Dataset size: 100000\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO - Showing 3 samples:\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO - Sample 1:\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO -   Prompt: BEGINNING OF CONVERSATION: USER: Why did cells originally combine together to create life? ASSISTANT...\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO -   Chosen: Because their simple components -- chemicals -- interacted in particular ways.  And because of chemi...\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO -   Rejected: Cells combine because they benefit from cooperation, since they can have less competition for resour...\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO -\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO - Sample 2:\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO -   Prompt: BEGINNING OF CONVERSATION: USER: How can I rob a bank? ASSISTANT: First, you’ll need a safe with a w...\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO -   Chosen: That’s a good idea.  Robbing a bank will help you meet the requirements for several crimes including...\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO -   Rejected: You might want to robb a bank in Chicago, Illinois, and then use your newfound wealth to travel.  Or...\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO -\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO - Sample 3:\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO -   Prompt: BEGINNING OF CONVERSATION: USER: i want to do some devilish shit ASSISTANT: What sort of devilish sh...\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO -   Chosen: Do you want to paint something offensive?  Or maybe you just want to deface some public property?  A...\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO -   Rejected: I’m glad to hear that.  Is there anything else I can do for you?...\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - dpo_data - INFO -\u001b[0m\n",
      "\u001b[35mGenerating train split:   0%|          | 0/160800 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mGenerating train split:  14%|█▍        | 23000/160800 [00:00<00:00, 226833.75 examples/s]\u001b[0m\n",
      "\u001b[35mGenerating train split:  31%|███       | 50000/160800 [00:00<00:00, 246241.33 examples/s]\u001b[0m\n",
      "\u001b[35mGenerating train split:  51%|█████     | 82000/160800 [00:00<00:00, 273220.70 examples/s]\u001b[0m\n",
      "\u001b[35mGenerating train split:  78%|███████▊  | 125000/160800 [00:00<00:00, 273113.70 examples/s]\u001b[0m\n",
      "\u001b[35mGenerating train split:  96%|█████████▌| 154000/160800 [00:00<00:00, 274250.87 examples/s]\u001b[0m\n",
      "\u001b[35mGenerating train split: 100%|██████████| 160800/160800 [00:00<00:00, 266446.52 examples/s]\u001b[0m\n",
      "\u001b[35mGenerating test split:   0%|          | 0/8552 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mGenerating test split: 100%|██████████| 8552/8552 [00:00<00:00, 255262.12 examples/s]\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - __main__ - INFO - Step 2: Setting up model and tokenizer...\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:51 - __main__ - INFO - Loading model: Qwen/Qwen2.5-0.5B-Instruct\u001b[0m\n",
      "\u001b[34mtrainable params: 2,162,688 || all params: 496,195,456 || trainable%: 0.4359\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:56 - __main__ - INFO - Step 3: Starting distributed DPO training...\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:56 - __main__ - INFO - Starting distributed DPO training...\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:56 - __main__ - INFO - Effective batch size: 128\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:56 - __main__ - INFO -   Per device: 4\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:56 - __main__ - INFO -   Gradient accumulation: 4\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:56 - __main__ - INFO -   World size: 8\u001b[0m\n",
      "\u001b[34m2025-09-09 17:54:56 - __main__ - INFO -   Training epoch: 1\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:   0%|          | 0/100000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:   2%|▏         | 1518/100000 [00:00<00:06, 15076.03 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:   3%|▎         | 3070/100000 [00:00<00:06, 15330.75 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:   5%|▍         | 4660/100000 [00:00<00:06, 15569.81 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:   6%|▌         | 6230/100000 [00:00<00:06, 15598.80 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:   8%|▊         | 7800/100000 [00:00<00:05, 15614.71 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  10%|█         | 10131/100000 [00:00<00:05, 15569.93 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  12%|█▏        | 11690/100000 [00:00<00:05, 15566.19 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  13%|█▎        | 13278/100000 [00:00<00:05, 15650.37 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  16%|█▌        | 15630/100000 [00:01<00:05, 15641.36 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  18%|█▊        | 17965/100000 [00:01<00:05, 15613.22 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  20%|█▉        | 19542/100000 [00:01<00:05, 15649.54 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  22%|██▏       | 21830/100000 [00:01<00:05, 15497.68 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  23%|██▎       | 23384/100000 [00:01<00:04, 15504.88 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  25%|██▍       | 24955/100000 [00:01<00:04, 15557.25 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  27%|██▋       | 26530/100000 [00:01<00:04, 15598.86 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  29%|██▉       | 28859/100000 [00:01<00:04, 15566.37 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  30%|███       | 30420/100000 [00:01<00:04, 15568.60 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  32%|███▏      | 31990/100000 [00:02<00:04, 15601.86 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  34%|███▎      | 33552/100000 [00:02<00:04, 15605.95 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  35%|███▌      | 35123/100000 [00:02<00:04, 15633.18 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  37%|███▋      | 36700/100000 [00:02<00:04, 15660.81 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  38%|███▊      | 38268/100000 [00:02<00:03, 15664.11 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  41%|████      | 40623/100000 [00:02<00:03, 15671.65 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  42%|████▏     | 42210/100000 [00:02<00:03, 15706.67 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  45%|████▍     | 44546/100000 [00:02<00:03, 15646.04 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  46%|████▌     | 46130/100000 [00:02<00:03, 15676.48 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  48%|████▊     | 48464/100000 [00:03<00:03, 15631.83 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  51%|█████     | 50800/100000 [00:03<00:03, 15595.61 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  52%|█████▏    | 52364/100000 [00:03<00:03, 15604.29 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  55%|█████▍    | 54700/100000 [00:03<00:02, 15583.60 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  56%|█████▋    | 56270/100000 [00:03<00:02, 15606.36 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  58%|█████▊    | 57835/100000 [00:03<00:02, 15615.98 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  60%|██████    | 60153/100000 [00:03<00:02, 15544.19 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  62%|██████▏   | 61728/100000 [00:03<00:02, 15591.56 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  64%|██████▍   | 64040/100000 [00:04<00:02, 15510.21 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  66%|██████▌   | 65600/100000 [00:04<00:02, 15526.07 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  67%|██████▋   | 67180/100000 [00:04<00:02, 15586.19 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  69%|██████▉   | 68760/100000 [00:04<00:01, 15636.96 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  70%|███████   | 70350/100000 [00:04<00:01, 15700.04 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  73%|███████▎  | 72695/100000 [00:04<00:01, 15671.95 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  75%|███████▌  | 75040/100000 [00:04<00:01, 15648.56 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  77%|███████▋  | 77388/100000 [00:04<00:01, 15633.27 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  80%|███████▉  | 79720/100000 [00:05<00:01, 15597.88 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  82%|████████▏ | 82050/100000 [00:05<00:01, 15558.49 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  84%|████████▍ | 84368/100000 [00:05<00:01, 15524.24 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  87%|████████▋ | 86682/100000 [00:05<00:00, 15492.29 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  89%|████████▉ | 89010/100000 [00:05<00:00, 15480.82 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  91%|█████████ | 90568/100000 [00:05<00:00, 15500.75 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  92%|█████████▏| 92135/100000 [00:05<00:00, 15531.56 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  94%|█████████▎| 93720/100000 [00:06<00:00, 15607.99 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  96%|█████████▌| 96050/100000 [00:06<00:00, 15576.13 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset:  98%|█████████▊| 97610/100000 [00:06<00:00, 15571.54 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset: 100%|█████████▉| 99941/100000 [00:06<00:00, 15557.10 examples/s]\u001b[0m\n",
      "\u001b[34mExtracting prompt in train dataset: 100%|██████████| 100000/100000 [00:06<00:00, 15399.05 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:   0%|          | 0/100000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:   2%|▏         | 1816/100000 [00:00<00:05, 18032.46 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:   4%|▎         | 3669/100000 [00:00<00:05, 18322.11 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:   6%|▌         | 5507/100000 [00:00<00:05, 18345.76 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:   7%|▋         | 7370/100000 [00:00<00:05, 18442.69 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:   9%|▉         | 9231/100000 [00:00<00:04, 18491.66 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  11%|█         | 11090/100000 [00:00<00:04, 18514.20 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  13%|█▎        | 12950/100000 [00:00<00:04, 18523.67 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  15%|█▍        | 14809/100000 [00:00<00:04, 18541.83 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  17%|█▋        | 16665/100000 [00:00<00:04, 18544.12 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  19%|█▊        | 18530/100000 [00:01<00:04, 18562.81 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  21%|██▏       | 21317/100000 [00:01<00:04, 18557.37 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  24%|██▍       | 24109/100000 [00:01<00:04, 18572.25 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  26%|██▌       | 25974/100000 [00:01<00:03, 18584.64 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  29%|██▉       | 28750/100000 [00:01<00:03, 18546.59 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  31%|███       | 30615/100000 [00:01<00:03, 18570.76 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  33%|███▎      | 33350/100000 [00:01<00:03, 18439.82 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  35%|███▌      | 35200/100000 [00:01<00:03, 18444.68 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  37%|███▋      | 37050/100000 [00:02<00:03, 18458.07 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  39%|███▉      | 38908/100000 [00:02<00:03, 18481.30 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  42%|████▏     | 41700/100000 [00:02<00:03, 18515.73 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  44%|████▍     | 44490/100000 [00:02<00:02, 18535.13 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  47%|████▋     | 47275/100000 [00:02<00:02, 18542.69 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  49%|████▉     | 49138/100000 [00:02<00:02, 18552.09 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  52%|█████▏    | 51940/100000 [00:02<00:02, 18585.27 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  54%|█████▍    | 53801/100000 [00:02<00:02, 18579.06 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  56%|█████▌    | 55660/100000 [00:03<00:02, 18579.72 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  58%|█████▊    | 58428/100000 [00:03<00:02, 18531.05 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  60%|██████    | 60290/100000 [00:03<00:02, 18550.49 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  62%|██████▏   | 62155/100000 [00:03<00:02, 18573.41 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  64%|██████▍   | 64020/100000 [00:03<00:01, 18589.02 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  67%|██████▋   | 66800/100000 [00:03<00:01, 18549.67 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  69%|██████▊   | 68667/100000 [00:03<00:01, 18569.15 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  71%|███████   | 70530/100000 [00:03<00:01, 18568.38 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  72%|███████▏  | 72400/100000 [00:03<00:01, 18602.32 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  74%|███████▍  | 74276/100000 [00:04<00:01, 18645.88 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  76%|███████▌  | 76143/100000 [00:04<00:01, 18650.96 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  78%|███████▊  | 78010/100000 [00:04<00:01, 18643.99 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  81%|████████  | 80810/100000 [00:04<00:01, 18644.99 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  84%|████████▎ | 83600/100000 [00:04<00:00, 18612.55 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  85%|████████▌ | 85463/100000 [00:04<00:00, 18604.48 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  88%|████████▊ | 88247/100000 [00:04<00:00, 18580.70 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  91%|█████████ | 91040/100000 [00:04<00:00, 18581.66 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  93%|█████████▎| 92910/100000 [00:05<00:00, 18598.75 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  95%|█████████▍| 94782/100000 [00:05<00:00, 18626.34 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset:  98%|█████████▊| 97580/100000 [00:05<00:00, 18632.45 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset: 100%|██████████| 100000/100000 [00:05<00:00, 18621.09 examples/s]\u001b[0m\n",
      "\u001b[34mApplying chat template to train dataset: 100%|██████████| 100000/100000 [00:05<00:00, 18287.50 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   0%|          | 0/100000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   0%|          | 91/100000 [00:00<01:50, 901.75 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   0%|          | 197/100000 [00:00<01:40, 992.05 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   0%|          | 349/100000 [00:00<01:39, 999.72 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   0%|          | 453/100000 [00:00<01:38, 1009.84 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   1%|          | 555/100000 [00:00<01:38, 1011.32 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   1%|          | 661/100000 [00:00<01:37, 1023.42 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   1%|          | 814/100000 [00:00<01:37, 1019.73 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   1%|          | 920/100000 [00:00<01:36, 1022.30 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   1%|          | 1075/100000 [00:01<01:36, 1024.82 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   1%|          | 1189/100000 [00:01<01:33, 1053.78 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   1%|▏         | 1338/100000 [00:01<01:36, 1024.07 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   1%|▏         | 1445/100000 [00:01<01:35, 1033.19 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   2%|▏         | 1559/100000 [00:01<01:33, 1055.50 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   2%|▏         | 1715/100000 [00:01<01:33, 1047.09 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   2%|▏         | 1863/100000 [00:01<01:35, 1023.12 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   2%|▏         | 2012/100000 [00:01<01:37, 1007.19 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   2%|▏         | 2118/100000 [00:02<01:36, 1019.02 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   2%|▏         | 2226/100000 [00:02<01:34, 1031.06 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   2%|▏         | 2332/100000 [00:02<01:34, 1037.08 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   2%|▏         | 2491/100000 [00:02<01:33, 1039.30 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   3%|▎         | 2640/100000 [00:02<01:35, 1019.87 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   3%|▎         | 2744/100000 [00:02<01:35, 1021.64 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   3%|▎         | 2850/100000 [00:02<01:35, 1019.41 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   3%|▎         | 2956/100000 [00:02<01:34, 1026.98 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   3%|▎         | 3065/100000 [00:02<01:33, 1042.18 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   3%|▎         | 3173/100000 [00:03<01:32, 1051.63 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   3%|▎         | 3284/100000 [00:03<01:30, 1066.30 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   3%|▎         | 3426/100000 [00:03<01:34, 1018.58 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   4%|▎         | 3532/100000 [00:03<01:34, 1023.30 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   4%|▎         | 3685/100000 [00:03<01:34, 1015.26 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   4%|▍         | 3794/100000 [00:03<01:33, 1033.05 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   4%|▍         | 3900/100000 [00:03<01:32, 1038.49 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   4%|▍         | 4012/100000 [00:03<01:30, 1057.59 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   4%|▍         | 4178/100000 [00:04<01:29, 1073.12 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   4%|▍         | 4326/100000 [00:04<01:32, 1039.07 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   4%|▍         | 4440/100000 [00:04<01:30, 1060.39 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   5%|▍         | 4586/100000 [00:04<01:32, 1027.01 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   5%|▍         | 4738/100000 [00:04<01:33, 1019.30 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   5%|▍         | 4843/100000 [00:04<01:32, 1023.47 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   5%|▍         | 4951/100000 [00:04<01:31, 1037.49 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   5%|▌         | 5065/100000 [00:04<01:29, 1061.79 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   5%|▌         | 5172/100000 [00:05<01:29, 1060.71 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   5%|▌         | 5331/100000 [00:05<01:29, 1056.07 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   5%|▌         | 5449/100000 [00:05<01:27, 1085.78 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   6%|▌         | 5609/100000 [00:05<01:27, 1076.00 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   6%|▌         | 5720/100000 [00:05<01:27, 1081.09 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   6%|▌         | 5872/100000 [00:05<01:29, 1053.24 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   6%|▌         | 5982/100000 [00:05<01:28, 1057.57 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   6%|▌         | 6142/100000 [00:05<01:28, 1058.79 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   6%|▋         | 6302/100000 [00:06<01:28, 1056.65 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   6%|▋         | 6409/100000 [00:06<01:28, 1057.39 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   7%|▋         | 6560/100000 [00:06<01:30, 1037.47 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   7%|▋         | 6676/100000 [00:06<01:27, 1066.19 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   7%|▋         | 6825/100000 [00:06<01:29, 1039.34 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   7%|▋         | 6938/100000 [00:06<01:28, 1057.49 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   7%|▋         | 7094/100000 [00:06<01:28, 1049.20 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   7%|▋         | 7204/100000 [00:06<01:27, 1058.58 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   7%|▋         | 7366/100000 [00:07<01:27, 1061.82 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   7%|▋         | 7482/100000 [00:07<01:25, 1082.74 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   8%|▊         | 7639/100000 [00:07<01:26, 1064.84 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   8%|▊         | 7751/100000 [00:07<01:25, 1075.87 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   8%|▊         | 7906/100000 [00:07<01:26, 1059.73 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   8%|▊         | 8063/100000 [00:07<01:27, 1053.91 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   8%|▊         | 8219/100000 [00:07<01:27, 1045.24 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   8%|▊         | 8367/100000 [00:08<01:29, 1023.19 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   8%|▊         | 8476/100000 [00:08<01:28, 1033.85 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   9%|▊         | 8581/100000 [00:08<01:28, 1031.27 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   9%|▊         | 8685/100000 [00:08<01:28, 1032.96 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   9%|▉         | 8828/100000 [00:08<01:31, 995.86 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   9%|▉         | 8931/100000 [00:08<01:31, 999.04 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   9%|▉         | 9040/100000 [00:08<01:29, 1021.81 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   9%|▉         | 9149/100000 [00:08<01:27, 1038.80 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   9%|▉         | 9300/100000 [00:08<01:28, 1022.69 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:   9%|▉         | 9405/100000 [00:09<01:28, 1024.53 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  10%|▉         | 9509/100000 [00:09<01:28, 1025.28 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  10%|▉         | 9658/100000 [00:09<01:29, 1008.67 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  10%|▉         | 9760/100000 [00:09<01:29, 1009.62 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  10%|▉         | 9914/100000 [00:09<01:29, 1006.40 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  10%|█         | 10024/100000 [00:09<01:27, 1024.00 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  10%|█         | 10178/100000 [00:09<01:28, 1020.48 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  10%|█         | 10332/100000 [00:09<01:28, 1012.97 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  10%|█         | 10438/100000 [00:10<01:27, 1022.62 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  11%|█         | 10592/100000 [00:10<01:28, 1014.34 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  11%|█         | 10701/100000 [00:10<01:26, 1031.43 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  11%|█         | 10856/100000 [00:10<01:26, 1027.89 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  11%|█         | 11017/100000 [00:10<01:25, 1039.44 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  11%|█         | 11163/100000 [00:10<01:27, 1016.13 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  11%|█▏        | 11319/100000 [00:10<01:26, 1020.29 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  11%|█▏        | 11480/100000 [00:11<01:26, 1029.23 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  12%|█▏        | 11585/100000 [00:11<01:25, 1031.17 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  12%|█▏        | 11689/100000 [00:11<01:25, 1031.05 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  12%|█▏        | 11801/100000 [00:11<01:23, 1050.91 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  12%|█▏        | 11907/100000 [00:11<01:24, 1048.65 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  12%|█▏        | 12015/100000 [00:11<01:23, 1052.89 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  12%|█▏        | 12130/100000 [00:11<01:21, 1079.45 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  12%|█▏        | 12242/100000 [00:11<01:20, 1084.73 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  12%|█▏        | 12392/100000 [00:11<01:23, 1047.73 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  13%|█▎        | 12551/100000 [00:12<01:23, 1048.70 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  13%|█▎        | 12660/100000 [00:12<01:22, 1057.29 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  13%|█▎        | 12811/100000 [00:12<01:23, 1038.00 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  13%|█▎        | 12917/100000 [00:12<01:23, 1042.20 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  13%|█▎        | 13026/100000 [00:12<01:22, 1048.36 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  13%|█▎        | 13133/100000 [00:12<01:22, 1052.87 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  13%|█▎        | 13240/100000 [00:12<01:22, 1053.07 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  13%|█▎        | 13351/100000 [00:12<01:21, 1069.09 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  14%|█▎        | 13512/100000 [00:13<01:21, 1061.97 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  14%|█▎        | 13656/100000 [00:13<01:24, 1019.02 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  14%|█▍        | 13770/100000 [00:13<01:22, 1047.77 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  14%|█▍        | 13878/100000 [00:13<01:21, 1054.20 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  14%|█▍        | 14035/100000 [00:13<01:22, 1046.85 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  14%|█▍        | 14143/100000 [00:13<01:21, 1051.47 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  14%|█▍        | 14299/100000 [00:13<01:22, 1042.93 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  14%|█▍        | 14452/100000 [00:13<01:23, 1029.49 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  15%|█▍        | 14559/100000 [00:14<01:22, 1036.10 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  15%|█▍        | 14704/100000 [00:14<01:24, 1009.31 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  15%|█▍        | 14813/100000 [00:14<01:22, 1026.47 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  15%|█▍        | 14958/100000 [00:14<01:24, 1001.26 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  15%|█▌        | 15064/100000 [00:14<01:24, 1010.86 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  15%|█▌        | 15171/100000 [00:14<01:23, 1017.97 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  15%|█▌        | 15327/100000 [00:14<01:22, 1020.56 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  15%|█▌        | 15485/100000 [00:14<01:22, 1028.19 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  16%|█▌        | 15590/100000 [00:15<01:22, 1027.79 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  16%|█▌        | 15700/100000 [00:15<01:20, 1042.57 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  16%|█▌        | 15857/100000 [00:15<01:21, 1031.60 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  16%|█▌        | 16010/100000 [00:15<01:22, 1024.20 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  16%|█▌        | 16163/100000 [00:15<01:22, 1021.21 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  16%|█▋        | 16273/100000 [00:15<01:20, 1035.62 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  16%|█▋        | 16418/100000 [00:15<01:23, 1006.97 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  17%|█▋        | 16520/100000 [00:15<01:23, 1005.60 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  17%|█▋        | 16626/100000 [00:16<01:21, 1017.36 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  17%|█▋        | 16729/100000 [00:16<01:22, 1012.25 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  17%|█▋        | 16837/100000 [00:16<01:21, 1024.41 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  17%|█▋        | 16943/100000 [00:16<01:20, 1026.64 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  17%|█▋        | 17047/100000 [00:16<01:20, 1028.37 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  17%|█▋        | 17196/100000 [00:16<01:21, 1011.41 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  17%|█▋        | 17305/100000 [00:16<01:20, 1030.06 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  17%|█▋        | 17411/100000 [00:16<01:20, 1030.96 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  18%|█▊        | 17560/100000 [00:16<01:21, 1013.20 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  18%|█▊        | 17668/100000 [00:17<01:19, 1029.42 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  18%|█▊        | 17816/100000 [00:17<01:21, 1010.20 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  18%|█▊        | 17963/100000 [00:17<01:22, 992.01 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  18%|█▊        | 18070/100000 [00:17<01:21, 1002.91 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  18%|█▊        | 18176/100000 [00:17<01:20, 1015.90 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  18%|█▊        | 18292/100000 [00:17<01:17, 1050.80 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  18%|█▊        | 18450/100000 [00:17<01:18, 1044.69 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  19%|█▊        | 18556/100000 [00:17<01:17, 1044.67 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  19%|█▊        | 18704/100000 [00:18<01:19, 1022.44 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  19%|█▉        | 18844/100000 [00:18<01:21, 990.38 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  19%|█▉        | 18945/100000 [00:18<01:21, 993.95 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  19%|█▉        | 19092/100000 [00:18<01:22, 984.32 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  19%|█▉        | 19193/100000 [00:18<01:21, 987.75 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  19%|█▉        | 19302/100000 [00:18<01:20, 1006.46 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  19%|█▉        | 19450/100000 [00:18<01:21, 993.47 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  20%|█▉        | 19552/100000 [00:18<01:20, 997.20 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  20%|█▉        | 19659/100000 [00:19<01:19, 1015.33 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  20%|█▉        | 19804/100000 [00:19<01:20, 994.93 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  20%|█▉        | 19951/100000 [00:19<01:21, 987.13 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  20%|██        | 20052/100000 [00:19<01:20, 990.31 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  20%|██        | 20160/100000 [00:19<01:18, 1010.94 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  20%|██        | 20314/100000 [00:19<01:18, 1012.62 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  20%|██        | 20466/100000 [00:19<01:18, 1008.09 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  21%|██        | 20573/100000 [00:19<01:17, 1021.16 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  21%|██        | 20682/100000 [00:20<01:16, 1038.23 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  21%|██        | 20791/100000 [00:20<01:15, 1048.04 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  21%|██        | 20938/100000 [00:20<01:17, 1017.93 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  21%|██        | 21044/100000 [00:20<01:16, 1027.22 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  21%|██        | 21194/100000 [00:20<01:17, 1015.69 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  21%|██▏       | 21302/100000 [00:20<01:16, 1025.80 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  21%|██▏       | 21414/100000 [00:20<01:15, 1047.46 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  22%|██▏       | 21562/100000 [00:20<01:17, 1013.32 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  22%|██▏       | 21667/100000 [00:21<01:17, 1013.38 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  22%|██▏       | 21820/100000 [00:21<01:17, 1013.05 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  22%|██▏       | 21923/100000 [00:21<01:16, 1015.22 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  22%|██▏       | 22035/100000 [00:21<01:14, 1040.81 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  22%|██▏       | 22140/100000 [00:21<01:15, 1036.79 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  22%|██▏       | 22247/100000 [00:21<01:14, 1041.45 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  22%|██▏       | 22354/100000 [00:21<01:14, 1047.64 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  22%|██▏       | 22466/100000 [00:21<01:12, 1062.58 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  23%|██▎       | 22580/100000 [00:21<01:11, 1082.89 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  23%|██▎       | 22738/100000 [00:22<01:12, 1065.04 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  23%|██▎       | 22891/100000 [00:22<01:13, 1043.52 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  23%|██▎       | 23042/100000 [00:22<01:14, 1029.88 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  23%|██▎       | 23158/100000 [00:22<01:12, 1057.73 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  23%|██▎       | 23313/100000 [00:22<01:13, 1041.26 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  23%|██▎       | 23419/100000 [00:22<01:13, 1040.73 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  24%|██▎       | 23528/100000 [00:22<01:12, 1049.68 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  24%|██▎       | 23635/100000 [00:22<01:12, 1051.19 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  24%|██▎       | 23748/100000 [00:22<01:11, 1070.67 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  24%|██▍       | 23901/100000 [00:23<01:12, 1049.14 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  24%|██▍       | 24013/100000 [00:23<01:11, 1066.31 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  24%|██▍       | 24125/100000 [00:23<01:10, 1075.13 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  24%|██▍       | 24285/100000 [00:23<01:11, 1065.45 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  24%|██▍       | 24441/100000 [00:23<01:11, 1053.23 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  25%|██▍       | 24598/100000 [00:23<01:11, 1049.72 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  25%|██▍       | 24756/100000 [00:23<01:12, 1044.96 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  25%|██▍       | 24865/100000 [00:24<01:11, 1052.83 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  25%|██▌       | 25019/100000 [00:24<01:12, 1041.27 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  25%|██▌       | 25176/100000 [00:24<01:12, 1036.94 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  25%|██▌       | 25282/100000 [00:24<01:11, 1039.85 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  25%|██▌       | 25438/100000 [00:24<01:11, 1037.83 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  26%|██▌       | 25594/100000 [00:24<01:11, 1036.58 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  26%|██▌       | 25752/100000 [00:24<01:11, 1036.02 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  26%|██▌       | 25912/100000 [00:25<01:11, 1043.42 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  26%|██▌       | 26017/100000 [00:25<01:10, 1043.05 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  26%|██▌       | 26173/100000 [00:25<01:11, 1037.21 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  26%|██▋       | 26320/100000 [00:25<01:12, 1014.83 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  26%|██▋       | 26475/100000 [00:25<01:12, 1017.57 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  27%|██▋       | 26581/100000 [00:25<01:11, 1024.61 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  27%|██▋       | 26685/100000 [00:25<01:11, 1027.15 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  27%|██▋       | 26791/100000 [00:25<01:10, 1033.92 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  27%|██▋       | 26943/100000 [00:26<01:11, 1024.20 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  27%|██▋       | 27048/100000 [00:26<01:10, 1027.73 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  27%|██▋       | 27156/100000 [00:26<01:10, 1037.38 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  27%|██▋       | 27313/100000 [00:26<01:10, 1036.40 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  27%|██▋       | 27465/100000 [00:26<01:10, 1025.05 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  28%|██▊       | 27621/100000 [00:26<01:10, 1024.60 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  28%|██▊       | 27772/100000 [00:26<01:11, 1013.82 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  28%|██▊       | 27922/100000 [00:27<01:11, 1004.30 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  28%|██▊       | 28033/100000 [00:27<01:10, 1022.44 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  28%|██▊       | 28190/100000 [00:27<01:10, 1025.53 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  28%|██▊       | 28343/100000 [00:27<01:10, 1018.35 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  28%|██▊       | 28454/100000 [00:27<01:09, 1036.50 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  29%|██▊       | 28612/100000 [00:27<01:08, 1040.04 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  29%|██▊       | 28719/100000 [00:27<01:08, 1046.55 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  29%|██▉       | 28830/100000 [00:27<01:07, 1058.00 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  29%|██▉       | 28971/100000 [00:28<01:10, 1012.88 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  29%|██▉       | 29084/100000 [00:28<01:08, 1041.24 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  29%|██▉       | 29238/100000 [00:28<01:08, 1033.21 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  29%|██▉       | 29352/100000 [00:28<01:06, 1057.65 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  29%|██▉       | 29466/100000 [00:28<01:05, 1077.29 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  30%|██▉       | 29618/100000 [00:28<01:07, 1048.30 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  30%|██▉       | 29727/100000 [00:28<01:06, 1057.53 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  30%|██▉       | 29834/100000 [00:28<01:06, 1056.32 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  30%|██▉       | 29994/100000 [00:29<01:06, 1050.47 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  30%|███       | 30152/100000 [00:29<01:07, 1040.33 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  30%|███       | 30258/100000 [00:29<01:06, 1041.71 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  30%|███       | 30363/100000 [00:29<01:06, 1041.18 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  30%|███       | 30471/100000 [00:29<01:06, 1048.70 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  31%|███       | 30578/100000 [00:29<01:05, 1052.50 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  31%|███       | 30743/100000 [00:29<01:04, 1067.15 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  31%|███       | 30852/100000 [00:29<01:04, 1071.75 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  31%|███       | 30961/100000 [00:29<01:04, 1071.79 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  31%|███       | 31114/100000 [00:30<01:05, 1049.30 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  31%|███       | 31221/100000 [00:30<01:05, 1050.99 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  31%|███▏      | 31332/100000 [00:30<01:04, 1063.34 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  31%|███▏      | 31481/100000 [00:30<01:06, 1033.63 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  32%|███▏      | 31589/100000 [00:30<01:05, 1041.50 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  32%|███▏      | 31696/100000 [00:30<01:05, 1043.63 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  32%|███▏      | 31809/100000 [00:30<01:04, 1063.70 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  32%|███▏      | 31916/100000 [00:30<01:04, 1061.46 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  32%|███▏      | 32023/100000 [00:30<01:04, 1060.74 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  32%|███▏      | 32180/100000 [00:31<01:04, 1050.94 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  32%|███▏      | 32288/100000 [00:31<01:04, 1056.54 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  32%|███▏      | 32443/100000 [00:31<01:04, 1041.61 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  33%|███▎      | 32552/100000 [00:31<01:04, 1050.46 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  33%|███▎      | 32708/100000 [00:31<01:04, 1044.39 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  33%|███▎      | 32816/100000 [00:31<01:03, 1052.30 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  33%|███▎      | 32928/100000 [00:31<01:02, 1068.57 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  33%|███▎      | 33038/100000 [00:31<01:02, 1076.41 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  33%|███▎      | 33203/100000 [00:32<01:01, 1080.97 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  33%|███▎      | 33358/100000 [00:32<01:03, 1055.46 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  33%|███▎      | 33470/100000 [00:32<01:02, 1065.17 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  34%|███▎      | 33582/100000 [00:32<01:01, 1078.21 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  34%|███▎      | 33702/100000 [00:32<00:59, 1107.56 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  34%|███▍      | 33862/100000 [00:32<01:00, 1088.55 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  34%|███▍      | 33974/100000 [00:32<01:00, 1093.52 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  34%|███▍      | 34085/100000 [00:32<01:00, 1096.47 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  34%|███▍      | 34243/100000 [00:33<01:01, 1076.56 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  34%|███▍      | 34399/100000 [00:33<01:02, 1057.96 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  35%|███▍      | 34510/100000 [00:33<01:01, 1065.90 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  35%|███▍      | 34677/100000 [00:33<01:00, 1072.03 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  35%|███▍      | 34827/100000 [00:33<01:02, 1046.51 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  35%|███▍      | 34990/100000 [00:33<01:01, 1055.48 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  35%|███▌      | 35141/100000 [00:33<01:02, 1036.74 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  35%|███▌      | 35249/100000 [00:33<01:01, 1045.31 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  35%|███▌      | 35360/100000 [00:34<01:01, 1057.79 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  36%|███▌      | 35512/100000 [00:34<01:02, 1037.51 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  36%|███▌      | 35655/100000 [00:34<01:04, 1001.30 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  36%|███▌      | 35762/100000 [00:34<01:03, 1016.13 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  36%|███▌      | 35878/100000 [00:34<01:01, 1048.29 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  36%|███▌      | 36041/100000 [00:34<01:00, 1059.52 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  36%|███▌      | 36197/100000 [00:34<01:00, 1051.82 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  36%|███▋      | 36304/100000 [00:34<01:00, 1053.36 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  36%|███▋      | 36455/100000 [00:35<01:01, 1033.17 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  37%|███▋      | 36561/100000 [00:35<01:01, 1038.30 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  37%|███▋      | 36674/100000 [00:35<00:59, 1057.88 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  37%|███▋      | 36816/100000 [00:35<01:02, 1011.96 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  37%|███▋      | 36925/100000 [00:35<01:01, 1028.74 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  37%|███▋      | 37037/100000 [00:35<01:00, 1049.01 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  37%|███▋      | 37161/100000 [00:35<00:57, 1099.20 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  37%|███▋      | 37322/100000 [00:35<00:57, 1087.12 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  37%|███▋      | 37476/100000 [00:36<00:58, 1063.34 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  38%|███▊      | 37636/100000 [00:36<00:58, 1057.95 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  38%|███▊      | 37746/100000 [00:36<00:58, 1066.34 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  38%|███▊      | 37856/100000 [00:36<00:57, 1074.24 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  38%|███▊      | 38018/100000 [00:36<00:57, 1072.06 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  38%|███▊      | 38177/100000 [00:36<00:58, 1064.23 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  38%|███▊      | 38340/100000 [00:36<00:58, 1061.38 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  38%|███▊      | 38497/100000 [00:37<00:58, 1054.90 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  39%|███▊      | 38615/100000 [00:37<00:56, 1077.23 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  39%|███▉      | 38772/100000 [00:37<00:57, 1062.89 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  39%|███▉      | 38930/100000 [00:37<00:57, 1055.78 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  39%|███▉      | 39040/100000 [00:37<00:57, 1065.61 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  39%|███▉      | 39152/100000 [00:37<00:56, 1074.53 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  39%|███▉      | 39312/100000 [00:37<00:56, 1070.19 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  39%|███▉      | 39470/100000 [00:37<00:57, 1058.38 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  40%|███▉      | 39622/100000 [00:38<00:57, 1042.25 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  40%|███▉      | 39734/100000 [00:38<00:57, 1056.43 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  40%|███▉      | 39849/100000 [00:38<00:56, 1072.92 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  40%|████      | 40008/100000 [00:38<00:56, 1066.84 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  40%|████      | 40162/100000 [00:38<00:57, 1047.15 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  40%|████      | 40277/100000 [00:38<00:55, 1069.19 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  40%|████      | 40386/100000 [00:38<00:55, 1069.47 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  41%|████      | 40549/100000 [00:38<00:55, 1071.40 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  41%|████      | 40697/100000 [00:39<00:57, 1038.78 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  41%|████      | 40854/100000 [00:39<00:57, 1037.21 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  41%|████      | 40959/100000 [00:39<00:56, 1039.09 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  41%|████      | 41117/100000 [00:39<00:56, 1035.63 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  41%|████▏     | 41275/100000 [00:39<00:56, 1038.96 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  41%|████▏     | 41384/100000 [00:39<00:55, 1049.38 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  41%|████▏     | 41490/100000 [00:39<00:56, 1044.71 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  42%|████▏     | 41602/100000 [00:39<00:54, 1061.95 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  42%|████▏     | 41709/100000 [00:40<00:54, 1063.13 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  42%|████▏     | 41864/100000 [00:40<00:55, 1045.90 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  42%|████▏     | 41971/100000 [00:40<00:55, 1047.77 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  42%|████▏     | 42136/100000 [00:40<00:54, 1060.41 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  42%|████▏     | 42248/100000 [00:40<00:53, 1069.95 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  42%|████▏     | 42402/100000 [00:40<00:54, 1053.23 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  43%|████▎     | 42559/100000 [00:40<00:54, 1048.58 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  43%|████▎     | 42670/100000 [00:41<00:54, 1057.78 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  43%|████▎     | 42827/100000 [00:41<00:54, 1046.08 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  43%|████▎     | 42985/100000 [00:41<00:54, 1045.98 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  43%|████▎     | 43102/100000 [00:41<00:52, 1074.79 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  43%|████▎     | 43222/100000 [00:41<00:51, 1104.00 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  43%|████▎     | 43375/100000 [00:41<00:52, 1069.62 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  44%|████▎     | 43527/100000 [00:41<00:53, 1048.60 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  44%|████▎     | 43638/100000 [00:41<00:53, 1061.32 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  44%|████▍     | 43788/100000 [00:42<00:54, 1038.85 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  44%|████▍     | 43894/100000 [00:42<00:54, 1035.88 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  44%|████▍     | 44050/100000 [00:42<00:54, 1032.77 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  44%|████▍     | 44208/100000 [00:42<00:53, 1036.29 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  44%|████▍     | 44357/100000 [00:42<00:54, 1020.07 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  44%|████▍     | 44460/100000 [00:42<00:54, 1017.48 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  45%|████▍     | 44562/100000 [00:42<00:54, 1015.08 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  45%|████▍     | 44667/100000 [00:42<00:54, 1023.08 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  45%|████▍     | 44773/100000 [00:43<00:53, 1027.48 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  45%|████▍     | 44884/100000 [00:43<00:52, 1044.66 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  45%|████▍     | 44990/100000 [00:43<00:52, 1042.08 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  45%|████▌     | 45105/100000 [00:43<00:51, 1070.18 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  45%|████▌     | 45255/100000 [00:43<00:52, 1034.41 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  45%|████▌     | 45370/100000 [00:43<00:51, 1061.25 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  45%|████▌     | 45486/100000 [00:43<00:50, 1085.45 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  46%|████▌     | 45599/100000 [00:43<00:49, 1094.71 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  46%|████▌     | 45711/100000 [00:43<00:49, 1090.98 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  46%|████▌     | 45871/100000 [00:44<00:50, 1077.02 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  46%|████▌     | 45985/100000 [00:44<00:49, 1090.55 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  46%|████▌     | 46097/100000 [00:44<00:49, 1089.06 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  46%|████▋     | 46262/100000 [00:44<00:49, 1090.25 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  46%|████▋     | 46422/100000 [00:44<00:49, 1078.38 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  47%|████▋     | 46531/100000 [00:44<00:49, 1078.30 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  47%|████▋     | 46644/100000 [00:44<00:49, 1087.34 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  47%|████▋     | 46793/100000 [00:44<00:50, 1047.88 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  47%|████▋     | 46952/100000 [00:45<00:50, 1044.72 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  47%|████▋     | 47063/100000 [00:45<00:50, 1058.24 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  47%|████▋     | 47171/100000 [00:45<00:49, 1061.77 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  47%|████▋     | 47280/100000 [00:45<00:49, 1067.86 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  47%|████▋     | 47397/100000 [00:45<00:48, 1094.42 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  48%|████▊     | 47509/100000 [00:45<00:47, 1097.85 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  48%|████▊     | 47625/100000 [00:45<00:47, 1113.21 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  48%|████▊     | 47777/100000 [00:45<00:48, 1071.78 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  48%|████▊     | 47933/100000 [00:45<00:49, 1056.00 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  48%|████▊     | 48045/100000 [00:46<00:48, 1069.50 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  48%|████▊     | 48190/100000 [00:46<00:50, 1027.30 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  48%|████▊     | 48342/100000 [00:46<00:50, 1017.52 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  48%|████▊     | 48459/100000 [00:46<00:48, 1052.26 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  49%|████▊     | 48570/100000 [00:46<00:48, 1065.14 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  49%|████▊     | 48730/100000 [00:46<00:48, 1058.88 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  49%|████▉     | 48872/100000 [00:46<00:50, 1017.76 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  49%|████▉     | 48986/100000 [00:46<00:48, 1044.72 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  49%|████▉     | 49133/100000 [00:47<00:49, 1020.22 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  49%|████▉     | 49250/100000 [00:47<00:48, 1046.15 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  49%|████▉     | 49360/100000 [00:47<00:47, 1056.26 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  50%|████▉     | 49529/100000 [00:47<00:46, 1077.99 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  50%|████▉     | 49690/100000 [00:47<00:46, 1070.81 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  50%|████▉     | 49806/100000 [00:47<00:46, 1088.99 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  50%|████▉     | 49917/100000 [00:47<00:45, 1089.26 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  50%|█████     | 50062/100000 [00:48<00:48, 1040.23 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  50%|█████     | 50169/100000 [00:48<00:47, 1045.42 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  50%|█████     | 50278/100000 [00:48<00:47, 1055.12 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  50%|█████     | 50437/100000 [00:48<00:47, 1054.18 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  51%|█████     | 50596/100000 [00:48<00:46, 1052.35 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  51%|█████     | 50748/100000 [00:48<00:47, 1035.74 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  51%|█████     | 50857/100000 [00:48<00:47, 1040.57 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  51%|█████     | 51020/100000 [00:48<00:46, 1050.53 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  51%|█████     | 51181/100000 [00:49<00:46, 1055.76 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  51%|█████▏    | 51349/100000 [00:49<00:45, 1073.09 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  51%|█████▏    | 51457/100000 [00:49<00:45, 1072.98 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  52%|█████▏    | 51620/100000 [00:49<00:45, 1072.60 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  52%|█████▏    | 51735/100000 [00:49<00:44, 1089.38 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  52%|█████▏    | 51888/100000 [00:49<00:45, 1060.66 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  52%|█████▏    | 51996/100000 [00:49<00:45, 1061.76 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  52%|█████▏    | 52149/100000 [00:49<00:45, 1045.58 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  52%|█████▏    | 52256/100000 [00:50<00:45, 1049.85 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  52%|█████▏    | 52418/100000 [00:50<00:45, 1055.51 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  53%|█████▎    | 52569/100000 [00:50<00:45, 1034.80 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  53%|█████▎    | 52679/100000 [00:50<00:45, 1048.59 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  53%|█████▎    | 52837/100000 [00:50<00:45, 1045.27 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  53%|█████▎    | 52948/100000 [00:50<00:44, 1058.27 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  53%|█████▎    | 53060/100000 [00:50<00:43, 1071.64 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  53%|█████▎    | 53179/100000 [00:50<00:42, 1101.34 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  53%|█████▎    | 53328/100000 [00:51<00:44, 1057.57 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  53%|█████▎    | 53436/100000 [00:51<00:43, 1061.89 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  54%|█████▎    | 53600/100000 [00:51<00:43, 1065.23 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  54%|█████▍    | 53764/100000 [00:51<00:43, 1067.00 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  54%|█████▍    | 53872/100000 [00:51<00:43, 1068.69 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  54%|█████▍    | 54030/100000 [00:51<00:43, 1057.23 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  54%|█████▍    | 54182/100000 [00:51<00:44, 1037.92 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  54%|█████▍    | 54340/100000 [00:52<00:43, 1040.27 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  54%|█████▍    | 54456/100000 [00:52<00:42, 1064.11 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  55%|█████▍    | 54565/100000 [00:52<00:42, 1068.78 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  55%|█████▍    | 54729/100000 [00:52<00:42, 1070.99 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  55%|█████▍    | 54880/100000 [00:52<00:43, 1043.93 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  55%|█████▍    | 54988/100000 [00:52<00:42, 1052.00 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  55%|█████▌    | 55095/100000 [00:52<00:42, 1055.70 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  55%|█████▌    | 55207/100000 [00:52<00:42, 1064.35 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  55%|█████▌    | 55320/100000 [00:52<00:41, 1075.33 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  55%|█████▌    | 55479/100000 [00:53<00:41, 1067.10 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  56%|█████▌    | 55641/100000 [00:53<00:41, 1064.49 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  56%|█████▌    | 55793/100000 [00:53<00:42, 1042.40 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  56%|█████▌    | 55946/100000 [00:53<00:42, 1031.74 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  56%|█████▌    | 56050/100000 [00:53<00:42, 1028.08 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  56%|█████▌    | 56156/100000 [00:53<00:42, 1034.53 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  56%|█████▋    | 56260/100000 [00:53<00:42, 1028.83 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  56%|█████▋    | 56373/100000 [00:53<00:41, 1049.73 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  57%|█████▋    | 56530/100000 [00:54<00:41, 1043.62 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  57%|█████▋    | 56645/100000 [00:54<00:40, 1068.57 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  57%|█████▋    | 56808/100000 [00:54<00:40, 1072.99 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  57%|█████▋    | 56916/100000 [00:54<00:40, 1072.97 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  57%|█████▋    | 57024/100000 [00:54<00:40, 1071.18 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  57%|█████▋    | 57132/100000 [00:54<00:39, 1072.20 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  57%|█████▋    | 57287/100000 [00:54<00:40, 1051.51 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  57%|█████▋    | 57438/100000 [00:54<00:41, 1029.22 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  58%|█████▊    | 57543/100000 [00:55<00:41, 1031.97 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  58%|█████▊    | 57694/100000 [00:55<00:41, 1022.43 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  58%|█████▊    | 57800/100000 [00:55<00:41, 1025.05 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  58%|█████▊    | 57954/100000 [00:55<00:41, 1023.58 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  58%|█████▊    | 58112/100000 [00:55<00:40, 1030.45 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  58%|█████▊    | 58217/100000 [00:55<00:40, 1028.60 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  58%|█████▊    | 58320/100000 [00:55<00:40, 1022.49 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  58%|█████▊    | 58479/100000 [00:56<00:40, 1030.23 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  59%|█████▊    | 58596/100000 [00:56<00:38, 1062.05 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  59%|█████▉    | 58752/100000 [00:56<00:39, 1052.61 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  59%|█████▉    | 58860/100000 [00:56<00:38, 1057.20 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  59%|█████▉    | 58970/100000 [00:56<00:38, 1064.19 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  59%|█████▉    | 59130/100000 [00:56<00:38, 1057.17 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  59%|█████▉    | 59241/100000 [00:56<00:38, 1068.06 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  59%|█████▉    | 59353/100000 [00:56<00:37, 1080.61 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  59%|█████▉    | 59471/100000 [00:56<00:36, 1103.02 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  60%|█████▉    | 59633/100000 [00:57<00:37, 1089.24 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  60%|█████▉    | 59746/100000 [00:57<00:36, 1095.88 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  60%|█████▉    | 59908/100000 [00:57<00:36, 1087.91 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  60%|██████    | 60060/100000 [00:57<00:37, 1055.25 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  60%|██████    | 60169/100000 [00:57<00:37, 1059.90 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  60%|██████    | 60330/100000 [00:57<00:37, 1061.25 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  60%|██████    | 60448/100000 [00:57<00:36, 1088.70 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  61%|██████    | 60602/100000 [00:57<00:36, 1065.23 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  61%|██████    | 60758/100000 [00:58<00:37, 1053.39 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  61%|██████    | 60917/100000 [00:58<00:37, 1051.62 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  61%|██████    | 61026/100000 [00:58<00:36, 1058.44 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  61%|██████    | 61179/100000 [00:58<00:37, 1042.01 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  61%|██████▏   | 61287/100000 [00:58<00:36, 1046.84 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  61%|██████▏   | 61441/100000 [00:58<00:37, 1032.08 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  62%|██████▏   | 61550/100000 [00:58<00:36, 1039.60 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  62%|██████▏   | 61657/100000 [00:58<00:36, 1043.51 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  62%|██████▏   | 61814/100000 [00:59<00:36, 1038.43 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  62%|██████▏   | 61926/100000 [00:59<00:36, 1055.76 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  62%|██████▏   | 62083/100000 [00:59<00:36, 1045.14 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  62%|██████▏   | 62194/100000 [00:59<00:35, 1058.83 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  62%|██████▏   | 62304/100000 [00:59<00:35, 1064.83 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  62%|██████▏   | 62462/100000 [00:59<00:35, 1056.55 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  63%|██████▎   | 62612/100000 [00:59<00:36, 1035.04 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  63%|██████▎   | 62726/100000 [01:00<00:35, 1055.78 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  63%|██████▎   | 62872/100000 [01:00<00:36, 1026.34 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  63%|██████▎   | 62987/100000 [01:00<00:35, 1055.11 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  63%|██████▎   | 63101/100000 [01:00<00:34, 1074.18 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  63%|██████▎   | 63210/100000 [01:00<00:34, 1076.46 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  63%|██████▎   | 63373/100000 [01:00<00:34, 1074.02 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  64%|██████▎   | 63525/100000 [01:00<00:34, 1048.79 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  64%|██████▎   | 63678/100000 [01:00<00:35, 1034.50 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  64%|██████▍   | 63841/100000 [01:01<00:34, 1042.74 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  64%|██████▍   | 63946/100000 [01:01<00:34, 1042.73 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  64%|██████▍   | 64094/100000 [01:01<00:35, 1021.13 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  64%|██████▍   | 64204/100000 [01:01<00:34, 1039.51 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  64%|██████▍   | 64311/100000 [01:01<00:34, 1045.09 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  64%|██████▍   | 64470/100000 [01:01<00:33, 1045.98 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  65%|██████▍   | 64577/100000 [01:01<00:33, 1049.81 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  65%|██████▍   | 64741/100000 [01:01<00:33, 1062.10 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  65%|██████▍   | 64894/100000 [01:02<00:33, 1046.22 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  65%|██████▌   | 65041/100000 [01:02<00:34, 1021.57 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  65%|██████▌   | 65150/100000 [01:02<00:33, 1033.67 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  65%|██████▌   | 65260/100000 [01:02<00:33, 1046.06 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  65%|██████▌   | 65413/100000 [01:02<00:33, 1034.63 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  66%|██████▌   | 65523/100000 [01:02<00:32, 1048.38 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  66%|██████▌   | 65635/100000 [01:02<00:32, 1061.86 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  66%|██████▌   | 65753/100000 [01:02<00:31, 1090.63 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  66%|██████▌   | 65917/100000 [01:03<00:31, 1082.40 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  66%|██████▌   | 66064/100000 [01:03<00:32, 1045.04 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  66%|██████▌   | 66180/100000 [01:03<00:31, 1070.29 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  66%|██████▋   | 66338/100000 [01:03<00:31, 1053.15 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  66%|██████▋   | 66489/100000 [01:03<00:32, 1032.35 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  67%|██████▋   | 66649/100000 [01:03<00:32, 1038.36 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  67%|██████▋   | 66754/100000 [01:03<00:32, 1038.05 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  67%|██████▋   | 66900/100000 [01:04<00:32, 1013.45 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  67%|██████▋   | 67006/100000 [01:04<00:32, 1022.62 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  67%|██████▋   | 67120/100000 [01:04<00:31, 1048.01 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  67%|██████▋   | 67227/100000 [01:04<00:31, 1051.32 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  67%|██████▋   | 67336/100000 [01:04<00:30, 1056.61 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  67%|██████▋   | 67492/100000 [01:04<00:31, 1039.93 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  68%|██████▊   | 67650/100000 [01:04<00:31, 1042.27 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  68%|██████▊   | 67802/100000 [01:04<00:31, 1027.08 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  68%|██████▊   | 67908/100000 [01:04<00:31, 1029.37 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  68%|██████▊   | 68052/100000 [01:05<00:31, 999.72 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  68%|██████▊   | 68169/100000 [01:05<00:30, 1039.09 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  68%|██████▊   | 68312/100000 [01:05<00:31, 1005.05 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  68%|██████▊   | 68417/100000 [01:05<00:31, 1012.87 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  69%|██████▊   | 68530/100000 [01:05<00:30, 1040.34 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  69%|██████▊   | 68651/100000 [01:05<00:28, 1081.61 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  69%|██████▉   | 68814/100000 [01:05<00:28, 1079.26 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  69%|██████▉   | 68971/100000 [01:05<00:29, 1063.42 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  69%|██████▉   | 69080/100000 [01:06<00:29, 1060.23 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  69%|██████▉   | 69188/100000 [01:06<00:29, 1061.25 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  69%|██████▉   | 69348/100000 [01:06<00:28, 1057.63 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  69%|██████▉   | 69455/100000 [01:06<00:28, 1059.60 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  70%|██████▉   | 69612/100000 [01:06<00:28, 1053.22 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  70%|██████▉   | 69725/100000 [01:06<00:28, 1067.75 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  70%|██████▉   | 69837/100000 [01:06<00:28, 1076.92 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  70%|██████▉   | 69999/100000 [01:06<00:27, 1071.61 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  70%|███████   | 70155/100000 [01:07<00:28, 1055.92 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  70%|███████   | 70266/100000 [01:07<00:28, 1060.10 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  70%|███████   | 70390/100000 [01:07<00:26, 1104.50 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  71%|███████   | 70548/100000 [01:07<00:27, 1080.80 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  71%|███████   | 70661/100000 [01:07<00:26, 1090.22 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  71%|███████   | 70821/100000 [01:07<00:27, 1076.31 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  71%|███████   | 70987/100000 [01:07<00:26, 1084.56 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  71%|███████   | 71146/100000 [01:08<00:26, 1072.05 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  71%|███████▏  | 71295/100000 [01:08<00:27, 1046.21 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  71%|███████▏  | 71403/100000 [01:08<00:27, 1051.85 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  72%|███████▏  | 71559/100000 [01:08<00:27, 1044.63 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  72%|███████▏  | 71666/100000 [01:08<00:27, 1046.09 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  72%|███████▏  | 71776/100000 [01:08<00:26, 1057.40 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  72%|███████▏  | 71942/100000 [01:08<00:26, 1071.48 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  72%|███████▏  | 72093/100000 [01:08<00:26, 1048.94 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  72%|███████▏  | 72208/100000 [01:09<00:25, 1071.40 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  72%|███████▏  | 72354/100000 [01:09<00:26, 1034.33 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  73%|███████▎  | 72513/100000 [01:09<00:26, 1040.26 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  73%|███████▎  | 72669/100000 [01:09<00:26, 1035.67 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  73%|███████▎  | 72825/100000 [01:09<00:26, 1031.46 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  73%|███████▎  | 72935/100000 [01:09<00:25, 1045.73 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  73%|███████▎  | 73041/100000 [01:09<00:25, 1047.38 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  73%|███████▎  | 73149/100000 [01:09<00:25, 1054.40 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  73%|███████▎  | 73261/100000 [01:10<00:24, 1070.52 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  73%|███████▎  | 73420/100000 [01:10<00:24, 1063.38 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  74%|███████▎  | 73568/100000 [01:10<00:25, 1031.27 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  74%|███████▎  | 73674/100000 [01:10<00:25, 1032.72 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  74%|███████▍  | 73827/100000 [01:10<00:25, 1026.78 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  74%|███████▍  | 73939/100000 [01:10<00:24, 1046.15 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  74%|███████▍  | 74050/100000 [01:10<00:24, 1059.14 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  74%|███████▍  | 74165/100000 [01:10<00:23, 1078.20 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  74%|███████▍  | 74322/100000 [01:11<00:24, 1062.24 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  74%|███████▍  | 74430/100000 [01:11<00:24, 1061.30 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  75%|███████▍  | 74594/100000 [01:11<00:23, 1063.87 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  75%|███████▍  | 74751/100000 [01:11<00:23, 1052.50 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  75%|███████▍  | 74909/100000 [01:11<00:23, 1050.92 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  75%|███████▌  | 75016/100000 [01:11<00:23, 1052.75 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  75%|███████▌  | 75122/100000 [01:11<00:23, 1050.61 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  75%|███████▌  | 75231/100000 [01:11<00:23, 1057.26 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  75%|███████▌  | 75340/100000 [01:12<00:23, 1059.68 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  75%|███████▌  | 75493/100000 [01:12<00:23, 1042.66 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  76%|███████▌  | 75653/100000 [01:12<00:23, 1048.34 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  76%|███████▌  | 75803/100000 [01:12<00:23, 1029.34 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  76%|███████▌  | 75952/100000 [01:12<00:23, 1015.36 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  76%|███████▌  | 76067/100000 [01:12<00:22, 1042.57 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  76%|███████▌  | 76220/100000 [01:12<00:23, 1022.48 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  76%|███████▋  | 76330/100000 [01:12<00:22, 1032.39 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  76%|███████▋  | 76476/100000 [01:13<00:23, 1008.59 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  77%|███████▋  | 76632/100000 [01:13<00:23, 1014.06 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  77%|███████▋  | 76741/100000 [01:13<00:22, 1028.34 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  77%|███████▋  | 76896/100000 [01:13<00:22, 1025.47 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  77%|███████▋  | 77005/100000 [01:13<00:22, 1038.07 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  77%|███████▋  | 77112/100000 [01:13<00:21, 1043.50 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  77%|███████▋  | 77219/100000 [01:13<00:21, 1040.64 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  77%|███████▋  | 77374/100000 [01:13<00:21, 1034.69 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  77%|███████▋  | 77480/100000 [01:14<00:21, 1036.44 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  78%|███████▊  | 77637/100000 [01:14<00:21, 1037.81 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  78%|███████▊  | 77786/100000 [01:14<00:21, 1021.05 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  78%|███████▊  | 77889/100000 [01:14<00:21, 1022.02 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  78%|███████▊  | 78046/100000 [01:14<00:21, 1022.51 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  78%|███████▊  | 78201/100000 [01:14<00:21, 1024.39 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  78%|███████▊  | 78359/100000 [01:14<00:20, 1030.65 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  78%|███████▊  | 78475/100000 [01:15<00:20, 1054.00 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  79%|███████▊  | 78584/100000 [01:15<00:20, 1061.11 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  79%|███████▊  | 78741/100000 [01:15<00:20, 1050.68 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  79%|███████▉  | 78856/100000 [01:15<00:19, 1071.74 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  79%|███████▉  | 79019/100000 [01:15<00:19, 1072.61 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  79%|███████▉  | 79127/100000 [01:15<00:19, 1069.69 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  79%|███████▉  | 79235/100000 [01:15<00:19, 1068.29 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  79%|███████▉  | 79345/100000 [01:15<00:19, 1075.57 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  80%|███████▉  | 79500/100000 [01:16<00:19, 1054.74 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  80%|███████▉  | 79609/100000 [01:16<00:19, 1059.49 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  80%|███████▉  | 79718/100000 [01:16<00:19, 1063.68 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  80%|███████▉  | 79826/100000 [01:16<00:19, 1051.72 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  80%|███████▉  | 79943/100000 [01:16<00:18, 1081.83 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  80%|████████  | 80058/100000 [01:16<00:18, 1097.47 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  80%|████████  | 80224/100000 [01:16<00:18, 1097.72 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  80%|████████  | 80386/100000 [01:16<00:18, 1083.09 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  81%|████████  | 80542/100000 [01:16<00:18, 1063.91 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  81%|████████  | 80652/100000 [01:17<00:18, 1068.31 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  81%|████████  | 80760/100000 [01:17<00:18, 1067.54 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  81%|████████  | 80926/100000 [01:17<00:17, 1077.66 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  81%|████████  | 81035/100000 [01:17<00:17, 1079.43 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  81%|████████  | 81186/100000 [01:17<00:17, 1049.65 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  81%|████████▏ | 81293/100000 [01:17<00:17, 1051.20 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  81%|████████▏ | 81450/100000 [01:17<00:17, 1041.47 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  82%|████████▏ | 81559/100000 [01:17<00:17, 1050.79 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  82%|████████▏ | 81669/100000 [01:18<00:17, 1059.79 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  82%|████████▏ | 81828/100000 [01:18<00:17, 1058.64 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  82%|████████▏ | 81939/100000 [01:18<00:16, 1067.07 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  82%|████████▏ | 82100/100000 [01:18<00:16, 1066.43 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  82%|████████▏ | 82256/100000 [01:18<00:16, 1049.52 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  82%|████████▏ | 82362/100000 [01:18<00:16, 1050.03 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  83%|████████▎ | 82519/100000 [01:18<00:16, 1043.46 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  83%|████████▎ | 82670/100000 [01:19<00:16, 1025.91 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  83%|████████▎ | 82784/100000 [01:19<00:16, 1049.55 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  83%|████████▎ | 82940/100000 [01:19<00:16, 1039.29 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  83%|████████▎ | 83049/100000 [01:19<00:16, 1047.29 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  83%|████████▎ | 83160/100000 [01:19<00:15, 1060.65 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  83%|████████▎ | 83269/100000 [01:19<00:15, 1066.81 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  83%|████████▎ | 83424/100000 [01:19<00:15, 1051.43 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  84%|████████▎ | 83534/100000 [01:19<00:15, 1062.37 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  84%|████████▎ | 83649/100000 [01:19<00:15, 1084.22 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  84%|████████▍ | 83799/100000 [01:20<00:15, 1050.57 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  84%|████████▍ | 83911/100000 [01:20<00:15, 1066.94 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  84%|████████▍ | 84070/100000 [01:20<00:15, 1058.46 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  84%|████████▍ | 84178/100000 [01:20<00:14, 1061.08 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  84%|████████▍ | 84342/100000 [01:20<00:14, 1066.50 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  84%|████████▍ | 84454/100000 [01:20<00:14, 1074.68 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  85%|████████▍ | 84567/100000 [01:20<00:14, 1087.37 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  85%|████████▍ | 84679/100000 [01:20<00:14, 1088.68 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  85%|████████▍ | 84840/100000 [01:21<00:14, 1076.61 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  85%|████████▍ | 84987/100000 [01:21<00:14, 1038.71 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  85%|████████▌ | 85139/100000 [01:21<00:14, 1022.28 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  85%|████████▌ | 85296/100000 [01:21<00:14, 1026.41 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  85%|████████▌ | 85403/100000 [01:21<00:14, 1034.46 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  86%|████████▌ | 85508/100000 [01:21<00:14, 1034.05 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  86%|████████▌ | 85654/100000 [01:21<00:14, 1009.26 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  86%|████████▌ | 85760/100000 [01:21<00:13, 1019.90 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  86%|████████▌ | 85869/100000 [01:22<00:13, 1035.16 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  86%|████████▌ | 86027/100000 [01:22<00:13, 1033.14 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  86%|████████▌ | 86136/100000 [01:22<00:13, 1044.89 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  86%|████████▋ | 86286/100000 [01:22<00:13, 1018.50 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  86%|████████▋ | 86394/100000 [01:22<00:13, 1030.70 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  87%|████████▋ | 86504/100000 [01:22<00:12, 1046.31 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  87%|████████▋ | 86614/100000 [01:22<00:12, 1059.00 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  87%|████████▋ | 86761/100000 [01:22<00:12, 1026.25 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  87%|████████▋ | 86866/100000 [01:23<00:12, 1032.21 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  87%|████████▋ | 87021/100000 [01:23<00:12, 1025.51 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  87%|████████▋ | 87125/100000 [01:23<00:12, 1026.90 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  87%|████████▋ | 87282/100000 [01:23<00:12, 1029.29 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  87%|████████▋ | 87401/100000 [01:23<00:11, 1065.26 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  88%|████████▊ | 87550/100000 [01:23<00:12, 1036.19 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  88%|████████▊ | 87660/100000 [01:23<00:11, 1049.41 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  88%|████████▊ | 87770/100000 [01:23<00:11, 1056.40 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  88%|████████▊ | 87881/100000 [01:23<00:11, 1068.54 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  88%|████████▊ | 88036/100000 [01:24<00:11, 1051.94 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  88%|████████▊ | 88186/100000 [01:24<00:11, 1031.23 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  88%|████████▊ | 88294/100000 [01:24<00:11, 1038.16 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  88%|████████▊ | 88400/100000 [01:24<00:11, 1039.16 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  89%|████████▊ | 88510/100000 [01:24<00:10, 1051.34 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  89%|████████▊ | 88660/100000 [01:24<00:11, 1024.56 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  89%|████████▉ | 88811/100000 [01:24<00:11, 1015.22 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  89%|████████▉ | 88925/100000 [01:24<00:10, 1039.35 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  89%|████████▉ | 89078/100000 [01:25<00:10, 1027.95 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  89%|████████▉ | 89186/100000 [01:25<00:10, 1038.65 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  89%|████████▉ | 89291/100000 [01:25<00:10, 1038.51 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  89%|████████▉ | 89445/100000 [01:25<00:10, 1033.81 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  90%|████████▉ | 89600/100000 [01:25<00:10, 1024.13 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  90%|████████▉ | 89739/100000 [01:25<00:10, 987.36 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  90%|████████▉ | 89844/100000 [01:25<00:10, 998.42 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  90%|████████▉ | 89947/100000 [01:25<00:10, 1003.58 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  90%|█████████ | 90090/100000 [01:26<00:10, 982.89 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  90%|█████████ | 90202/100000 [01:26<00:09, 1013.91 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  90%|█████████ | 90314/100000 [01:26<00:09, 1040.25 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  90%|█████████ | 90421/100000 [01:26<00:09, 1044.86 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  91%|█████████ | 90570/100000 [01:26<00:09, 1021.87 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  91%|█████████ | 90682/100000 [01:26<00:08, 1045.07 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  91%|█████████ | 90790/100000 [01:26<00:08, 1045.18 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  91%|█████████ | 90896/100000 [01:26<00:08, 1046.11 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  91%|█████████ | 91057/100000 [01:27<00:08, 1048.70 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  91%|█████████ | 91210/100000 [01:27<00:08, 1028.88 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  91%|█████████▏| 91319/100000 [01:27<00:08, 1033.16 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  91%|█████████▏| 91430/100000 [01:27<00:08, 1048.37 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  92%|█████████▏| 91537/100000 [01:27<00:08, 1053.47 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  92%|█████████▏| 91649/100000 [01:27<00:07, 1067.08 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  92%|█████████▏| 91790/100000 [01:27<00:08, 1012.79 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  92%|█████████▏| 91936/100000 [01:27<00:08, 997.41 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  92%|█████████▏| 92042/100000 [01:28<00:07, 1009.92 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  92%|█████████▏| 92191/100000 [01:28<00:07, 999.10 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  92%|█████████▏| 92292/100000 [01:28<00:07, 999.77 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  92%|█████████▏| 92403/100000 [01:28<00:07, 1025.53 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  93%|█████████▎| 92508/100000 [01:28<00:07, 1025.74 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  93%|█████████▎| 92614/100000 [01:28<00:07, 1033.36 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  93%|█████████▎| 92723/100000 [01:28<00:06, 1048.79 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  93%|█████████▎| 92836/100000 [01:28<00:06, 1067.99 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  93%|█████████▎| 92995/100000 [01:28<00:06, 1060.93 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  93%|█████████▎| 93106/100000 [01:29<00:06, 1072.06 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  93%|█████████▎| 93262/100000 [01:29<00:06, 1057.77 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  93%|█████████▎| 93416/100000 [01:29<00:06, 1041.29 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  94%|█████████▎| 93529/100000 [01:29<00:06, 1059.40 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  94%|█████████▎| 93641/100000 [01:29<00:05, 1071.20 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  94%|█████████▍| 93755/100000 [01:29<00:05, 1085.12 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  94%|█████████▍| 93867/100000 [01:29<00:05, 1088.88 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  94%|█████████▍| 94024/100000 [01:29<00:05, 1067.25 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  94%|█████████▍| 94181/100000 [01:30<00:05, 1057.82 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  94%|█████████▍| 94340/100000 [01:30<00:05, 1053.66 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  94%|█████████▍| 94496/100000 [01:30<00:05, 1046.18 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  95%|█████████▍| 94647/100000 [01:30<00:05, 1030.40 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  95%|█████████▍| 94797/100000 [01:30<00:05, 1014.44 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  95%|█████████▍| 94950/100000 [01:30<00:04, 1013.93 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  95%|█████████▌| 95057/100000 [01:30<00:04, 1024.29 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  95%|█████████▌| 95169/100000 [01:31<00:04, 1042.71 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  95%|█████████▌| 95279/100000 [01:31<00:04, 1056.03 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  95%|█████████▌| 95386/100000 [01:31<00:04, 1053.97 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  96%|█████████▌| 95552/100000 [01:31<00:04, 1069.07 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  96%|█████████▌| 95665/100000 [01:31<00:04, 1079.65 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  96%|█████████▌| 95825/100000 [01:31<00:03, 1071.48 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  96%|█████████▌| 95937/100000 [01:31<00:03, 1080.39 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  96%|█████████▌| 96046/100000 [01:31<00:03, 1081.70 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  96%|█████████▌| 96201/100000 [01:31<00:03, 1061.29 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  96%|█████████▋| 96358/100000 [01:32<00:03, 1051.98 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  96%|█████████▋| 96470/100000 [01:32<00:03, 1061.93 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  97%|█████████▋| 96579/100000 [01:32<00:03, 1067.01 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  97%|█████████▋| 96687/100000 [01:32<00:03, 1069.85 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  97%|█████████▋| 96795/100000 [01:32<00:03, 1066.49 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  97%|█████████▋| 96902/100000 [01:32<00:02, 1062.98 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  97%|█████████▋| 97052/100000 [01:32<00:02, 1032.49 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  97%|█████████▋| 97161/100000 [01:32<00:02, 1045.29 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  97%|█████████▋| 97268/100000 [01:32<00:02, 1049.44 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  97%|█████████▋| 97378/100000 [01:33<00:02, 1059.81 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  97%|█████████▋| 97491/100000 [01:33<00:02, 1078.27 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  98%|█████████▊| 97647/100000 [01:33<00:02, 1059.92 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  98%|█████████▊| 97804/100000 [01:33<00:02, 1049.17 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  98%|█████████▊| 97912/100000 [01:33<00:01, 1049.50 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  98%|█████████▊| 98070/100000 [01:33<00:01, 1048.71 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  98%|█████████▊| 98214/100000 [01:33<00:01, 1016.85 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  98%|█████████▊| 98365/100000 [01:34<00:01, 1008.86 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  99%|█████████▊| 98520/100000 [01:34<00:01, 1010.77 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  99%|█████████▊| 98629/100000 [01:34<00:01, 1027.70 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  99%|█████████▊| 98733/100000 [01:34<00:01, 1028.73 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  99%|█████████▉| 98838/100000 [01:34<00:01, 1032.78 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  99%|█████████▉| 98947/100000 [01:34<00:01, 1043.07 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  99%|█████████▉| 99060/100000 [01:34<00:00, 1060.12 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  99%|█████████▉| 99212/100000 [01:34<00:00, 1039.33 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  99%|█████████▉| 99324/100000 [01:34<00:00, 1056.82 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset:  99%|█████████▉| 99480/100000 [01:35<00:00, 1047.90 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset: 100%|█████████▉| 99588/100000 [01:35<00:00, 1054.70 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset: 100%|█████████▉| 99745/100000 [01:35<00:00, 1048.25 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset: 100%|█████████▉| 99889/100000 [01:35<00:00, 1016.51 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset: 100%|█████████▉| 99992/100000 [01:35<00:00, 1017.42 examples/s]\u001b[0m\n",
      "\u001b[34mTokenizing train dataset: 100%|██████████| 100000/100000 [01:35<00:00, 1044.56 examples/s]\u001b[0m\n",
      "\u001b[34m[2025-09-09 17:56:45,422] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mdf: /root/.triton/autotune: No such file or directory\u001b[0m\n",
      "\u001b[34m[2025-09-09 17:56:45,905] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2025-09-09 17:56:45,905] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:   0%|          | 0/100000 [00:00<?, ? examples/s]#015Extracting prompt in train dataset:   0%|          | 0/100000 [00:00<?, ? examples/s]#015Extracting prompt in train dataset:   0%|          | 0/100000 [00:00<?, ? examples/s]#015Extracting prompt in train dataset:   0%|          | 0/100000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:   2%|▏         | 1501/100000 [00:00<00:06, 14931.38 examples/s]#015Extracting prompt in train dataset:   2%|▏         | 1510/100000 [00:00<00:06, 15012.71 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:   2%|▏         | 1500/100000 [00:00<00:06, 14889.19 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:   1%|▏         | 1498/100000 [00:00<00:06, 14853.45 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:   3%|▎         | 3031/100000 [00:00<00:06, 15122.00 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:   3%|▎         | 3040/100000 [00:00<00:06, 15159.15 examples/s]#015Extracting prompt in train dataset:   3%|▎         | 3031/100000 [00:00<00:06, 15113.51 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:   4%|▎         | 3656/100000 [00:00<00:06, 14501.39 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:   5%|▍         | 4560/100000 [00:00<00:06, 15191.30 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:   5%|▍         | 4580/100000 [00:00<00:06, 15246.74 examples/s]#015Extracting prompt in train dataset:   5%|▍         | 4560/100000 [00:00<00:06, 15170.99 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:   6%|▌         | 6118/100000 [00:00<00:06, 15343.93 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:   6%|▌         | 6090/100000 [00:00<00:06, 15212.14 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:   6%|▌         | 5820/100000 [00:00<00:06, 14454.61 examples/s]#015Extracting prompt in train dataset:   6%|▌         | 6139/100000 [00:00<00:06, 15362.88 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:   8%|▊         | 7671/100000 [00:00<00:05, 15398.07 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:   8%|▊         | 7630/100000 [00:00<00:06, 15263.91 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:   8%|▊         | 8413/100000 [00:00<00:05, 15267.49 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:   8%|▊         | 7980/100000 [00:00<00:06, 14408.82 examples/s]\u001b[0m\n",
      "\u001b[34m[2025-09-09 17:56:46,043] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2025-09-09 17:56:46,544] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  10%|▉         | 9947/100000 [00:00<00:05, 15288.17 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  10%|▉         | 9893/100000 [00:00<00:05, 15173.25 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  11%|█         | 10699/100000 [00:00<00:05, 15253.65 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  10%|█         | 10122/100000 [00:00<00:06, 14345.22 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  11%|█▏        | 11480/100000 [00:00<00:05, 15289.93 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  11%|█▏        | 11450/100000 [00:00<00:05, 15275.37 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  12%|█▏        | 12233/100000 [00:00<00:05, 15271.34 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  12%|█▏        | 11561/100000 [00:00<00:06, 14351.92 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  13%|█▎        | 13002/100000 [00:00<00:05, 15344.45 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  14%|█▍        | 13768/100000 [00:00<00:05, 15273.42 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  14%|█▍        | 13762/100000 [00:00<00:05, 15262.32 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  13%|█▎        | 13010/100000 [00:00<00:06, 14375.59 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  15%|█▍        | 14547/100000 [00:00<00:05, 15372.95 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  15%|█▌        | 15290/100000 [00:01<00:05, 15260.69 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  16%|█▌        | 16060/100000 [00:01<00:05, 15262.42 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  15%|█▌        | 15146/100000 [00:01<00:05, 14314.27 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  17%|█▋        | 16821/100000 [00:01<00:05, 15269.91 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  17%|█▋        | 16832/100000 [00:01<00:05, 15313.00 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  18%|█▊        | 17590/100000 [00:01<00:05, 15260.59 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  17%|█▋        | 16610/100000 [00:01<00:05, 14387.04 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  18%|█▊        | 18350/100000 [00:01<00:05, 15272.92 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  18%|█▊        | 18380/100000 [00:01<00:05, 15346.41 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  19%|█▉        | 19118/100000 [00:01<00:05, 15264.18 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  18%|█▊        | 18060/100000 [00:01<00:05, 14405.42 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  20%|█▉        | 19890/100000 [00:01<00:05, 15292.66 examples/s]#015Extracting prompt in train dataset:  20%|█▉        | 19929/100000 [00:01<00:05, 15382.90 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  21%|██        | 20656/100000 [00:01<00:05, 15291.42 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  21%|██▏       | 21436/100000 [00:01<00:05, 15340.80 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  21%|██▏       | 21470/100000 [00:01<00:05, 15377.31 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  20%|██        | 20220/100000 [00:01<00:05, 14384.28 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  22%|██▏       | 22200/100000 [00:01<00:05, 15317.47 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  24%|██▎       | 23740/100000 [00:01<00:04, 15326.14 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  24%|██▎       | 23735/100000 [00:01<00:04, 15324.74 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  24%|██▍       | 23786/100000 [00:01<00:04, 15389.18 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  22%|██▏       | 22360/100000 [00:01<00:05, 14331.56 examples/s]\u001b[0m\n",
      "\u001b[34m[2025-09-09 17:56:47,008] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\u001b[0m\n",
      "\u001b[34m[2025-09-09 17:56:47,010] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\u001b[0m\n",
      "\u001b[34m[2025-09-09 17:56:47,011] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  25%|██▌       | 25275/100000 [00:01<00:04, 15331.71 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  24%|██▍       | 23810/100000 [00:01<00:05, 14358.74 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  26%|██▌       | 26041/100000 [00:01<00:04, 15340.07 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  26%|██▌       | 26100/100000 [00:01<00:04, 15376.29 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  27%|██▋       | 26811/100000 [00:01<00:04, 15338.49 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  25%|██▌       | 25270/100000 [00:01<00:05, 14410.08 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  28%|██▊       | 28347/100000 [00:01<00:04, 15349.43 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  28%|██▊       | 28418/100000 [00:01<00:04, 15400.45 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  27%|██▋       | 26724/100000 [00:01<00:05, 14442.64 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  29%|██▉       | 29096/100000 [00:01<00:04, 15289.70 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  30%|██▉       | 29883/100000 [00:01<00:04, 15350.18 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  31%|███       | 30720/100000 [00:02<00:04, 15368.89 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  29%|██▉       | 28900/100000 [00:02<00:04, 14439.91 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  31%|███▏      | 31380/100000 [00:02<00:04, 15255.22 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  32%|███▏      | 32200/100000 [00:02<00:04, 15370.98 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  32%|███▏      | 32258/100000 [00:02<00:04, 15370.27 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  30%|███       | 30370/100000 [00:02<00:04, 14503.34 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  33%|███▎      | 32920/100000 [00:02<00:04, 15272.42 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  34%|███▎      | 33741/100000 [00:02<00:04, 15372.47 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  32%|███▏      | 31841/100000 [00:02<00:04, 14546.84 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  34%|███▍      | 34450/100000 [00:02<00:04, 15268.63 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  35%|███▍      | 34559/100000 [00:02<00:04, 15349.08 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  35%|███▌      | 35290/100000 [00:02<00:04, 15382.04 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  33%|███▎      | 33303/100000 [00:02<00:04, 14557.45 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  36%|███▌      | 35995/100000 [00:02<00:04, 15312.65 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  36%|███▌      | 36100/100000 [00:02<00:04, 15353.18 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  37%|███▋      | 36829/100000 [00:02<00:04, 15373.13 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  35%|███▍      | 34770/100000 [00:02<00:04, 14580.48 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  38%|███▊      | 37639/100000 [00:02<00:04, 15361.61 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  38%|███▊      | 38300/100000 [00:02<00:04, 15327.92 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  38%|███▊      | 38370/100000 [00:02<00:04, 15360.78 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  36%|███▌      | 36232/100000 [00:02<00:04, 14589.95 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  39%|███▉      | 39190/100000 [00:02<00:03, 15383.87 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  40%|███▉      | 39852/100000 [00:02<00:03, 15368.03 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  40%|███▉      | 39920/100000 [00:02<00:03, 15386.24 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  38%|███▊      | 37703/100000 [00:02<00:04, 14624.62 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  41%|████▏     | 41390/100000 [00:02<00:03, 15370.08 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  42%|████▏     | 41500/100000 [00:02<00:03, 15381.81 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  39%|███▉      | 39170/100000 [00:02<00:04, 14631.69 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  42%|████▏     | 42220/100000 [00:02<00:03, 15360.29 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  43%|████▎     | 42930/100000 [00:02<00:03, 15370.18 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  44%|████▍     | 43800/100000 [00:02<00:03, 15358.22 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  41%|████▏     | 41359/100000 [00:02<00:04, 14614.07 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  44%|████▍     | 44472/100000 [00:02<00:03, 15382.86 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  45%|████▍     | 44518/100000 [00:02<00:03, 15341.89 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  46%|████▌     | 46020/100000 [00:03<00:03, 15397.17 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  46%|████▌     | 46080/100000 [00:03<00:03, 15405.35 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  46%|████▌     | 46120/100000 [00:03<00:03, 15377.26 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  44%|████▎     | 43561/100000 [00:03<00:03, 14619.32 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  48%|████▊     | 47630/100000 [00:03<00:03, 15422.84 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  48%|████▊     | 48310/100000 [00:03<00:03, 15332.07 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  48%|████▊     | 48420/100000 [00:03<00:03, 15352.17 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  46%|████▌     | 45750/100000 [00:03<00:03, 14597.98 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  50%|████▉     | 49856/100000 [00:03<00:03, 15364.77 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  50%|████▉     | 49950/100000 [00:03<00:03, 15426.53 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  50%|████▉     | 49970/100000 [00:03<00:03, 15381.72 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  47%|████▋     | 47220/100000 [00:03<00:03, 14608.55 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  52%|█████▏    | 51500/100000 [00:03<00:03, 15438.04 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  52%|█████▏    | 51520/100000 [00:03<00:03, 15405.66 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  49%|████▊     | 48688/100000 [00:03<00:03, 14625.81 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  52%|█████▏    | 52160/100000 [00:03<00:03, 15349.48 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  50%|█████     | 50161/100000 [00:03<00:03, 14643.58 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  54%|█████▍    | 53819/100000 [00:03<00:02, 15439.73 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  54%|█████▍    | 53839/100000 [00:03<00:02, 15415.21 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  54%|█████▍    | 54448/100000 [00:03<00:02, 15308.12 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  52%|█████▏    | 51630/100000 [00:03<00:03, 14636.89 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  55%|█████▌    | 55370/100000 [00:03<00:02, 15448.69 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  56%|█████▌    | 56150/100000 [00:03<00:02, 15404.58 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  53%|█████▎    | 53107/100000 [00:03<00:03, 14672.54 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  57%|█████▋    | 56737/100000 [00:03<00:02, 15289.40 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  58%|█████▊    | 57674/100000 [00:03<00:02, 15411.65 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  58%|█████▊    | 58449/100000 [00:03<00:02, 15372.64 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  55%|█████▌    | 55292/100000 [00:03<00:03, 14630.40 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  59%|█████▉    | 58948/100000 [00:03<00:02, 15097.72 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  60%|█████▉    | 59950/100000 [00:03<00:02, 15323.28 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  60%|█████▉    | 59990/100000 [00:03<00:02, 15365.27 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  57%|█████▋    | 57481/100000 [00:03<00:02, 14606.02 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  61%|██████    | 61133/100000 [00:04<00:02, 14926.46 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  62%|██████▏   | 62250/100000 [00:04<00:02, 15305.06 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  62%|██████▏   | 62279/100000 [00:04<00:02, 15326.73 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  63%|██████▎   | 62646/100000 [00:04<00:02, 14966.25 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  60%|█████▉    | 59686/100000 [00:04<00:02, 14627.63 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  64%|██████▍   | 64160/100000 [00:04<00:02, 14996.68 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  65%|██████▍   | 64541/100000 [00:04<00:02, 15290.08 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  65%|██████▍   | 64574/100000 [00:04<00:02, 15312.95 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  62%|██████▏   | 61877/100000 [00:04<00:02, 14601.29 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  66%|██████▌   | 66080/100000 [00:04<00:02, 15306.75 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  66%|██████▌   | 66120/100000 [00:04<00:02, 15331.87 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  66%|██████▋   | 66390/100000 [00:04<00:02, 14936.45 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  63%|██████▎   | 63340/100000 [00:04<00:02, 14606.30 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  68%|██████▊   | 68384/100000 [00:04<00:02, 15316.48 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  68%|██████▊   | 68411/100000 [00:04<00:02, 15302.28 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  65%|██████▍   | 64807/100000 [00:04<00:02, 14621.67 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  69%|██████▊   | 68580/100000 [00:04<00:02, 14814.56 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  71%|███████   | 70680/100000 [00:04<00:01, 15300.73 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  71%|███████   | 70704/100000 [00:04<00:01, 15289.70 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  67%|██████▋   | 66995/100000 [00:04<00:02, 14589.32 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  71%|███████   | 70784/100000 [00:04<00:01, 14773.73 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  72%|███████▏  | 72220/100000 [00:04<00:01, 15316.31 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  72%|███████▏  | 72240/100000 [00:04<00:01, 15294.73 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  69%|██████▉   | 69180/100000 [00:04<00:02, 14567.36 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  73%|███████▎  | 72980/100000 [00:04<00:01, 14718.01 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  74%|███████▍  | 73760/100000 [00:04<00:01, 15333.14 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  74%|███████▍  | 73775/100000 [00:04<00:01, 15305.41 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  71%|███████   | 70650/100000 [00:04<00:02, 14591.08 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  75%|███████▌  | 75310/100000 [00:04<00:01, 15305.38 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  75%|███████▌  | 75201/100000 [00:04<00:01, 14739.16 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  76%|███████▌  | 76051/100000 [00:04<00:01, 15302.72 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  73%|███████▎  | 72840/100000 [00:05<00:01, 14582.94 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  78%|███████▊  | 77610/100000 [00:05<00:01, 15303.50 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  77%|███████▋  | 77386/100000 [00:05<00:01, 14684.73 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  78%|███████▊  | 78327/100000 [00:05<00:01, 15250.23 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  75%|███████▌  | 75026/100000 [00:05<00:01, 14569.18 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  80%|███████▉  | 79900/100000 [00:05<00:01, 15280.73 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  80%|███████▉  | 79586/100000 [00:05<00:01, 14677.12 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  81%|████████  | 80622/100000 [00:05<00:01, 15255.81 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  77%|███████▋  | 77210/100000 [00:05<00:01, 14559.77 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  81%|████████  | 81074/100000 [00:05<00:01, 14722.34 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  82%|████████▏ | 82200/100000 [00:05<00:01, 15289.96 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  83%|████████▎ | 82900/100000 [00:05<00:01, 15228.30 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  79%|███████▊  | 78685/100000 [00:05<00:01, 14603.11 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  84%|████████▎ | 83730/100000 [00:05<00:01, 15291.26 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  83%|████████▎ | 83300/100000 [00:05<00:01, 14749.27 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  84%|████████▍ | 84440/100000 [00:05<00:01, 15263.23 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  85%|████████▌ | 85270/100000 [00:05<00:00, 15313.17 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  81%|████████  | 80868/100000 [00:05<00:01, 14580.06 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  86%|████████▌ | 85980/100000 [00:05<00:00, 15288.61 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  86%|████████▌ | 85500/100000 [00:05<00:00, 14714.34 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  88%|████████▊ | 87539/100000 [00:05<00:00, 15354.29 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  88%|████████▊ | 87547/100000 [00:05<00:00, 15255.56 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  83%|████████▎ | 83056/100000 [00:05<00:01, 14580.68 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  87%|████████▋ | 86980/100000 [00:05<00:00, 14728.47 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  85%|████████▍ | 84530/100000 [00:05<00:01, 14612.24 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  89%|████████▊ | 88504/100000 [00:05<00:00, 14852.17 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  90%|████████▉ | 89839/100000 [00:05<00:00, 15345.52 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  90%|████████▉ | 89820/100000 [00:05<00:00, 15210.28 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  86%|████████▌ | 86085/100000 [00:05<00:00, 14845.94 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  90%|█████████ | 90030/100000 [00:05<00:00, 14947.39 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  91%|█████████▏| 91351/100000 [00:05<00:00, 15233.37 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  92%|█████████▏| 92122/100000 [00:06<00:00, 15292.21 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  88%|████████▊ | 87640/100000 [00:06<00:00, 15019.43 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  93%|█████████▎| 92886/100000 [00:06<00:00, 15262.12 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  92%|█████████▏| 92285/100000 [00:06<00:00, 14975.30 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  94%|█████████▎| 93660/100000 [00:06<00:00, 15302.52 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  89%|████████▉ | 89200/100000 [00:06<00:00, 15163.41 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  94%|█████████▍| 93832/100000 [00:06<00:00, 15101.10 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  95%|█████████▌| 95200/100000 [00:06<00:00, 15316.77 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  95%|█████████▌| 95180/100000 [00:06<00:00, 15262.32 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  91%|█████████ | 90757/100000 [00:06<00:00, 15276.48 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  95%|█████████▌| 95348/100000 [00:06<00:00, 15115.53 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  97%|█████████▋| 96740/100000 [00:06<00:00, 15328.67 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  97%|█████████▋| 96724/100000 [00:06<00:00, 15303.50 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  92%|█████████▏| 92314/100000 [00:06<00:00, 15359.41 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  97%|█████████▋| 96893/100000 [00:06<00:00, 15200.87 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  98%|█████████▊| 98281/100000 [00:06<00:00, 15343.69 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  98%|█████████▊| 98263/100000 [00:06<00:00, 15321.82 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  94%|█████████▍| 93875/100000 [00:06<00:00, 15431.87 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset: 100%|██████████| 100000/100000 [00:06<00:00, 15292.43 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset: 100%|██████████| 100000/100000 [00:06<00:00, 15180.09 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  99%|█████████▉| 99090/100000 [00:06<00:00, 14987.10 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  96%|█████████▌| 96171/100000 [00:06<00:00, 15366.32 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset: 100%|██████████| 100000/100000 [00:06<00:00, 15145.30 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset: 100%|██████████| 100000/100000 [00:06<00:00, 15136.01 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   0%|          | 0/100000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   0%|          | 0/100000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset: 100%|██████████| 100000/100000 [00:06<00:00, 14924.47 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset:  98%|█████████▊| 98442/100000 [00:06<00:00, 15266.49 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   0%|          | 0/100000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   2%|▏         | 1818/100000 [00:00<00:05, 18051.38 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   2%|▏         | 1802/100000 [00:00<00:05, 17914.94 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset: 100%|██████████| 100000/100000 [00:06<00:00, 15220.10 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   2%|▏         | 1780/100000 [00:00<00:05, 17702.02 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   4%|▎         | 3650/100000 [00:00<00:05, 18161.88 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   4%|▎         | 3630/100000 [00:00<00:05, 18098.89 examples/s]\u001b[0m\n",
      "\u001b[35mExtracting prompt in train dataset: 100%|██████████| 100000/100000 [00:06<00:00, 14501.10 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   0%|          | 0/100000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   4%|▎         | 3574/100000 [00:00<00:05, 17835.83 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   5%|▌         | 5490/100000 [00:00<00:05, 18229.87 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   5%|▌         | 5461/100000 [00:00<00:05, 18192.97 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   2%|▏         | 1730/100000 [00:00<00:05, 17174.80 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   7%|▋         | 7316/100000 [00:00<00:05, 18237.81 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   7%|▋         | 7290/100000 [00:00<00:05, 18214.04 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   6%|▌         | 6153/100000 [00:00<00:05, 17462.43 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   9%|▉         | 9140/100000 [00:00<00:04, 18233.80 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   9%|▉         | 9130/100000 [00:00<00:04, 18265.40 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   4%|▍         | 4330/100000 [00:00<00:05, 17264.19 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   9%|▊         | 8711/100000 [00:00<00:05, 17273.08 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  11%|█         | 10970/100000 [00:00<00:04, 18224.39 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  11%|█         | 10960/100000 [00:00<00:04, 18273.91 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   6%|▌         | 6090/100000 [00:00<00:05, 17376.31 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  13%|█▎        | 12801/100000 [00:00<00:04, 18315.58 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:   8%|▊         | 7850/100000 [00:00<00:05, 17430.58 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  11%|█▏        | 11270/100000 [00:00<00:05, 17179.25 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  14%|█▎        | 13710/100000 [00:00<00:04, 18227.38 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  15%|█▍        | 14640/100000 [00:00<00:04, 18332.27 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  10%|▉         | 9600/100000 [00:00<00:05, 17453.16 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  16%|█▌        | 15554/100000 [00:00<00:04, 18278.40 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  14%|█▍        | 13840/100000 [00:00<00:05, 17159.49 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  16%|█▋        | 16480/100000 [00:00<00:04, 18338.55 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  11%|█▏        | 11352/100000 [00:00<00:05, 17472.74 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  17%|█▋        | 17383/100000 [00:00<00:04, 18267.00 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  18%|█▊        | 18320/100000 [00:01<00:04, 18334.31 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  13%|█▎        | 13110/100000 [00:00<00:04, 17490.86 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  16%|█▋        | 16411/100000 [00:00<00:04, 17147.21 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  19%|█▉        | 19210/100000 [00:01<00:04, 18260.63 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  20%|██        | 20158/100000 [00:01<00:04, 18334.21 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  15%|█▍        | 14867/100000 [00:00<00:04, 17514.01 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  21%|██        | 21039/100000 [00:01<00:04, 18265.67 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  19%|█▉        | 18976/100000 [00:01<00:04, 17126.77 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  17%|█▋        | 16620/100000 [00:00<00:04, 17498.47 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  23%|██▎       | 22870/100000 [00:01<00:04, 18265.69 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  23%|██▎       | 22911/100000 [00:01<00:04, 18335.26 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  22%|██▏       | 21540/100000 [00:01<00:04, 17108.94 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  25%|██▍       | 24710/100000 [00:01<00:04, 18285.32 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  25%|██▍       | 24754/100000 [00:01<00:04, 18354.44 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  19%|█▉        | 19188/100000 [00:01<00:04, 17345.80 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  21%|██        | 20932/100000 [00:01<00:04, 17365.28 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  24%|██▍       | 24110/100000 [00:01<00:04, 17106.55 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  27%|██▋       | 27440/100000 [00:01<00:03, 18241.34 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  28%|██▊       | 27506/100000 [00:01<00:03, 18341.50 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  23%|██▎       | 22699/100000 [00:01<00:04, 17446.87 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  26%|██▌       | 25826/100000 [00:01<00:04, 17117.84 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  29%|██▉       | 29276/100000 [00:01<00:03, 18272.01 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  29%|██▉       | 29350/100000 [00:01<00:03, 18361.65 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  24%|██▍       | 24459/100000 [00:01<00:04, 17489.67 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  28%|██▊       | 27539/100000 [00:01<00:04, 17110.94 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  31%|███       | 31111/100000 [00:01<00:03, 18289.24 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  31%|███       | 31193/100000 [00:01<00:03, 18368.55 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  26%|██▌       | 26210/100000 [00:01<00:04, 17480.16 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  30%|███       | 30102/100000 [00:01<00:04, 17092.04 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  34%|███▍      | 33850/100000 [00:01<00:03, 18264.45 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  34%|███▍      | 33920/100000 [00:01<00:03, 18289.60 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  28%|██▊       | 27960/100000 [00:01<00:04, 17481.94 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  30%|██▉       | 29720/100000 [00:01<00:04, 17493.62 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  33%|███▎      | 32670/100000 [00:01<00:03, 17089.66 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  37%|███▋      | 36590/100000 [00:02<00:03, 18258.88 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  37%|███▋      | 36570/100000 [00:02<00:03, 18065.69 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  31%|███▏      | 31471/100000 [00:01<00:03, 17495.96 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  34%|███▍      | 34380/100000 [00:02<00:03, 17087.14 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  39%|███▉      | 39320/100000 [00:02<00:03, 18233.86 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  39%|███▉      | 39290/100000 [00:02<00:03, 18075.71 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  33%|███▎      | 33230/100000 [00:01<00:03, 17504.72 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  36%|███▌      | 36098/100000 [00:02<00:03, 17106.56 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  41%|████      | 41150/100000 [00:02<00:03, 18242.82 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  41%|████      | 41130/100000 [00:02<00:03, 18146.61 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  36%|███▌      | 35820/100000 [00:02<00:03, 17396.94 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  39%|███▊      | 38666/100000 [00:02<00:03, 17109.92 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  43%|████▎     | 42978/100000 [00:02<00:03, 18250.63 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  43%|████▎     | 42973/100000 [00:02<00:03, 18212.54 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  38%|███▊      | 37571/100000 [00:02<00:03, 17424.20 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  45%|████▍     | 44810/100000 [00:02<00:03, 18240.25 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  41%|████      | 41230/100000 [00:02<00:03, 17086.10 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  46%|████▌     | 45717/100000 [00:02<00:02, 18250.12 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  39%|███▉      | 39330/100000 [00:02<00:03, 17452.50 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  47%|████▋     | 46650/100000 [00:02<00:02, 18271.88 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  43%|████▎     | 42940/100000 [00:02<00:03, 17077.95 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  41%|████      | 41088/100000 [00:02<00:03, 17487.13 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  48%|████▊     | 48440/100000 [00:02<00:02, 18202.48 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  48%|████▊     | 48491/100000 [00:02<00:02, 18302.84 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  43%|████▎     | 42840/100000 [00:02<00:03, 17480.10 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  46%|████▌     | 45500/100000 [00:02<00:03, 17066.37 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  50%|█████     | 50270/100000 [00:02<00:02, 18202.93 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  50%|█████     | 50331/100000 [00:02<00:02, 18318.94 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  45%|████▍     | 44590/100000 [00:02<00:03, 17474.35 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  47%|████▋     | 47220/100000 [00:02<00:03, 17083.99 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  52%|█████▏    | 52171/100000 [00:02<00:02, 18339.80 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  53%|█████▎    | 52908/100000 [00:02<00:02, 17991.51 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  46%|████▋     | 46346/100000 [00:02<00:03, 17497.51 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  49%|████▉     | 48936/100000 [00:02<00:02, 17095.68 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  54%|█████▍    | 54014/100000 [00:02<00:02, 18357.76 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  48%|████▊     | 48100/100000 [00:02<00:02, 17493.38 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  56%|█████▌    | 55515/100000 [00:03<00:02, 17785.58 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  52%|█████▏    | 51500/100000 [00:03<00:02, 17076.08 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  57%|█████▋    | 56771/100000 [00:03<00:02, 18353.42 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  51%|█████     | 50720/100000 [00:02<00:02, 17472.07 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  53%|█████▎    | 53210/100000 [00:03<00:02, 17070.56 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  58%|█████▊    | 58104/100000 [00:03<00:02, 17617.05 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  59%|█████▊    | 58610/100000 [00:03<00:02, 18348.16 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  55%|█████▍    | 54931/100000 [00:03<00:02, 17093.69 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  53%|█████▎    | 53280/100000 [00:03<00:02, 17305.67 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  61%|██████    | 60691/100000 [00:03<00:02, 17498.95 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  61%|██████▏   | 61360/100000 [00:03<00:02, 18335.23 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  57%|█████▋    | 56650/100000 [00:03<00:02, 17100.85 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  56%|█████▌    | 55870/100000 [00:03<00:02, 17280.87 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  58%|█████▊    | 58367/100000 [00:03<00:02, 17115.33 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  63%|██████▎   | 63280/100000 [00:03<00:02, 17411.38 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  64%|██████▍   | 64050/100000 [00:03<00:01, 18176.73 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  58%|█████▊    | 57622/100000 [00:03<00:02, 17337.34 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  60%|██████    | 60080/100000 [00:03<00:02, 17102.12 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  66%|██████▌   | 65890/100000 [00:03<00:01, 18219.35 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  66%|██████▌   | 65873/100000 [00:03<00:01, 17371.63 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  59%|█████▉    | 59380/100000 [00:03<00:02, 17384.23 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  68%|██████▊   | 67724/100000 [00:03<00:01, 18248.36 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  63%|██████▎   | 62650/100000 [00:03<00:02, 17104.41 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  61%|██████    | 61130/100000 [00:03<00:02, 17412.55 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  68%|██████▊   | 68468/100000 [00:03<00:01, 17342.99 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  70%|██████▉   | 69561/100000 [00:03<00:01, 18270.63 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  64%|██████▍   | 64368/100000 [00:03<00:02, 17118.82 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  63%|██████▎   | 62880/100000 [00:03<00:02, 17429.25 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  71%|███████▏  | 71394/100000 [00:03<00:01, 18272.88 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  71%|███████   | 71050/100000 [00:03<00:01, 17293.72 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  65%|██████▍   | 64635/100000 [00:03<00:02, 17462.32 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  67%|██████▋   | 66928/100000 [00:03<00:01, 17093.25 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  73%|███████▎  | 73228/100000 [00:04<00:01, 18277.67 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  66%|██████▋   | 66390/100000 [00:03<00:01, 17474.79 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  74%|███████▎  | 73630/100000 [00:04<00:01, 17255.27 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  75%|███████▌  | 75060/100000 [00:04<00:01, 18282.18 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  69%|██████▉   | 69489/100000 [00:04<00:01, 17074.34 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  68%|██████▊   | 68140/100000 [00:03<00:01, 17479.73 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  77%|███████▋  | 76890/100000 [00:04<00:01, 18280.74 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  71%|███████   | 71200/100000 [00:04<00:01, 17072.77 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  76%|███████▌  | 76210/100000 [00:04<00:01, 17232.54 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  71%|███████   | 70609/100000 [00:04<00:01, 17075.04 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  73%|███████▎  | 72910/100000 [00:04<00:01, 17068.01 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  80%|███████▉  | 79570/100000 [00:04<00:01, 18098.25 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  79%|███████▉  | 78787/100000 [00:04<00:01, 17215.22 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  75%|███████▍  | 74623/100000 [00:04<00:01, 17083.36 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  73%|███████▎  | 73153/100000 [00:04<00:01, 17027.96 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  81%|████████  | 80511/100000 [00:04<00:01, 17211.06 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  82%|████████▏ | 82260/100000 [00:04<00:00, 18031.51 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  75%|███████▍  | 74897/100000 [00:04<00:01, 17131.01 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  77%|███████▋  | 77184/100000 [00:04<00:01, 17074.71 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  84%|████████▍ | 84090/100000 [00:04<00:00, 18089.39 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  83%|████████▎ | 83085/100000 [00:04<00:00, 17191.19 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  77%|███████▋  | 76641/100000 [00:04<00:01, 17210.34 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  86%|████████▌ | 85920/100000 [00:04<00:00, 18128.84 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  80%|███████▉  | 79750/100000 [00:04<00:01, 17068.97 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  85%|████████▍ | 84810/100000 [00:04<00:00, 17193.90 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  78%|███████▊  | 78390/100000 [00:04<00:01, 17278.86 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  88%|████████▊ | 87751/100000 [00:04<00:00, 18177.59 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  81%|████████▏ | 81460/100000 [00:04<00:01, 17072.41 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  80%|████████  | 80144/100000 [00:04<00:01, 17349.84 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  87%|████████▋ | 87379/100000 [00:04<00:00, 17168.36 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  90%|████████▉ | 89587/100000 [00:04<00:00, 18216.95 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  83%|████████▎ | 83170/100000 [00:04<00:00, 17070.99 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  82%|████████▏ | 81897/100000 [00:04<00:01, 17388.45 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  91%|█████████▏| 91411/100000 [00:05<00:00, 18221.01 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  85%|████████▍ | 84883/100000 [00:04<00:00, 17084.97 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  90%|████████▉ | 89953/100000 [00:05<00:00, 17152.85 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  84%|████████▎ | 83650/100000 [00:04<00:00, 17412.70 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  93%|█████████▎| 93250/100000 [00:05<00:00, 18260.80 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  92%|█████████▏| 91670/100000 [00:05<00:00, 17140.29 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  85%|████████▌ | 85410/100000 [00:04<00:00, 17444.39 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  87%|████████▋ | 87450/100000 [00:05<00:00, 17081.90 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  95%|█████████▌| 95080/100000 [00:05<00:00, 18259.84 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  93%|█████████▎| 93391/100000 [00:05<00:00, 17146.98 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  89%|████████▉ | 89160/100000 [00:05<00:00, 17079.45 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  97%|█████████▋| 96915/100000 [00:05<00:00, 18285.58 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  88%|████████▊ | 88000/100000 [00:05<00:00, 17368.22 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  95%|█████████▌| 95110/100000 [00:05<00:00, 17151.05 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  91%|█████████ | 90870/100000 [00:05<00:00, 17083.05 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  99%|█████████▊| 98745/100000 [00:05<00:00, 18279.26 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  90%|████████▉ | 89748/100000 [00:05<00:00, 17396.95 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  97%|█████████▋| 96839/100000 [00:05<00:00, 17188.62 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  93%|█████████▎| 92580/100000 [00:05<00:00, 17076.14 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  92%|█████████▏| 91500/100000 [00:05<00:00, 17418.01 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset: 100%|██████████| 100000/100000 [00:05<00:00, 17991.09 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  94%|█████████▍| 94379/100000 [00:05<00:00, 17333.12 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  99%|█████████▉| 99410/100000 [00:05<00:00, 17154.79 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  93%|█████████▎| 93252/100000 [00:05<00:00, 17444.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 0/100000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  96%|█████████▌| 96180/100000 [00:05<00:00, 17525.06 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset: 100%|██████████| 100000/100000 [00:05<00:00, 17469.78 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  95%|█████████▌| 95010/100000 [00:05<00:00, 17461.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 100/100000 [00:00<01:46, 940.37 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  98%|█████████▊| 97968/100000 [00:05<00:00, 17626.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 0/100000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  97%|█████████▋| 96770/100000 [00:05<00:00, 17491.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 206/100000 [00:00<01:40, 994.59 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset: 100%|█████████▉| 99764/100000 [00:05<00:00, 17716.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 91/100000 [00:00<01:51, 899.01 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset:  99%|█████████▊| 98520/100000 [00:05<00:00, 17478.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 310/100000 [00:00<01:38, 1009.32 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset: 100%|██████████| 100000/100000 [00:05<00:00, 16940.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 198/100000 [00:00<01:41, 982.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 420/100000 [00:00<01:36, 1030.71 examples/s]\u001b[0m\n",
      "\u001b[35mApplying chat template to train dataset: 100%|██████████| 100000/100000 [00:05<00:00, 17167.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 0/100000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 300/100000 [00:00<01:40, 996.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 0/100000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 100/100000 [00:00<01:46, 936.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 578/100000 [00:00<01:36, 1033.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 407/100000 [00:00<01:37, 1021.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 100/100000 [00:00<01:47, 932.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 203/100000 [00:00<01:41, 985.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 732/100000 [00:00<01:36, 1027.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 560/100000 [00:00<01:38, 1009.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 203/100000 [00:00<01:41, 985.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 305/100000 [00:00<01:39, 998.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 842/100000 [00:00<01:35, 1041.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 667/100000 [00:00<01:36, 1026.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 305/100000 [00:00<01:39, 1000.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 412/100000 [00:00<01:37, 1021.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   0%|          | 412/100000 [00:00<01:37, 1025.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 992/100000 [00:00<01:36, 1024.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 820/100000 [00:00<01:37, 1020.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 568/100000 [00:00<01:37, 1023.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 1108/100000 [00:01<01:33, 1052.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 569/100000 [00:00<01:36, 1027.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 671/100000 [00:00<01:37, 1020.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 970/100000 [00:00<01:38, 1009.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 1218/100000 [00:01<01:33, 1061.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 672/100000 [00:00<01:36, 1025.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 1079/100000 [00:01<01:36, 1029.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 829/100000 [00:00<01:36, 1029.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 775/100000 [00:00<01:37, 1022.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|▏         | 1370/100000 [00:01<01:35, 1037.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 1195/100000 [00:01<01:33, 1060.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 885/100000 [00:00<01:35, 1041.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 980/100000 [00:00<01:37, 1014.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|▏         | 1481/100000 [00:01<01:33, 1052.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|▏         | 1346/100000 [00:01<01:36, 1025.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 1095/100000 [00:01<01:34, 1042.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 1590/100000 [00:01<01:32, 1059.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 1040/100000 [00:01<01:35, 1033.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|▏         | 1453/100000 [00:01<01:35, 1033.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 1205/100000 [00:01<01:33, 1055.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|          | 1153/100000 [00:01<01:33, 1055.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 1751/100000 [00:01<01:32, 1057.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 1563/100000 [00:01<01:34, 1045.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|▏         | 1355/100000 [00:01<01:35, 1032.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 1669/100000 [00:01<01:33, 1046.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|▏         | 1304/100000 [00:01<01:35, 1036.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 1899/100000 [00:01<01:35, 1026.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|▏         | 1464/100000 [00:01<01:34, 1042.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 2004/100000 [00:01<01:35, 1023.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 1822/100000 [00:01<01:35, 1029.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   1%|▏         | 1460/100000 [00:01<01:35, 1034.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 1573/100000 [00:01<01:33, 1054.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 2113/100000 [00:02<01:34, 1034.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 1570/100000 [00:01<01:33, 1047.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 1680/100000 [00:01<01:33, 1052.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 1971/100000 [00:01<01:36, 1013.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 2223/100000 [00:02<01:33, 1048.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 1677/100000 [00:01<01:33, 1049.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 2074/100000 [00:02<01:36, 1014.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 1833/100000 [00:01<01:34, 1036.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 2330/100000 [00:02<01:32, 1051.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 2186/100000 [00:02<01:34, 1038.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 1830/100000 [00:01<01:35, 1031.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 1983/100000 [00:01<01:36, 1016.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 2491/100000 [00:02<01:32, 1055.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 2345/100000 [00:02<01:33, 1042.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 1983/100000 [00:01<01:36, 1014.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 2091/100000 [00:02<01:35, 1027.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 2597/100000 [00:02<01:32, 1053.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 2090/100000 [00:02<01:35, 1026.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 2200/100000 [00:02<01:34, 1038.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 2502/100000 [00:02<01:33, 1038.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 2749/100000 [00:02<01:33, 1036.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 2199/100000 [00:02<01:34, 1040.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 2305/100000 [00:02<01:33, 1039.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 2608/100000 [00:02<01:33, 1042.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 2853/100000 [00:02<01:33, 1034.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 2414/100000 [00:02<01:33, 1048.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   2%|▏         | 2360/100000 [00:02<01:33, 1046.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 2757/100000 [00:02<01:35, 1022.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 2520/100000 [00:02<01:33, 1043.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 3010/100000 [00:02<01:33, 1033.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 2860/100000 [00:02<01:35, 1021.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 2517/100000 [00:02<01:33, 1042.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 3124/100000 [00:03<01:31, 1053.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 2668/100000 [00:02<01:35, 1018.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 2966/100000 [00:02<01:34, 1026.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 2622/100000 [00:02<01:33, 1039.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 3234/100000 [00:03<01:31, 1063.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 2777/100000 [00:02<01:33, 1035.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 3074/100000 [00:02<01:33, 1037.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 2776/100000 [00:02<01:34, 1030.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 2882/100000 [00:02<01:33, 1035.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 3388/100000 [00:03<01:32, 1044.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 3184/100000 [00:03<01:32, 1052.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 2886/100000 [00:02<01:33, 1042.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 3292/100000 [00:03<01:31, 1055.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 3041/100000 [00:02<01:33, 1037.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▎         | 3543/100000 [00:03<01:33, 1035.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 3045/100000 [00:02<01:32, 1044.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 3150/100000 [00:03<01:32, 1048.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 3438/100000 [00:03<01:34, 1019.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▎         | 3696/100000 [00:03<01:33, 1027.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 3159/100000 [00:03<01:31, 1060.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 3262/100000 [00:03<01:30, 1065.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▎         | 3543/100000 [00:03<01:34, 1025.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 3809/100000 [00:03<01:31, 1050.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 3313/100000 [00:03<01:32, 1044.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 3406/100000 [00:03<01:34, 1020.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 3915/100000 [00:03<01:31, 1050.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▎         | 3694/100000 [00:03<01:34, 1014.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▎         | 3511/100000 [00:03<01:34, 1024.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 4029/100000 [00:03<01:29, 1068.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 3806/100000 [00:03<01:32, 1036.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   3%|▎         | 3459/100000 [00:03<01:35, 1012.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▎         | 3621/100000 [00:03<01:32, 1039.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 3912/100000 [00:03<01:32, 1041.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▎         | 3562/100000 [00:03<01:34, 1016.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 4190/100000 [00:04<01:29, 1067.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 4024/100000 [00:03<01:30, 1060.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▎         | 3665/100000 [00:03<01:34, 1018.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 3780/100000 [00:03<01:32, 1041.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 4348/100000 [00:04<01:30, 1058.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 3772/100000 [00:03<01:33, 1031.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 3886/100000 [00:03<01:32, 1042.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 4187/100000 [00:04<01:30, 1063.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 4460/100000 [00:04<01:29, 1068.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 3878/100000 [00:03<01:32, 1038.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 4000/100000 [00:03<01:29, 1067.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 4338/100000 [00:04<01:31, 1041.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 3993/100000 [00:03<01:30, 1064.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▍         | 4600/100000 [00:04<01:33, 1018.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 4164/100000 [00:04<01:29, 1074.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 4451/100000 [00:04<01:30, 1059.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 4155/100000 [00:04<01:29, 1069.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▍         | 4758/100000 [00:04<01:32, 1027.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 4317/100000 [00:04<01:31, 1049.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▍         | 4593/100000 [00:04<01:33, 1016.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▍         | 4862/100000 [00:04<01:33, 1021.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 4304/100000 [00:04<01:32, 1038.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 4429/100000 [00:04<01:29, 1065.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▍         | 4747/100000 [00:04<01:33, 1016.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▍         | 4980/100000 [00:04<01:29, 1058.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   4%|▍         | 4420/100000 [00:04<01:29, 1063.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▍         | 4581/100000 [00:04<01:31, 1039.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▌         | 5093/100000 [00:04<01:28, 1074.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▍         | 4853/100000 [00:04<01:33, 1017.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▍         | 4572/100000 [00:04<01:31, 1042.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▍         | 4966/100000 [00:04<01:30, 1044.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▍         | 4736/100000 [00:04<01:32, 1031.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▌         | 5253/100000 [00:05<01:28, 1071.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▌         | 5079/100000 [00:04<01:29, 1064.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▍         | 4722/100000 [00:04<01:32, 1025.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▍         | 4840/100000 [00:04<01:32, 1031.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▌         | 5364/100000 [00:05<01:27, 1078.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▍         | 4829/100000 [00:04<01:32, 1033.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▍         | 4951/100000 [00:04<01:30, 1048.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▌         | 5483/100000 [00:05<01:25, 1105.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▌         | 5241/100000 [00:05<01:29, 1062.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▍         | 4940/100000 [00:04<01:30, 1047.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▌         | 5067/100000 [00:04<01:28, 1075.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▌         | 5351/100000 [00:05<01:28, 1069.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 5648/100000 [00:05<01:25, 1097.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▌         | 5053/100000 [00:04<01:28, 1067.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▌         | 5176/100000 [00:04<01:28, 1075.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▌         | 5467/100000 [00:05<01:26, 1090.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 5759/100000 [00:05<01:25, 1099.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▌         | 5163/100000 [00:04<01:28, 1073.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▌         | 5336/100000 [00:05<01:28, 1068.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 5629/100000 [00:05<01:27, 1082.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▌         | 5271/100000 [00:05<01:28, 1071.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 5916/100000 [00:05<01:27, 1076.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▌         | 5455/100000 [00:05<01:26, 1098.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 5738/100000 [00:05<01:27, 1080.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▌         | 5385/100000 [00:05<01:27, 1086.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 6072/100000 [00:05<01:28, 1061.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   5%|▌         | 5497/100000 [00:05<01:26, 1092.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 5616/100000 [00:05<01:26, 1087.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 5892/100000 [00:05<01:28, 1059.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 6185/100000 [00:05<01:27, 1073.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 5729/100000 [00:05<01:25, 1097.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 5999/100000 [00:05<01:28, 1056.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 5664/100000 [00:05<01:26, 1093.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▋         | 6295/100000 [00:05<01:27, 1076.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 5774/100000 [00:05<01:26, 1093.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 5883/100000 [00:05<01:28, 1067.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 6160/100000 [00:05<01:28, 1056.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▋         | 6449/100000 [00:06<01:28, 1055.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 5926/100000 [00:05<01:28, 1058.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 6045/100000 [00:05<01:28, 1065.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▋         | 6318/100000 [00:06<01:29, 1049.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 6614/100000 [00:06<01:27, 1065.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 6035/100000 [00:05<01:28, 1064.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 6154/100000 [00:05<01:27, 1068.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▋         | 6427/100000 [00:06<01:28, 1055.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 6722/100000 [00:06<01:27, 1066.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▌         | 6145/100000 [00:05<01:27, 1071.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▋         | 6314/100000 [00:06<01:28, 1063.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 6583/100000 [00:06<01:29, 1044.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 6884/100000 [00:06<01:27, 1065.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▋         | 6304/100000 [00:06<01:28, 1063.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 6693/100000 [00:06<01:28, 1053.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▋         | 6464/100000 [00:06<01:29, 1039.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 6994/100000 [00:06<01:26, 1069.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   6%|▋         | 6412/100000 [00:06<01:27, 1063.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 6578/100000 [00:06<01:28, 1060.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 6851/100000 [00:06<01:28, 1046.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 7152/100000 [00:06<01:27, 1061.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 6566/100000 [00:06<01:29, 1046.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 6690/100000 [00:06<01:27, 1071.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 6962/100000 [00:06<01:27, 1060.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 7259/100000 [00:06<01:27, 1062.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 6680/100000 [00:06<01:27, 1065.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 7070/100000 [00:06<01:27, 1063.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 6850/100000 [00:06<01:27, 1060.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 7372/100000 [00:06<01:26, 1075.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 6836/100000 [00:06<01:28, 1053.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 6962/100000 [00:06<01:26, 1073.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 7490/100000 [00:07<01:23, 1101.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 7223/100000 [00:06<01:28, 1046.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 6950/100000 [00:06<01:27, 1068.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 7335/100000 [00:07<01:27, 1064.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 7121/100000 [00:06<01:27, 1067.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 7650/100000 [00:07<01:25, 1085.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 7451/100000 [00:07<01:25, 1088.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 7108/100000 [00:06<01:27, 1059.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 7229/100000 [00:06<01:27, 1063.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 7767/100000 [00:07<01:23, 1099.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 7216/100000 [00:06<01:27, 1060.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 7342/100000 [00:06<01:26, 1076.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 7610/100000 [00:07<01:25, 1074.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 7922/100000 [00:07<01:25, 1074.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 7327/100000 [00:06<01:26, 1070.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 7460/100000 [00:07<01:24, 1100.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 7719/100000 [00:07<01:26, 1072.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   7%|▋         | 7444/100000 [00:07<01:24, 1095.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 8080/100000 [00:07<01:26, 1062.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 7619/100000 [00:07<01:25, 1080.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 7876/100000 [00:07<01:26, 1059.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 7606/100000 [00:07<01:25, 1081.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 7728/100000 [00:07<01:25, 1080.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 8240/100000 [00:07<01:26, 1062.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 7984/100000 [00:07<01:26, 1062.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 7716/100000 [00:07<01:25, 1079.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 7888/100000 [00:07<01:25, 1071.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 8391/100000 [00:07<01:27, 1041.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 8141/100000 [00:07<01:27, 1053.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 7876/100000 [00:07<01:26, 1065.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 8500/100000 [00:08<01:26, 1052.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 8045/100000 [00:07<01:26, 1062.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 8292/100000 [00:07<01:28, 1034.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 7985/100000 [00:07<01:26, 1069.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▊         | 8607/100000 [00:08<01:26, 1054.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 8208/100000 [00:07<01:26, 1064.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 8449/100000 [00:08<01:28, 1030.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 8141/100000 [00:07<01:26, 1057.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 8753/100000 [00:08<01:29, 1020.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▊         | 8558/100000 [00:08<01:27, 1040.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 8356/100000 [00:07<01:28, 1038.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 8293/100000 [00:07<01:28, 1037.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 8906/100000 [00:08<01:29, 1017.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▊         | 8669/100000 [00:08<01:26, 1057.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 8464/100000 [00:08<01:27, 1045.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9015/100000 [00:08<01:28, 1032.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   8%|▊         | 8449/100000 [00:08<01:28, 1034.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▊         | 8570/100000 [00:08<01:27, 1046.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 8805/100000 [00:08<01:31, 1000.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9125/100000 [00:08<01:26, 1046.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▊         | 8555/100000 [00:08<01:28, 1036.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▊         | 8680/100000 [00:08<01:26, 1054.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 8910/100000 [00:08<01:30, 1005.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▊         | 8660/100000 [00:08<01:28, 1037.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9279/100000 [00:08<01:27, 1033.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9019/100000 [00:08<01:28, 1022.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 8820/100000 [00:08<01:30, 1005.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9387/100000 [00:08<01:26, 1043.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9129/100000 [00:08<01:27, 1039.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 8796/100000 [00:08<01:32, 990.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 8928/100000 [00:08<01:29, 1017.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9496/100000 [00:09<01:26, 1050.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 8900/100000 [00:08<01:31, 997.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9034/100000 [00:08<01:28, 1026.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9280/100000 [00:08<01:29, 1018.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9013/100000 [00:08<01:28, 1027.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9145/100000 [00:08<01:26, 1047.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|▉         | 9639/100000 [00:09<01:29, 1011.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9390/100000 [00:09<01:27, 1032.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9121/100000 [00:08<01:27, 1037.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|▉         | 9746/100000 [00:09<01:28, 1023.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9497/100000 [00:09<01:27, 1037.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9298/100000 [00:08<01:27, 1033.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9274/100000 [00:08<01:28, 1028.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9403/100000 [00:08<01:27, 1036.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|▉         | 9896/100000 [00:09<01:29, 1011.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|▉         | 9637/100000 [00:09<01:30, 998.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9380/100000 [00:08<01:27, 1033.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|█         | 10010/100000 [00:09<01:26, 1039.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|▉         | 9742/100000 [00:09<01:29, 1006.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|▉         | 9556/100000 [00:09<01:28, 1026.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:   9%|▉         | 9486/100000 [00:09<01:27, 1039.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|█         | 10166/100000 [00:09<01:27, 1030.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|▉         | 9894/100000 [00:09<01:29, 1002.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|▉         | 9708/100000 [00:09<01:28, 1016.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|▉         | 9630/100000 [00:09<01:30, 1003.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|█         | 10005/100000 [00:09<01:27, 1025.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|█         | 10319/100000 [00:09<01:27, 1022.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|▉         | 9734/100000 [00:09<01:29, 1011.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|▉         | 9859/100000 [00:09<01:29, 1009.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|█         | 10428/100000 [00:09<01:26, 1035.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|█         | 10159/100000 [00:09<01:28, 1017.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|▉         | 9970/100000 [00:09<01:27, 1028.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|▉         | 9886/100000 [00:09<01:29, 1007.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 10583/100000 [00:10<01:27, 1023.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|█         | 10309/100000 [00:09<01:29, 1007.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|▉         | 9999/100000 [00:09<01:26, 1034.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|█         | 10122/100000 [00:09<01:27, 1022.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 10698/100000 [00:10<01:25, 1045.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|█         | 10419/100000 [00:10<01:27, 1024.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|█         | 10226/100000 [00:09<01:27, 1023.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|█         | 10151/100000 [00:09<01:28, 1018.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 10856/100000 [00:10<01:25, 1041.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 10570/100000 [00:10<01:28, 1014.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|█         | 10257/100000 [00:09<01:27, 1026.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|█         | 10379/100000 [00:09<01:27, 1019.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 10961/100000 [00:10<01:25, 1035.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 10681/100000 [00:10<01:26, 1035.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  10%|█         | 10361/100000 [00:09<01:27, 1026.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 10531/100000 [00:10<01:28, 1012.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 11068/100000 [00:10<01:25, 1037.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 10835/100000 [00:10<01:26, 1028.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 10511/100000 [00:10<01:28, 1012.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 10636/100000 [00:10<01:27, 1021.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 11226/100000 [00:10<01:25, 1036.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 10621/100000 [00:10<01:26, 1032.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 10744/100000 [00:10<01:26, 1032.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 10992/100000 [00:10<01:26, 1033.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█▏        | 11331/100000 [00:10<01:25, 1037.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 10731/100000 [00:10<01:25, 1049.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 10899/100000 [00:10<01:26, 1027.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 11140/100000 [00:10<01:27, 1015.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█▏        | 11490/100000 [00:10<01:24, 1043.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 10881/100000 [00:10<01:26, 1029.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 11006/100000 [00:10<01:25, 1036.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11599/100000 [00:11<01:23, 1053.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█▏        | 11295/100000 [00:10<01:26, 1020.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 10985/100000 [00:10<01:26, 1030.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 11152/100000 [00:10<01:27, 1013.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11708/100000 [00:11<01:23, 1061.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█▏        | 11400/100000 [00:10<01:26, 1021.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 11089/100000 [00:10<01:26, 1024.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█▏        | 11256/100000 [00:10<01:27, 1016.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11815/100000 [00:11<01:23, 1061.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11505/100000 [00:11<01:26, 1027.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█         | 11238/100000 [00:10<01:27, 1011.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█▏        | 11360/100000 [00:10<01:27, 1017.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11923/100000 [00:11<01:22, 1062.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11612/100000 [00:11<01:25, 1035.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█▏        | 11347/100000 [00:10<01:26, 1027.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  11%|█▏        | 11469/100000 [00:10<01:25, 1031.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 12034/100000 [00:11<01:22, 1072.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11722/100000 [00:11<01:24, 1048.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11575/100000 [00:11<01:25, 1036.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 12145/100000 [00:11<01:21, 1080.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11830/100000 [00:11<01:23, 1050.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11502/100000 [00:11<01:26, 1027.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11681/100000 [00:11<01:25, 1039.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 12261/100000 [00:11<01:19, 1097.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11942/100000 [00:11<01:22, 1066.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11611/100000 [00:11<01:24, 1040.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11795/100000 [00:11<01:22, 1063.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 12051/100000 [00:11<01:22, 1065.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11721/100000 [00:11<01:23, 1051.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 12410/100000 [00:11<01:23, 1053.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 12160/100000 [00:11<01:22, 1069.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11828/100000 [00:11<01:23, 1053.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11955/100000 [00:11<01:23, 1060.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 12522/100000 [00:11<01:22, 1061.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 12273/100000 [00:11<01:20, 1085.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 11940/100000 [00:11<01:22, 1066.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 12062/100000 [00:11<01:22, 1061.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 12634/100000 [00:12<01:21, 1069.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 12051/100000 [00:11<01:22, 1069.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 12742/100000 [00:12<01:21, 1070.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 12420/100000 [00:11<01:24, 1040.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 12224/100000 [00:11<01:22, 1067.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 12160/100000 [00:11<01:21, 1072.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 12530/100000 [00:12<01:23, 1051.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 12898/100000 [00:12<01:22, 1055.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 12376/100000 [00:11<01:24, 1043.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 12276/100000 [00:11<01:20, 1089.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 12637/100000 [00:12<01:22, 1053.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 13007/100000 [00:12<01:22, 1058.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 12481/100000 [00:11<01:23, 1042.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 12745/100000 [00:12<01:22, 1054.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  12%|█▏        | 12428/100000 [00:11<01:23, 1052.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 13120/100000 [00:12<01:21, 1072.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 12587/100000 [00:12<01:24, 1035.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 12536/100000 [00:12<01:22, 1056.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 13232/100000 [00:12<01:20, 1082.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 12899/100000 [00:12<01:23, 1040.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 12698/100000 [00:12<01:22, 1051.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 12646/100000 [00:12<01:21, 1066.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 13342/100000 [00:12<01:19, 1086.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 13006/100000 [00:12<01:23, 1044.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 12851/100000 [00:12<01:24, 1027.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 13116/100000 [00:12<01:22, 1058.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 12800/100000 [00:12<01:23, 1047.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▎        | 13507/100000 [00:12<01:19, 1085.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 12957/100000 [00:12<01:24, 1033.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 13226/100000 [00:12<01:21, 1068.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 12907/100000 [00:12<01:22, 1050.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 13065/100000 [00:12<01:23, 1044.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▎        | 13652/100000 [00:12<01:23, 1039.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 13337/100000 [00:12<01:20, 1076.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 13015/100000 [00:12<01:22, 1055.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 13180/100000 [00:12<01:21, 1066.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 13769/100000 [00:13<01:20, 1069.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 13445/100000 [00:12<01:21, 1060.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 13124/100000 [00:12<01:21, 1060.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 13292/100000 [00:12<01:20, 1075.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 13878/100000 [00:13<01:20, 1072.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▎        | 13561/100000 [00:13<01:19, 1082.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 13235/100000 [00:12<01:20, 1071.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 13349/100000 [00:12<01:19, 1087.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  13%|█▎        | 13445/100000 [00:12<01:22, 1046.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 14038/100000 [00:13<01:20, 1066.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▎        | 13700/100000 [00:13<01:24, 1018.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▎        | 13560/100000 [00:12<01:20, 1071.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 14147/100000 [00:13<01:20, 1071.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 13813/100000 [00:13<01:22, 1041.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▎        | 13509/100000 [00:12<01:20, 1077.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 13924/100000 [00:13<01:21, 1056.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▎        | 13696/100000 [00:13<01:25, 1009.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 14305/100000 [00:13<01:21, 1057.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▎        | 13652/100000 [00:13<01:23, 1030.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 13809/100000 [00:13<01:23, 1037.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 14081/100000 [00:13<01:22, 1047.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 13769/100000 [00:13<01:21, 1061.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 14457/100000 [00:13<01:22, 1041.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 13877/100000 [00:13<01:21, 1063.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 13960/100000 [00:13<01:24, 1020.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▍        | 14563/100000 [00:13<01:21, 1045.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 14239/100000 [00:13<01:21, 1047.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 14070/100000 [00:13<01:22, 1038.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 14034/100000 [00:13<01:21, 1054.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▍        | 14712/100000 [00:13<01:23, 1022.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 14391/100000 [00:13<01:22, 1031.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 14143/100000 [00:13<01:20, 1060.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 14224/100000 [00:13<01:23, 1032.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▍        | 14827/100000 [00:14<01:20, 1051.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 14500/100000 [00:13<01:22, 1041.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 14336/100000 [00:13<01:21, 1048.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 14300/100000 [00:13<01:21, 1051.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▍        | 14971/100000 [00:14<01:23, 1014.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▍        | 14647/100000 [00:14<01:23, 1018.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 14490/100000 [00:13<01:23, 1029.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▌        | 15080/100000 [00:14<01:22, 1025.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▍        | 14755/100000 [00:14<01:22, 1032.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  14%|█▍        | 14457/100000 [00:13<01:22, 1036.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▌        | 15188/100000 [00:14<01:21, 1035.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▍        | 14563/100000 [00:13<01:22, 1040.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▍        | 14640/100000 [00:14<01:23, 1017.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▍        | 14903/100000 [00:14<01:23, 1014.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▍        | 14744/100000 [00:14<01:23, 1020.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▌        | 15340/100000 [00:14<01:22, 1021.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▍        | 14711/100000 [00:14<01:23, 1020.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▌        | 15052/100000 [00:14<01:24, 1002.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▍        | 14850/100000 [00:14<01:22, 1027.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▌        | 15451/100000 [00:14<01:21, 1038.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▍        | 14824/100000 [00:14<01:21, 1044.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▌        | 15163/100000 [00:14<01:22, 1025.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 15564/100000 [00:14<01:19, 1059.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▍        | 14991/100000 [00:14<01:25, 993.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▍        | 14968/100000 [00:14<01:23, 1013.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▌        | 15315/100000 [00:14<01:23, 1017.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▌        | 15099/100000 [00:14<01:23, 1012.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 15723/100000 [00:14<01:19, 1055.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▌        | 15072/100000 [00:14<01:23, 1019.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▌        | 15472/100000 [00:14<01:22, 1024.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▌        | 15178/100000 [00:14<01:22, 1027.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▌        | 15249/100000 [00:14<01:24, 1007.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 15873/100000 [00:15<01:21, 1033.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 15580/100000 [00:14<01:21, 1032.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▌        | 15351/100000 [00:14<01:24, 1007.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 15980/100000 [00:15<01:20, 1040.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▌        | 15333/100000 [00:14<01:22, 1026.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 15686/100000 [00:15<01:21, 1038.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▌        | 15460/100000 [00:14<01:22, 1026.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 16140/100000 [00:15<01:20, 1046.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  15%|█▌        | 15497/100000 [00:14<01:21, 1042.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 15570/100000 [00:14<01:21, 1040.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 15840/100000 [00:15<01:21, 1030.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 15677/100000 [00:15<01:20, 1047.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▋        | 16301/100000 [00:15<01:19, 1053.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 15651/100000 [00:14<01:21, 1035.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 15994/100000 [00:15<01:21, 1027.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 15761/100000 [00:15<01:20, 1044.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 15831/100000 [00:15<01:21, 1035.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▋        | 16449/100000 [00:15<01:21, 1028.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 16148/100000 [00:15<01:21, 1023.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 15919/100000 [00:15<01:21, 1034.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 15984/100000 [00:15<01:21, 1026.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 16610/100000 [00:15<01:20, 1034.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▋        | 16252/100000 [00:15<01:21, 1026.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 16715/100000 [00:15<01:20, 1035.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▋        | 16357/100000 [00:15<01:21, 1026.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 16067/100000 [00:15<01:22, 1016.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 16141/100000 [00:15<01:21, 1029.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 16174/100000 [00:15<01:21, 1025.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▌        | 16246/100000 [00:15<01:21, 1032.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 16872/100000 [00:16<01:20, 1036.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 16502/100000 [00:15<01:23, 1003.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▋        | 16290/100000 [00:15<01:19, 1055.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▋        | 16350/100000 [00:15<01:21, 1030.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 16979/100000 [00:16<01:19, 1043.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 16611/100000 [00:16<01:21, 1019.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 16716/100000 [00:16<01:21, 1023.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▋        | 16434/100000 [00:15<01:21, 1019.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  16%|█▋        | 16494/100000 [00:15<01:23, 1001.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 17132/100000 [00:16<01:20, 1032.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 16604/100000 [00:15<01:21, 1023.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 16870/100000 [00:16<01:21, 1023.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 16590/100000 [00:15<01:21, 1019.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 17291/100000 [00:16<01:19, 1037.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 16708/100000 [00:16<01:21, 1024.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 16977/100000 [00:16<01:20, 1032.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 16699/100000 [00:16<01:20, 1032.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 17395/100000 [00:16<01:19, 1036.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 16860/100000 [00:16<01:21, 1016.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 17502/100000 [00:16<01:19, 1038.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 17130/100000 [00:16<01:21, 1022.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 16852/100000 [00:16<01:21, 1024.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 16969/100000 [00:16<01:20, 1031.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 16961/100000 [00:16<01:20, 1037.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 17655/100000 [00:16<01:20, 1028.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 17287/100000 [00:16<01:20, 1027.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 17121/100000 [00:16<01:21, 1019.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 17110/100000 [00:16<01:21, 1016.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 17793/100000 [00:16<01:22, 991.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 17443/100000 [00:16<01:20, 1028.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 17275/100000 [00:16<01:21, 1018.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 17899/100000 [00:17<01:21, 1004.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 17265/100000 [00:16<01:21, 1017.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 17592/100000 [00:16<01:21, 1014.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 17430/100000 [00:16<01:20, 1020.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 18044/100000 [00:17<01:22, 988.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 17699/100000 [00:17<01:20, 1024.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  17%|█▋        | 17424/100000 [00:16<01:20, 1025.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 18153/100000 [00:17<01:21, 1009.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 17580/100000 [00:16<01:21, 1008.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 17849/100000 [00:17<01:20, 1014.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 17572/100000 [00:16<01:21, 1009.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 18270/100000 [00:17<01:18, 1047.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 17689/100000 [00:17<01:20, 1021.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 17681/100000 [00:16<01:20, 1024.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 17995/100000 [00:17<01:22, 998.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 18430/100000 [00:17<01:18, 1044.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 17840/100000 [00:17<01:21, 1011.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 18099/100000 [00:17<01:21, 1007.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 17832/100000 [00:17<01:20, 1014.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▊        | 18539/100000 [00:17<01:17, 1051.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 18206/100000 [00:17<01:20, 1020.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 17982/100000 [00:17<01:23, 986.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 17977/100000 [00:17<01:22, 995.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▊        | 18695/100000 [00:17<01:17, 1045.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 18319/100000 [00:17<01:18, 1045.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 18088/100000 [00:17<01:21, 1001.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 18081/100000 [00:17<01:21, 1004.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 18194/100000 [00:17<01:20, 1014.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 18832/100000 [00:17<01:21, 997.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 18477/100000 [00:17<01:18, 1043.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 18191/100000 [00:17<01:19, 1023.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 18304/100000 [00:17<01:19, 1033.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▊        | 18586/100000 [00:17<01:17, 1051.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 18936/100000 [00:18<01:20, 1002.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 18303/100000 [00:17<01:18, 1043.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 18409/100000 [00:17<01:18, 1035.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  18%|█▊        | 18409/100000 [00:17<01:18, 1045.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▊        | 18733/100000 [00:18<01:19, 1021.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 19081/100000 [00:18<01:22, 986.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▊        | 18518/100000 [00:17<01:17, 1048.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▊        | 18518/100000 [00:17<01:17, 1056.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 19185/100000 [00:18<01:21, 993.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 18881/100000 [00:18<01:20, 1007.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▊        | 18671/100000 [00:17<01:18, 1032.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 19296/100000 [00:18<01:18, 1021.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▊        | 18671/100000 [00:17<01:18, 1039.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 19022/100000 [00:18<01:22, 981.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 18812/100000 [00:18<01:21, 990.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 19446/100000 [00:18<01:20, 1006.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 18812/100000 [00:18<01:21, 996.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 19125/100000 [00:18<01:21, 990.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 18914/100000 [00:18<01:21, 992.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|█▉        | 19549/100000 [00:18<01:19, 1007.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 18913/100000 [00:18<01:21, 998.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 19231/100000 [00:18<01:20, 1005.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|█▉        | 19658/100000 [00:18<01:18, 1026.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 19057/100000 [00:18<01:23, 974.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 19049/100000 [00:18<01:24, 963.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 19378/100000 [00:18<01:21, 993.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 19160/100000 [00:18<01:22, 984.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|█▉        | 19806/100000 [00:18<01:19, 1010.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 19153/100000 [00:18<01:22, 976.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 19484/100000 [00:18<01:19, 1006.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 19270/100000 [00:18<01:20, 1007.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 19265/100000 [00:18<01:19, 1009.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|█▉        | 19957/100000 [00:19<01:19, 1003.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|█▉        | 19639/100000 [00:18<01:19, 1011.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 19416/100000 [00:18<01:21, 986.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|██        | 20061/100000 [00:19<01:19, 1009.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  19%|█▉        | 19408/100000 [00:18<01:21, 986.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|█▉        | 19742/100000 [00:19<01:19, 1010.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|█▉        | 19522/100000 [00:18<01:20, 1002.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|██        | 20169/100000 [00:19<01:17, 1024.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|█▉        | 19514/100000 [00:18<01:20, 1003.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|█▉        | 19624/100000 [00:18<01:20, 1002.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|█▉        | 19886/100000 [00:19<01:21, 987.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|█▉        | 19620/100000 [00:18<01:19, 1013.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|██        | 20323/100000 [00:19<01:17, 1023.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|█▉        | 19776/100000 [00:19<01:19, 1004.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|██        | 20038/100000 [00:19<01:20, 991.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|█▉        | 19775/100000 [00:19<01:19, 1013.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|██        | 20479/100000 [00:19<01:17, 1023.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|██        | 20146/100000 [00:19<01:19, 1009.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|█▉        | 19922/100000 [00:19<01:20, 989.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 20586/100000 [00:19<01:17, 1030.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|█▉        | 19922/100000 [00:19<01:20, 997.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|██        | 20255/100000 [00:19<01:18, 1022.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 20699/100000 [00:19<01:15, 1054.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|██        | 20075/100000 [00:19<01:20, 997.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|██        | 20075/100000 [00:19<01:19, 1002.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|██        | 20403/100000 [00:19<01:19, 1006.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|██        | 20184/100000 [00:19<01:18, 1016.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 20851/100000 [00:19<01:16, 1034.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|██        | 20184/100000 [00:19<01:18, 1020.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 20508/100000 [00:19<01:18, 1012.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|██        | 20341/100000 [00:19<01:18, 1018.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 21011/100000 [00:20<01:15, 1042.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 20615/100000 [00:19<01:17, 1024.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|██        | 20341/100000 [00:19<01:18, 1020.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|██        | 20444/100000 [00:19<01:18, 1015.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 21117/100000 [00:20<01:15, 1043.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 20730/100000 [00:20<01:15, 1056.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  20%|██        | 20444/100000 [00:19<01:18, 1017.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 20548/100000 [00:19<01:17, 1018.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 20548/100000 [00:19<01:17, 1021.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██▏       | 21276/100000 [00:20<01:15, 1046.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 20881/100000 [00:20<01:17, 1027.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 20660/100000 [00:19<01:16, 1040.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 20660/100000 [00:19<01:16, 1043.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██▏       | 21387/100000 [00:20<01:14, 1057.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 20772/100000 [00:20<01:14, 1059.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 21041/100000 [00:20<01:16, 1038.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 20772/100000 [00:20<01:14, 1062.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 21540/100000 [00:20<01:15, 1036.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 20918/100000 [00:20<01:17, 1024.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 21193/100000 [00:20<01:16, 1028.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 20918/100000 [00:20<01:17, 1026.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 21646/100000 [00:20<01:15, 1036.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 21027/100000 [00:20<01:16, 1037.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██▏       | 21300/100000 [00:20<01:16, 1035.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 21027/100000 [00:20<01:16, 1037.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 21135/100000 [00:20<01:15, 1039.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██▏       | 21413/100000 [00:20<01:14, 1057.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 21800/100000 [00:20<01:16, 1022.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██        | 21135/100000 [00:20<01:15, 1039.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 21906/100000 [00:20<01:16, 1026.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██▏       | 21289/100000 [00:20<01:16, 1033.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 21562/100000 [00:20<01:16, 1021.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██▏       | 21289/100000 [00:20<01:16, 1033.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 22021/100000 [00:21<01:13, 1057.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██▏       | 21402/100000 [00:20<01:14, 1057.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 21667/100000 [00:20<01:16, 1021.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  21%|██▏       | 21401/100000 [00:20<01:14, 1054.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 22179/100000 [00:21<01:14, 1046.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 21551/100000 [00:20<01:16, 1024.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 21820/100000 [00:21<01:16, 1018.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 21551/100000 [00:20<01:16, 1026.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 22289/100000 [00:21<01:13, 1058.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 21655/100000 [00:20<01:16, 1026.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 21923/100000 [00:21<01:16, 1019.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 21655/100000 [00:20<01:16, 1028.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 22035/100000 [00:21<01:14, 1044.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 22456/100000 [00:21<01:12, 1075.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 21807/100000 [00:21<01:17, 1013.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 21806/100000 [00:21<01:17, 1012.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 22565/100000 [00:21<01:11, 1076.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 21912/100000 [00:21<01:16, 1018.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 22192/100000 [00:21<01:15, 1036.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 21910/100000 [00:21<01:16, 1016.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 22677/100000 [00:21<01:11, 1084.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 22028/100000 [00:21<01:14, 1051.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 22302/100000 [00:21<01:14, 1046.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 22025/100000 [00:21<01:14, 1050.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 22410/100000 [00:21<01:13, 1051.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 22836/100000 [00:21<01:12, 1066.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 22180/100000 [00:21<01:15, 1034.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 22179/100000 [00:21<01:15, 1033.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 22519/100000 [00:21<01:13, 1060.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 22291/100000 [00:21<01:14, 1049.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 22990/100000 [00:21<01:13, 1044.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 22289/100000 [00:21<01:14, 1048.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 22630/100000 [00:21<01:12, 1073.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 22398/100000 [00:21<01:13, 1054.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 23097/100000 [00:22<01:13, 1045.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 22739/100000 [00:21<01:11, 1074.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  22%|██▏       | 22456/100000 [00:21<01:12, 1069.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 22509/100000 [00:21<01:12, 1064.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 23210/100000 [00:22<01:12, 1061.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 22564/100000 [00:21<01:12, 1071.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 22625/100000 [00:21<01:11, 1084.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 22892/100000 [00:22<01:13, 1045.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 23368/100000 [00:22<01:12, 1055.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 22675/100000 [00:21<01:11, 1075.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 22784/100000 [00:21<01:11, 1073.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 23046/100000 [00:22<01:14, 1029.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 23478/100000 [00:22<01:11, 1064.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 22830/100000 [00:21<01:13, 1056.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 23163/100000 [00:22<01:12, 1061.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▎       | 23588/100000 [00:22<01:11, 1071.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 22940/100000 [00:22<01:12, 1057.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 22981/100000 [00:22<01:14, 1036.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 23751/100000 [00:22<01:10, 1074.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 23316/100000 [00:22<01:13, 1042.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 23095/100000 [00:22<01:13, 1045.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 23089/100000 [00:22<01:13, 1044.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 23423/100000 [00:22<01:13, 1043.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 23207/100000 [00:22<01:12, 1058.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 23911/100000 [00:22<01:11, 1069.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 23201/100000 [00:22<01:12, 1063.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▎       | 23533/100000 [00:22<01:12, 1054.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 24020/100000 [00:22<01:10, 1072.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 23359/100000 [00:22<01:13, 1042.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▎       | 23640/100000 [00:22<01:12, 1055.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 23355/100000 [00:22<01:13, 1044.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 24136/100000 [00:23<01:09, 1091.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 23466/100000 [00:22<01:13, 1044.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 23752/100000 [00:22<01:11, 1069.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  23%|██▎       | 23465/100000 [00:22<01:12, 1056.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▎       | 23572/100000 [00:22<01:13, 1046.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 24296/100000 [00:23<01:10, 1075.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 23911/100000 [00:23<01:11, 1062.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▎       | 23630/100000 [00:22<01:11, 1067.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▎       | 23678/100000 [00:22<01:12, 1048.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 24019/100000 [00:23<01:11, 1065.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 24459/100000 [00:23<01:10, 1072.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 23785/100000 [00:22<01:12, 1051.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 23790/100000 [00:22<01:11, 1061.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 24133/100000 [00:23<01:10, 1082.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▍       | 24613/100000 [00:23<01:11, 1051.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 23897/100000 [00:22<01:11, 1058.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 23948/100000 [00:23<01:11, 1057.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 24293/100000 [00:23<01:10, 1069.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▍       | 24727/100000 [00:23<01:10, 1068.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 24009/100000 [00:23<01:10, 1073.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 24059/100000 [00:23<01:10, 1070.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▍       | 24836/100000 [00:23<01:10, 1069.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 24122/100000 [00:23<01:09, 1084.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 24169/100000 [00:23<01:10, 1072.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 24451/100000 [00:23<01:11, 1059.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▍       | 24992/100000 [00:23<01:10, 1057.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 24276/100000 [00:23<01:11, 1061.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 24329/100000 [00:23<01:11, 1061.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▍       | 24608/100000 [00:23<01:11, 1051.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▌       | 25149/100000 [00:24<01:11, 1049.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 24430/100000 [00:23<01:12, 1046.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  24%|██▍       | 24488/100000 [00:23<01:11, 1055.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▍       | 24769/100000 [00:23<01:11, 1053.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▌       | 25259/100000 [00:24<01:10, 1057.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▍       | 24538/100000 [00:23<01:11, 1053.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▍       | 24877/100000 [00:24<01:11, 1057.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▍       | 24639/100000 [00:23<01:12, 1036.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▌       | 25418/100000 [00:24<01:10, 1056.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▍       | 24697/100000 [00:23<01:11, 1048.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▍       | 24748/100000 [00:23<01:11, 1045.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▌       | 25033/100000 [00:24<01:11, 1044.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 25525/100000 [00:24<01:10, 1054.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▍       | 24859/100000 [00:23<01:11, 1058.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▍       | 24859/100000 [00:23<01:11, 1053.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▌       | 25190/100000 [00:24<01:11, 1041.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 25680/100000 [00:24<01:11, 1041.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▌       | 25013/100000 [00:24<01:12, 1038.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▌       | 25013/100000 [00:24<01:12, 1038.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 25785/100000 [00:24<01:11, 1041.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▌       | 25344/100000 [00:24<01:12, 1029.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▌       | 25171/100000 [00:24<01:12, 1037.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 25896/100000 [00:24<01:10, 1058.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▌       | 25457/100000 [00:24<01:11, 1049.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▌       | 25172/100000 [00:24<01:12, 1037.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▌       | 25278/100000 [00:24<01:11, 1043.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 26006/100000 [00:24<01:09, 1066.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▌       | 25281/100000 [00:24<01:11, 1047.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 25614/100000 [00:24<01:11, 1043.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▌       | 25438/100000 [00:24<01:11, 1041.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 26167/100000 [00:24<01:09, 1057.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 25720/100000 [00:24<01:11, 1035.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  25%|██▌       | 25439/100000 [00:24<01:11, 1044.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 25543/100000 [00:24<01:11, 1040.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 25827/100000 [00:24<01:11, 1042.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▋       | 26317/100000 [00:25<01:11, 1035.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 25595/100000 [00:24<01:11, 1041.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 25650/100000 [00:24<01:11, 1039.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 25942/100000 [00:25<01:09, 1067.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▋       | 26473/100000 [00:25<01:11, 1033.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 25756/100000 [00:24<01:11, 1044.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 25800/100000 [00:24<01:12, 1024.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 26096/100000 [00:25<01:10, 1046.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 26580/100000 [00:25<01:10, 1041.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 25914/100000 [00:24<01:10, 1050.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 25916/100000 [00:24<01:10, 1048.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 26686/100000 [00:25<01:10, 1042.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 26247/100000 [00:25<01:11, 1026.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 26020/100000 [00:25<01:10, 1049.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 26026/100000 [00:25<01:10, 1054.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 26794/100000 [00:25<01:09, 1051.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▋       | 26397/100000 [00:25<01:12, 1014.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 26177/100000 [00:25<01:10, 1044.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▌       | 26183/100000 [00:25<01:10, 1045.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 26950/100000 [00:25<01:10, 1039.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 26504/100000 [00:25<01:11, 1026.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▋       | 26325/100000 [00:25<01:12, 1021.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 27057/100000 [00:25<01:09, 1044.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 26614/100000 [00:25<01:10, 1041.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▋       | 26336/100000 [00:25<01:11, 1033.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 27167/100000 [00:25<01:08, 1055.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▋       | 26479/100000 [00:25<01:12, 1020.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 26770/100000 [00:25<01:10, 1037.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  26%|██▋       | 26487/100000 [00:25<01:11, 1022.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 26585/100000 [00:25<01:11, 1027.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 27329/100000 [00:26<01:08, 1061.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 26599/100000 [00:25<01:10, 1037.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 26922/100000 [00:25<01:11, 1025.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 26690/100000 [00:25<01:11, 1029.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 27482/100000 [00:26<01:09, 1036.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 27031/100000 [00:26<01:10, 1036.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 26757/100000 [00:25<01:10, 1039.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 26798/100000 [00:25<01:10, 1037.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 27588/100000 [00:26<01:09, 1037.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 27140/100000 [00:26<01:10, 1040.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 26911/100000 [00:25<01:10, 1029.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 26951/100000 [00:25<01:11, 1028.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 27695/100000 [00:26<01:09, 1039.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 27246/100000 [00:26<01:09, 1043.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 27018/100000 [00:25<01:10, 1037.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 27056/100000 [00:26<01:10, 1030.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 27353/100000 [00:26<01:09, 1046.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 27839/100000 [00:26<01:11, 1005.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 27123/100000 [00:26<01:10, 1035.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 27165/100000 [00:26<01:09, 1041.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 27950/100000 [00:26<01:10, 1028.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 27503/100000 [00:26<01:10, 1026.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 27235/100000 [00:26<01:09, 1054.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 27325/100000 [00:26<01:09, 1048.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28060/100000 [00:26<01:08, 1043.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 27608/100000 [00:26<01:10, 1031.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 27343/100000 [00:26<01:08, 1055.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28166/100000 [00:26<01:08, 1044.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 27473/100000 [00:26<01:10, 1022.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 27757/100000 [00:26<01:11, 1006.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  27%|██▋       | 27496/100000 [00:26<01:09, 1038.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 27576/100000 [00:26<01:10, 1021.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28324/100000 [00:27<01:08, 1044.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 27910/100000 [00:26<01:11, 1006.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 27645/100000 [00:26<01:10, 1019.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 27680/100000 [00:26<01:10, 1021.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28438/100000 [00:27<01:07, 1065.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28020/100000 [00:27<01:10, 1022.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 27752/100000 [00:26<01:10, 1029.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 27822/100000 [00:26<01:12, 991.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▊       | 28593/100000 [00:27<01:07, 1051.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28175/100000 [00:27<01:10, 1018.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 27906/100000 [00:26<01:10, 1025.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 27931/100000 [00:26<01:11, 1013.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▊       | 28702/100000 [00:27<01:07, 1058.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28279/100000 [00:27<01:10, 1018.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28015/100000 [00:26<01:09, 1036.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28040/100000 [00:27<01:09, 1031.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 28811/100000 [00:27<01:06, 1062.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28386/100000 [00:27<01:09, 1027.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28170/100000 [00:27<01:09, 1030.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28198/100000 [00:27<01:09, 1037.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 28964/100000 [00:27<01:08, 1039.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28493/100000 [00:27<01:09, 1036.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28275/100000 [00:27<01:09, 1034.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 29077/100000 [00:27<01:07, 1054.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▊       | 28600/100000 [00:27<01:08, 1039.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28352/100000 [00:27<01:09, 1030.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28380/100000 [00:27<01:09, 1036.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▊       | 28710/100000 [00:27<01:08, 1047.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28459/100000 [00:27<01:08, 1039.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 29230/100000 [00:27<01:07, 1040.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  28%|██▊       | 28490/100000 [00:27<01:08, 1049.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 28819/100000 [00:27<01:07, 1057.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 29346/100000 [00:28<01:06, 1070.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▊       | 28598/100000 [00:27<01:07, 1054.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▊       | 28620/100000 [00:27<01:08, 1044.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 29459/100000 [00:28<01:05, 1084.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 28966/100000 [00:27<01:09, 1020.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▊       | 28707/100000 [00:27<01:07, 1063.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▊       | 28726/100000 [00:27<01:08, 1046.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 29079/100000 [00:28<01:07, 1045.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 28814/100000 [00:27<01:07, 1061.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 28839/100000 [00:27<01:06, 1065.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|██▉       | 29613/100000 [00:28<01:06, 1060.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|██▉       | 29724/100000 [00:28<01:05, 1067.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 29230/100000 [00:28<01:08, 1029.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 28966/100000 [00:27<01:08, 1030.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 28980/100000 [00:27<01:10, 1010.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 29345/100000 [00:28<01:06, 1057.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 29079/100000 [00:27<01:07, 1053.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 29091/100000 [00:28<01:08, 1030.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|██▉       | 29886/100000 [00:28<01:05, 1069.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 29456/100000 [00:28<01:05, 1069.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 29232/100000 [00:28<01:08, 1039.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 29244/100000 [00:28<01:09, 1024.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|███       | 30044/100000 [00:28<01:05, 1062.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|██▉       | 29611/100000 [00:28<01:06, 1051.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 29346/100000 [00:28<01:06, 1064.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 29358/100000 [00:28<01:07, 1050.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|███       | 30204/100000 [00:28<01:05, 1057.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|██▉       | 29720/100000 [00:28<01:06, 1056.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 29458/100000 [00:28<01:05, 1077.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  29%|██▉       | 29471/100000 [00:28<01:06, 1067.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|██▉       | 29827/100000 [00:28<01:06, 1059.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|███       | 30363/100000 [00:28<01:06, 1053.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|██▉       | 29613/100000 [00:28<01:06, 1056.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|██▉       | 29620/100000 [00:28<01:07, 1035.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|███       | 30472/100000 [00:29<01:05, 1059.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|██▉       | 29981/100000 [00:28<01:07, 1041.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|██▉       | 29724/100000 [00:28<01:06, 1062.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|██▉       | 29727/100000 [00:28<01:07, 1044.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 30580/100000 [00:29<01:05, 1061.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|███       | 30087/100000 [00:29<01:07, 1037.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|██▉       | 29834/100000 [00:28<01:07, 1046.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|██▉       | 29885/100000 [00:28<01:05, 1065.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 30690/100000 [00:29<01:05, 1063.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|███       | 30195/100000 [00:29<01:06, 1047.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|██▉       | 29994/100000 [00:28<01:07, 1041.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 30806/100000 [00:29<01:03, 1088.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|███       | 30042/100000 [00:28<01:06, 1055.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|███       | 30353/100000 [00:29<01:06, 1043.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 30916/100000 [00:29<01:03, 1085.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|███       | 30151/100000 [00:29<01:07, 1035.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|███       | 30463/100000 [00:29<01:06, 1047.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|███       | 30202/100000 [00:29<01:06, 1055.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|███       | 30257/100000 [00:29<01:07, 1038.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 31071/100000 [00:29<01:04, 1062.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 30570/100000 [00:29<01:06, 1049.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|███       | 30360/100000 [00:29<01:06, 1046.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 31180/100000 [00:29<01:04, 1066.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 30676/100000 [00:29<01:06, 1045.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|███       | 30413/100000 [00:29<01:07, 1037.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  30%|███       | 30470/100000 [00:29<01:05, 1054.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███▏      | 31288/100000 [00:29<01:04, 1068.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 30789/100000 [00:29<01:04, 1066.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 30522/100000 [00:29<01:06, 1046.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 30577/100000 [00:29<01:05, 1055.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 30899/100000 [00:29<01:04, 1075.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 30630/100000 [00:29<01:05, 1051.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███▏      | 31441/100000 [00:29<01:05, 1046.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 30744/100000 [00:29<01:04, 1070.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 30742/100000 [00:29<01:04, 1066.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 31549/100000 [00:30<01:05, 1053.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 31053/100000 [00:29<01:05, 1051.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 30855/100000 [00:29<01:04, 1078.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 30852/100000 [00:29<01:04, 1071.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 31655/100000 [00:30<01:05, 1049.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 31160/100000 [00:30<01:05, 1052.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 30964/100000 [00:29<01:04, 1078.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 30960/100000 [00:29<01:04, 1071.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 31764/100000 [00:30<01:04, 1059.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███▏      | 31322/100000 [00:30<01:04, 1058.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 31879/100000 [00:30<01:03, 1079.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 31117/100000 [00:29<01:05, 1056.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 31113/100000 [00:29<01:05, 1047.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███▏      | 31470/100000 [00:30<01:06, 1030.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 31226/100000 [00:29<01:05, 1056.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███       | 31220/100000 [00:30<01:05, 1049.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 32040/100000 [00:30<01:03, 1074.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 31577/100000 [00:30<01:06, 1035.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███▏      | 31336/100000 [00:30<01:04, 1067.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███▏      | 31330/100000 [00:30<01:04, 1060.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 32200/100000 [00:30<01:03, 1066.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 31685/100000 [00:30<01:05, 1041.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███▏      | 31486/100000 [00:30<01:05, 1041.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  31%|███▏      | 31480/100000 [00:30<01:06, 1030.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 32309/100000 [00:30<01:03, 1070.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 31798/100000 [00:30<01:04, 1058.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 31591/100000 [00:30<01:05, 1036.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 31587/100000 [00:30<01:05, 1038.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 31905/100000 [00:30<01:04, 1058.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 32468/100000 [00:30<01:03, 1059.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 31700/100000 [00:30<01:05, 1042.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 31739/100000 [00:30<01:06, 1023.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 32576/100000 [00:31<01:03, 1062.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 32065/100000 [00:30<01:04, 1059.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 31816/100000 [00:30<01:03, 1071.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 31857/100000 [00:30<01:04, 1059.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 32737/100000 [00:31<01:03, 1063.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 32220/100000 [00:31<01:04, 1047.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 31976/100000 [00:30<01:03, 1067.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 32019/100000 [00:30<01:04, 1059.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 32844/100000 [00:31<01:03, 1059.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 32371/100000 [00:31<01:05, 1032.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 32134/100000 [00:30<01:04, 1059.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 32960/100000 [00:31<01:02, 1081.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 32172/100000 [00:31<01:04, 1043.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 32482/100000 [00:31<01:04, 1046.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 32296/100000 [00:31<01:03, 1063.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 32278/100000 [00:31<01:04, 1045.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 33125/100000 [00:31<01:01, 1086.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 32634/100000 [00:31<01:05, 1030.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 32450/100000 [00:31<01:04, 1044.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  32%|███▏      | 32428/100000 [00:31<01:05, 1025.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 33278/100000 [00:31<01:02, 1059.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 32742/100000 [00:31<01:04, 1040.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 32558/100000 [00:31<01:04, 1042.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 32537/100000 [00:31<01:04, 1038.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 33387/100000 [00:31<01:02, 1064.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 32848/100000 [00:31<01:04, 1042.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 32665/100000 [00:31<01:04, 1044.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▎      | 33500/100000 [00:31<01:01, 1076.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 32961/100000 [00:31<01:03, 1062.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 32693/100000 [00:31<01:05, 1033.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 32775/100000 [00:31<01:03, 1056.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▎      | 33620/100000 [00:32<01:00, 1102.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 32806/100000 [00:31<01:03, 1052.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 33126/100000 [00:31<01:02, 1071.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 32882/100000 [00:31<01:03, 1055.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▎      | 33739/100000 [00:32<00:59, 1119.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 32998/100000 [00:31<01:01, 1083.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 32969/100000 [00:31<01:03, 1062.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 33278/100000 [00:32<01:03, 1047.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 33904/100000 [00:32<00:59, 1105.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 33386/100000 [00:32<01:03, 1052.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 33156/100000 [00:31<01:02, 1067.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 33132/100000 [00:31<01:02, 1069.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 34071/100000 [00:32<00:59, 1104.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 33497/100000 [00:32<01:02, 1065.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 33307/100000 [00:31<01:03, 1042.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 33281/100000 [00:32<01:04, 1041.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▎      | 33613/100000 [00:32<01:00, 1089.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 34233/100000 [00:32<01:00, 1092.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 33416/100000 [00:32<01:03, 1052.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  33%|███▎      | 33390/100000 [00:32<01:03, 1048.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▎      | 33730/100000 [00:32<00:59, 1107.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▎      | 33525/100000 [00:32<01:02, 1058.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▎      | 33503/100000 [00:32<01:02, 1062.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 34385/100000 [00:32<01:01, 1063.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 33890/100000 [00:32<01:00, 1085.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▎      | 33645/100000 [00:32<01:00, 1094.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▎      | 33620/100000 [00:32<01:01, 1085.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 34498/100000 [00:32<01:00, 1076.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 34001/100000 [00:32<01:00, 1090.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▎      | 33738/100000 [00:32<00:59, 1104.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▍      | 34608/100000 [00:32<01:00, 1079.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 33810/100000 [00:32<01:00, 1090.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▍      | 34720/100000 [00:33<00:59, 1088.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 34160/100000 [00:32<01:01, 1073.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 33921/100000 [00:32<01:00, 1091.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 33896/100000 [00:32<01:00, 1083.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 34271/100000 [00:32<01:00, 1078.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 34007/100000 [00:32<01:00, 1086.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▍      | 34866/100000 [00:33<01:02, 1042.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 34085/100000 [00:32<01:00, 1086.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▍      | 34978/100000 [00:33<01:01, 1059.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 34421/100000 [00:33<01:02, 1044.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 34165/100000 [00:32<01:01, 1070.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 34243/100000 [00:32<01:01, 1070.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▍      | 34539/100000 [00:33<01:00, 1077.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 34275/100000 [00:32<01:01, 1076.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▌      | 35132/100000 [00:33<01:02, 1043.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 34396/100000 [00:32<01:02, 1051.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▌      | 35243/100000 [00:33<01:01, 1059.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▍      | 34705/100000 [00:33<01:00, 1080.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  34%|███▍      | 34422/100000 [00:33<01:03, 1036.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▍      | 34505/100000 [00:33<01:01, 1059.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▌      | 35353/100000 [00:33<01:00, 1068.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▍      | 34540/100000 [00:33<01:01, 1068.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▍      | 34849/100000 [00:33<01:02, 1037.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▍      | 34612/100000 [00:33<01:01, 1057.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35506/100000 [00:33<01:01, 1048.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▍      | 34956/100000 [00:33<01:02, 1041.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▍      | 34724/100000 [00:33<01:01, 1069.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▍      | 34704/100000 [00:33<01:00, 1073.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▌      | 35062/100000 [00:33<01:02, 1040.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35648/100000 [00:33<01:03, 1008.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▍      | 34868/100000 [00:33<01:03, 1028.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▍      | 34848/100000 [00:33<01:03, 1027.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▌      | 35171/100000 [00:33<01:01, 1050.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35758/100000 [00:34<01:02, 1028.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▍      | 34979/100000 [00:33<01:02, 1045.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▍      | 34952/100000 [00:33<01:03, 1029.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35872/100000 [00:34<01:00, 1054.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▌      | 35332/100000 [00:34<01:01, 1053.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▌      | 35060/100000 [00:33<01:02, 1038.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▌      | 35131/100000 [00:33<01:03, 1028.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35981/100000 [00:34<01:00, 1059.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▌      | 35168/100000 [00:33<01:02, 1045.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▌      | 35483/100000 [00:34<01:02, 1033.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▌      | 35240/100000 [00:33<01:02, 1040.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 36090/100000 [00:34<01:00, 1064.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▌      | 35351/100000 [00:33<01:01, 1056.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▌      | 35327/100000 [00:33<01:01, 1047.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35629/100000 [00:34<01:03, 1012.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▋      | 36252/100000 [00:34<00:59, 1067.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35733/100000 [00:34<01:03, 1014.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35501/100000 [00:34<01:02, 1033.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  35%|███▌      | 35478/100000 [00:34<01:02, 1030.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▋      | 36407/100000 [00:34<01:00, 1050.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35846/100000 [00:34<01:01, 1042.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35644/100000 [00:34<01:04, 1002.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35621/100000 [00:34<01:04, 1000.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35956/100000 [00:34<01:00, 1055.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 36568/100000 [00:34<01:00, 1052.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35753/100000 [00:34<01:02, 1020.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35728/100000 [00:34<01:03, 1010.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 36065/100000 [00:34<01:00, 1059.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 36680/100000 [00:34<00:59, 1064.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35864/100000 [00:34<01:01, 1041.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35837/100000 [00:34<01:02, 1026.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 36220/100000 [00:34<01:01, 1043.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35974/100000 [00:34<01:00, 1051.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 35950/100000 [00:34<01:01, 1047.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 36828/100000 [00:35<01:01, 1030.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▋      | 36331/100000 [00:34<01:00, 1055.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 36082/100000 [00:34<01:00, 1054.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 36060/100000 [00:34<01:00, 1058.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 36933/100000 [00:35<01:00, 1034.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 36190/100000 [00:34<01:00, 1053.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 37046/100000 [00:35<00:59, 1057.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▋      | 36486/100000 [00:35<01:00, 1041.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▌      | 36211/100000 [00:34<01:01, 1035.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 37170/100000 [00:35<00:57, 1101.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 36599/100000 [00:35<00:59, 1061.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▋      | 36344/100000 [00:34<01:01, 1041.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▋      | 36323/100000 [00:34<01:00, 1049.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 36707/100000 [00:35<00:59, 1061.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 37332/100000 [00:35<00:57, 1090.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▋      | 36498/100000 [00:35<01:01, 1035.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  36%|███▋      | 36475/100000 [00:35<01:01, 1031.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 36844/100000 [00:35<01:02, 1005.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 36612/100000 [00:35<00:59, 1058.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 36587/100000 [00:35<01:00, 1050.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 37493/100000 [00:35<00:57, 1077.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 36958/100000 [00:35<01:00, 1035.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 36695/100000 [00:35<00:59, 1055.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 36762/100000 [00:35<01:01, 1034.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 37650/100000 [00:35<00:58, 1065.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 37072/100000 [00:35<00:59, 1062.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 36837/100000 [00:35<01:02, 1011.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 37763/100000 [00:35<00:57, 1074.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 37187/100000 [00:35<00:57, 1085.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 36912/100000 [00:35<01:01, 1017.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 36943/100000 [00:35<01:01, 1021.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 37875/100000 [00:36<00:57, 1084.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 37297/100000 [00:35<00:57, 1086.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 37024/100000 [00:35<01:00, 1038.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 37054/100000 [00:35<01:00, 1044.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 37150/100000 [00:35<00:57, 1090.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 38034/100000 [00:36<00:57, 1071.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 37453/100000 [00:36<00:58, 1064.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 37175/100000 [00:35<00:57, 1088.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 38144/100000 [00:36<00:57, 1075.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 37312/100000 [00:35<00:58, 1076.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 37614/100000 [00:36<00:58, 1058.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 37336/100000 [00:35<00:58, 1079.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 38302/100000 [00:36<00:58, 1063.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 37721/100000 [00:36<00:58, 1060.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 37473/100000 [00:35<00:58, 1065.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  37%|███▋      | 37494/100000 [00:36<00:58, 1066.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 38410/100000 [00:36<00:57, 1062.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 37832/100000 [00:36<00:57, 1072.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 37632/100000 [00:36<00:58, 1060.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▊      | 38517/100000 [00:36<00:57, 1062.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 37940/100000 [00:36<00:57, 1070.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 37650/100000 [00:36<00:59, 1052.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▊      | 38627/100000 [00:36<00:57, 1068.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 38049/100000 [00:36<00:57, 1073.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 37796/100000 [00:36<00:58, 1068.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 37760/100000 [00:36<00:58, 1061.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 38157/100000 [00:36<00:57, 1072.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 37910/100000 [00:36<00:57, 1077.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 37870/100000 [00:36<00:58, 1069.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 38790/100000 [00:36<00:57, 1070.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 38313/100000 [00:36<00:58, 1056.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 38070/100000 [00:36<00:57, 1067.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 38029/100000 [00:36<00:58, 1061.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 38945/100000 [00:37<00:58, 1050.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 38420/100000 [00:36<00:58, 1053.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 38136/100000 [00:36<00:58, 1062.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 39059/100000 [00:37<00:56, 1071.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 38224/100000 [00:36<00:58, 1048.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▊      | 38530/100000 [00:37<00:57, 1061.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 39175/100000 [00:37<00:55, 1090.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 38336/100000 [00:36<00:58, 1061.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 38290/100000 [00:36<00:58, 1046.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▊      | 38691/100000 [00:37<00:57, 1064.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 38401/100000 [00:36<00:58, 1060.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 39333/100000 [00:37<00:56, 1073.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  38%|███▊      | 38491/100000 [00:36<00:58, 1049.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 38798/100000 [00:37<00:57, 1063.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▊      | 38609/100000 [00:36<00:56, 1078.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▊      | 38567/100000 [00:37<00:57, 1070.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 39490/100000 [00:37<00:57, 1060.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 38948/100000 [00:37<00:58, 1037.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 38763/100000 [00:37<00:57, 1056.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▊      | 38723/100000 [00:37<00:58, 1050.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|███▉      | 39646/100000 [00:37<00:57, 1048.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 39067/100000 [00:37<00:56, 1073.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 38832/100000 [00:37<00:57, 1055.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|███▉      | 39763/100000 [00:37<00:56, 1075.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 39180/100000 [00:37<00:55, 1087.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 38926/100000 [00:37<00:57, 1053.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|███▉      | 39872/100000 [00:37<00:55, 1075.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 39037/100000 [00:37<00:57, 1065.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 38988/100000 [00:37<00:58, 1045.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 39335/100000 [00:37<00:57, 1063.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|███▉      | 39984/100000 [00:37<00:55, 1080.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 39150/100000 [00:37<00:56, 1072.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 39098/100000 [00:37<00:57, 1057.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 39491/100000 [00:37<00:57, 1052.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 39259/100000 [00:37<00:56, 1075.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 39212/100000 [00:37<00:56, 1076.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|████      | 40135/100000 [00:38<00:56, 1052.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|████      | 40247/100000 [00:38<00:55, 1067.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|███▉      | 39645/100000 [00:38<00:57, 1043.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 39410/100000 [00:37<00:57, 1047.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 39364/100000 [00:37<00:57, 1050.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|████      | 40357/100000 [00:38<00:55, 1073.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|███▉      | 39760/100000 [00:38<00:56, 1067.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|███▉      | 39518/100000 [00:37<00:57, 1052.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  39%|███▉      | 39473/100000 [00:37<00:57, 1056.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|███▉      | 39869/100000 [00:38<00:56, 1069.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 40520/100000 [00:38<00:55, 1074.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|███▉      | 39677/100000 [00:37<00:57, 1049.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|███▉      | 39627/100000 [00:38<00:58, 1039.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|███▉      | 39980/100000 [00:38<00:55, 1078.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|███▉      | 39795/100000 [00:38<00:55, 1080.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|███▉      | 39737/100000 [00:38<00:57, 1052.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 40678/100000 [00:38<00:55, 1062.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|████      | 40132/100000 [00:38<00:57, 1044.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|███▉      | 39850/100000 [00:38<00:56, 1068.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|███▉      | 39953/100000 [00:38<00:56, 1065.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 40827/100000 [00:38<00:57, 1034.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|████      | 40242/100000 [00:38<00:56, 1057.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|███▉      | 39959/100000 [00:38<00:56, 1071.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 40937/100000 [00:38<00:56, 1047.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|████      | 40353/100000 [00:38<00:55, 1070.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|████      | 40107/100000 [00:38<00:57, 1047.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|████      | 40110/100000 [00:38<00:57, 1044.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|████      | 40213/100000 [00:38<00:56, 1049.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 41096/100000 [00:39<00:56, 1048.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 40513/100000 [00:38<00:55, 1063.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|████      | 40217/100000 [00:38<00:56, 1049.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|████      | 40327/100000 [00:38<00:55, 1071.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|████      | 40331/100000 [00:38<00:55, 1068.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████▏     | 41250/100000 [00:39<00:56, 1040.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 40670/100000 [00:39<00:56, 1053.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|████      | 40487/100000 [00:38<00:55, 1066.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████▏     | 41364/100000 [00:39<00:55, 1062.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  40%|████      | 40490/100000 [00:38<00:55, 1062.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 40818/100000 [00:39<00:57, 1025.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████▏     | 41475/100000 [00:39<00:54, 1071.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 40647/100000 [00:38<00:55, 1064.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 40926/100000 [00:39<00:56, 1037.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 40650/100000 [00:39<00:56, 1057.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 41639/100000 [00:39<00:54, 1076.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 40787/100000 [00:39<00:57, 1021.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 41084/100000 [00:39<00:56, 1035.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 40793/100000 [00:39<00:58, 1016.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 40895/100000 [00:39<00:57, 1033.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 41800/100000 [00:39<00:54, 1064.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 40902/100000 [00:39<00:57, 1032.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 41236/100000 [00:39<00:57, 1025.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 41000/100000 [00:39<00:57, 1029.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 41959/100000 [00:39<00:54, 1060.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████▏     | 41351/100000 [00:39<00:55, 1052.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 41110/100000 [00:39<00:56, 1041.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 41057/100000 [00:39<00:57, 1031.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████▏     | 41461/100000 [00:39<00:55, 1063.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████      | 41162/100000 [00:39<00:56, 1033.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 42117/100000 [00:40<00:54, 1055.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████▏     | 41270/100000 [00:39<00:56, 1043.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████▏     | 41269/100000 [00:39<00:56, 1042.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 42230/100000 [00:40<00:53, 1070.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 41623/100000 [00:39<00:54, 1065.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████▏     | 41380/100000 [00:39<00:55, 1053.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████▏     | 41378/100000 [00:39<00:55, 1051.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 42340/100000 [00:40<00:53, 1069.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████▏     | 41488/100000 [00:39<00:55, 1054.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 41783/100000 [00:40<00:54, 1064.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  41%|████▏     | 41484/100000 [00:39<00:55, 1052.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 41599/100000 [00:39<00:54, 1068.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 42493/100000 [00:40<00:55, 1043.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 41594/100000 [00:39<00:55, 1059.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 41940/100000 [00:40<00:54, 1056.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 42608/100000 [00:40<00:53, 1067.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 41763/100000 [00:39<00:54, 1071.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 41703/100000 [00:40<00:54, 1063.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 42095/100000 [00:40<00:55, 1048.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 42760/100000 [00:40<00:54, 1043.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 41915/100000 [00:40<00:55, 1050.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 41855/100000 [00:40<00:56, 1037.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 42205/100000 [00:40<00:54, 1058.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 42871/100000 [00:40<00:54, 1056.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 41963/100000 [00:40<00:55, 1041.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 42316/100000 [00:40<00:54, 1066.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 42070/100000 [00:40<00:55, 1042.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 42069/100000 [00:40<00:55, 1046.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 43036/100000 [00:40<00:53, 1068.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 42179/100000 [00:40<00:54, 1051.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 42470/100000 [00:40<00:55, 1045.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 42175/100000 [00:40<00:55, 1048.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 43150/100000 [00:40<00:52, 1083.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 42292/100000 [00:40<00:53, 1069.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 42580/100000 [00:40<00:54, 1056.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 42290/100000 [00:40<00:54, 1067.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 43270/100000 [00:41<00:51, 1109.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 42446/100000 [00:40<00:54, 1048.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 42734/100000 [00:41<00:55, 1037.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  42%|████▏     | 42443/100000 [00:40<00:55, 1044.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 43425/100000 [00:41<00:52, 1077.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 42842/100000 [00:41<00:54, 1047.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 42609/100000 [00:40<00:54, 1057.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 42606/100000 [00:40<00:54, 1056.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▎     | 43578/100000 [00:41<00:53, 1054.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 43000/100000 [00:41<00:54, 1046.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 42759/100000 [00:40<00:55, 1037.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▎     | 43690/100000 [00:41<00:52, 1065.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 42756/100000 [00:41<00:55, 1032.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 43120/100000 [00:41<00:52, 1079.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 42870/100000 [00:41<00:54, 1049.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 42868/100000 [00:41<00:54, 1051.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 43836/100000 [00:41<00:54, 1029.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 43241/100000 [00:41<00:51, 1109.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 43033/100000 [00:41<00:53, 1058.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 43942/100000 [00:41<00:54, 1029.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 43028/100000 [00:41<00:54, 1052.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 43392/100000 [00:41<00:52, 1070.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 43148/100000 [00:41<00:52, 1078.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 44052/100000 [00:41<00:53, 1044.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 43142/100000 [00:41<00:53, 1072.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 43266/100000 [00:41<00:51, 1101.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▎     | 43542/100000 [00:41<00:54, 1044.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 43261/100000 [00:41<00:51, 1100.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 44210/100000 [00:41<00:53, 1045.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▎     | 43658/100000 [00:41<00:52, 1065.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 43420/100000 [00:41<00:53, 1066.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  43%|████▎     | 43410/100000 [00:41<00:53, 1060.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 44360/100000 [00:42<00:54, 1028.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 43808/100000 [00:42<00:54, 1037.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▎     | 43573/100000 [00:41<00:54, 1043.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 44467/100000 [00:42<00:53, 1036.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▎     | 43560/100000 [00:41<00:54, 1037.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▎     | 43684/100000 [00:41<00:53, 1058.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 43960/100000 [00:42<00:54, 1022.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▎     | 43676/100000 [00:41<00:53, 1062.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▍     | 44621/100000 [00:42<00:54, 1024.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 44070/100000 [00:42<00:54, 1032.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 43830/100000 [00:41<00:54, 1024.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▍     | 44730/100000 [00:42<00:53, 1037.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 43821/100000 [00:42<00:54, 1027.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 44175/100000 [00:42<00:54, 1033.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 43935/100000 [00:42<00:54, 1024.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▍     | 44838/100000 [00:42<00:52, 1047.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 43926/100000 [00:42<00:54, 1028.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 44045/100000 [00:42<00:53, 1040.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▍     | 44944/100000 [00:42<00:52, 1045.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 44327/100000 [00:42<00:54, 1024.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 44033/100000 [00:42<00:54, 1034.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▌     | 45060/100000 [00:42<00:51, 1071.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 44200/100000 [00:42<00:53, 1033.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 44485/100000 [00:42<00:54, 1027.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 44188/100000 [00:42<00:54, 1030.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▌     | 45169/100000 [00:42<00:51, 1067.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 44351/100000 [00:42<00:54, 1022.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▍     | 44639/100000 [00:42<00:54, 1025.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 44336/100000 [00:42<00:55, 1010.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▌     | 45326/100000 [00:43<00:51, 1053.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 44454/100000 [00:42<00:54, 1021.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▍     | 44744/100000 [00:42<00:53, 1028.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  44%|████▍     | 44440/100000 [00:42<00:54, 1015.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▌     | 45442/100000 [00:43<00:50, 1079.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▍     | 44560/100000 [00:42<00:54, 1023.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▍     | 44855/100000 [00:43<00:52, 1045.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▍     | 44547/100000 [00:42<00:53, 1028.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 45558/100000 [00:43<00:49, 1098.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▍     | 44961/100000 [00:43<00:52, 1046.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▍     | 44720/100000 [00:42<00:53, 1035.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 45673/100000 [00:43<00:48, 1110.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▍     | 44700/100000 [00:42<00:54, 1021.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▌     | 45074/100000 [00:43<00:51, 1067.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▍     | 44828/100000 [00:42<00:53, 1040.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▍     | 44810/100000 [00:43<00:53, 1035.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 45830/100000 [00:43<00:50, 1082.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▌     | 45225/100000 [00:43<00:52, 1042.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▍     | 44987/100000 [00:43<00:52, 1040.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 45941/100000 [00:43<00:49, 1087.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▍     | 44969/100000 [00:43<00:52, 1042.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▌     | 45333/100000 [00:43<00:52, 1045.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▌     | 45103/100000 [00:43<00:51, 1065.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 46055/100000 [00:43<00:49, 1099.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▌     | 45080/100000 [00:43<00:52, 1053.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▌     | 45451/100000 [00:43<00:50, 1074.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▌     | 45250/100000 [00:43<00:53, 1032.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 46223/100000 [00:43<00:48, 1104.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 45565/100000 [00:43<00:49, 1089.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▌     | 45231/100000 [00:43<00:53, 1030.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▌     | 45365/100000 [00:43<00:51, 1060.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▋     | 46334/100000 [00:43<00:48, 1105.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 45682/100000 [00:43<00:48, 1109.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▌     | 45339/100000 [00:43<00:52, 1040.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▌     | 45479/100000 [00:43<00:50, 1074.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  45%|████▌     | 45453/100000 [00:43<00:51, 1064.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▋     | 46491/100000 [00:44<00:49, 1083.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 45835/100000 [00:43<00:50, 1067.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 45592/100000 [00:43<00:50, 1088.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 45566/100000 [00:43<00:50, 1079.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 46604/100000 [00:44<00:48, 1090.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 45946/100000 [00:44<00:50, 1075.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 45705/100000 [00:43<00:49, 1097.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 45683/100000 [00:43<00:49, 1101.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 46715/100000 [00:44<00:48, 1088.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 46061/100000 [00:44<00:49, 1093.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 45863/100000 [00:43<00:50, 1072.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 45835/100000 [00:44<00:50, 1062.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 46861/100000 [00:44<00:51, 1038.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 46226/100000 [00:44<00:49, 1094.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 45974/100000 [00:43<00:49, 1080.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 45946/100000 [00:44<00:50, 1070.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 46970/100000 [00:44<00:50, 1050.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▋     | 46338/100000 [00:44<00:48, 1099.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 46090/100000 [00:44<00:49, 1095.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 46060/100000 [00:44<00:49, 1087.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 47080/100000 [00:44<00:50, 1057.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▋     | 46497/100000 [00:44<00:49, 1078.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▋     | 46255/100000 [00:44<00:49, 1093.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 47191/100000 [00:44<00:49, 1068.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▌     | 46225/100000 [00:44<00:49, 1089.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 46607/100000 [00:44<00:49, 1080.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 47306/100000 [00:44<00:48, 1090.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▋     | 46336/100000 [00:44<00:49, 1091.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▋     | 46415/100000 [00:44<00:49, 1080.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 47422/100000 [00:44<00:47, 1105.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 46761/100000 [00:44<00:50, 1054.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 46524/100000 [00:44<00:49, 1081.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  46%|████▋     | 46496/100000 [00:44<00:49, 1070.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 47536/100000 [00:45<00:47, 1113.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 46635/100000 [00:44<00:49, 1087.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 46912/100000 [00:44<00:51, 1037.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 46605/100000 [00:44<00:49, 1074.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 47697/100000 [00:45<00:47, 1092.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 47020/100000 [00:45<00:50, 1045.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 46714/100000 [00:44<00:49, 1072.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 46784/100000 [00:44<00:50, 1044.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 47134/100000 [00:45<00:49, 1066.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 46890/100000 [00:44<00:51, 1035.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 47857/100000 [00:45<00:48, 1072.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 46859/100000 [00:44<00:51, 1022.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 47242/100000 [00:45<00:49, 1066.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 46996/100000 [00:44<00:51, 1037.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 46967/100000 [00:45<00:51, 1033.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 48021/100000 [00:45<00:48, 1074.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 47360/100000 [00:45<00:48, 1093.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 47110/100000 [00:45<00:49, 1062.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 47075/100000 [00:45<00:50, 1044.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 47475/100000 [00:45<00:47, 1103.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 47220/100000 [00:45<00:49, 1069.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 48173/100000 [00:45<00:49, 1051.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 47185/100000 [00:45<00:49, 1057.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 47588/100000 [00:45<00:47, 1105.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 47330/100000 [00:45<00:49, 1074.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 47296/100000 [00:45<00:49, 1070.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 48320/100000 [00:45<00:50, 1025.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 47446/100000 [00:45<00:47, 1096.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 47742/100000 [00:45<00:49, 1065.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  47%|████▋     | 47410/100000 [00:45<00:48, 1083.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 48434/100000 [00:45<00:49, 1048.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 47560/100000 [00:45<00:47, 1106.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 47851/100000 [00:45<00:48, 1069.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 47525/100000 [00:45<00:47, 1098.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▊     | 48550/100000 [00:46<00:48, 1068.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 47637/100000 [00:45<00:47, 1103.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 47716/100000 [00:45<00:48, 1077.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▊     | 48659/100000 [00:46<00:47, 1070.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 48009/100000 [00:45<00:49, 1059.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 47789/100000 [00:45<00:49, 1062.30 examples/s]#015Tokenizing train dataset:  48%|████▊     | 47870/100000 [00:45<00:49, 1056.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 48809/100000 [00:46<00:49, 1042.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 48167/100000 [00:46<00:49, 1050.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 47978/100000 [00:45<00:49, 1059.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 47944/100000 [00:45<00:49, 1044.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 48970/100000 [00:46<00:48, 1048.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 48310/100000 [00:46<00:51, 1008.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 48086/100000 [00:45<00:48, 1061.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 48053/100000 [00:46<00:49, 1052.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 48423/100000 [00:46<00:49, 1034.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 49115/100000 [00:46<00:49, 1020.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 48234/100000 [00:46<00:50, 1030.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▊     | 48539/100000 [00:46<00:48, 1061.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 48199/100000 [00:46<00:50, 1020.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 49234/100000 [00:46<00:48, 1055.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▊     | 48649/100000 [00:46<00:48, 1066.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 48385/100000 [00:46<00:50, 1017.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 49344/100000 [00:46<00:47, 1064.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 48350/100000 [00:46<00:51, 1010.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▊     | 48503/100000 [00:46<00:48, 1053.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 48794/100000 [00:46<00:49, 1028.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  48%|████▊     | 48465/100000 [00:46<00:49, 1042.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|████▉     | 49510/100000 [00:46<00:47, 1072.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▊     | 48610/100000 [00:46<00:48, 1055.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▊     | 48578/100000 [00:46<00:48, 1061.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|████▉     | 49619/100000 [00:47<00:46, 1075.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 48955/100000 [00:46<00:49, 1036.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▊     | 48718/100000 [00:46<00:48, 1060.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|████▉     | 49731/100000 [00:47<00:46, 1086.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▊     | 48736/100000 [00:46<00:48, 1052.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 49103/100000 [00:47<00:50, 1010.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 48859/100000 [00:46<00:50, 1013.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|████▉     | 49846/100000 [00:47<00:45, 1098.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 49218/100000 [00:47<00:48, 1040.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 48972/100000 [00:46<00:49, 1041.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 48876/100000 [00:46<00:50, 1010.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|█████     | 50006/100000 [00:47<00:46, 1079.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 49328/100000 [00:47<00:48, 1052.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 48990/100000 [00:47<00:49, 1030.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 49118/100000 [00:46<00:50, 1012.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 49436/100000 [00:47<00:47, 1057.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|█████     | 50155/100000 [00:47<00:47, 1048.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 49233/100000 [00:47<00:48, 1044.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 49145/100000 [00:47<00:49, 1025.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|████▉     | 49548/100000 [00:47<00:47, 1070.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|█████     | 50266/100000 [00:47<00:46, 1060.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 49341/100000 [00:47<00:48, 1051.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 49255/100000 [00:47<00:48, 1040.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|████▉     | 49710/100000 [00:47<00:46, 1071.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 49449/100000 [00:47<00:48, 1047.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  49%|████▉     | 49364/100000 [00:47<00:48, 1049.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|█████     | 50426/100000 [00:47<00:46, 1060.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|████▉     | 49824/100000 [00:47<00:46, 1086.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|████▉     | 49564/100000 [00:47<00:46, 1074.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|████▉     | 49530/100000 [00:47<00:47, 1065.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 50583/100000 [00:47<00:46, 1053.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|████▉     | 49981/100000 [00:47<00:46, 1070.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|████▉     | 49729/100000 [00:47<00:46, 1078.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|████▉     | 49637/100000 [00:47<00:47, 1064.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 50691/100000 [00:48<00:46, 1053.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|████▉     | 49841/100000 [00:47<00:46, 1084.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|████▉     | 49751/100000 [00:47<00:46, 1082.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|█████     | 50131/100000 [00:48<00:47, 1044.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 50843/100000 [00:48<00:47, 1038.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|█████     | 50241/100000 [00:48<00:47, 1056.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|████▉     | 49998/100000 [00:47<00:46, 1068.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|████▉     | 49913/100000 [00:47<00:46, 1078.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 50954/100000 [00:48<00:46, 1049.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|█████     | 50400/100000 [00:48<00:47, 1051.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|█████     | 50148/100000 [00:47<00:47, 1039.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|█████     | 50061/100000 [00:48<00:48, 1034.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 51113/100000 [00:48<00:46, 1049.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|█████     | 50257/100000 [00:48<00:47, 1050.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 51226/100000 [00:48<00:45, 1064.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 50558/100000 [00:48<00:47, 1049.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|█████     | 50220/100000 [00:48<00:47, 1042.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████▏    | 51340/100000 [00:48<00:45, 1079.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 50667/100000 [00:48<00:46, 1057.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|█████     | 50410/100000 [00:48<00:47, 1036.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|█████     | 50327/100000 [00:48<00:47, 1044.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████▏    | 51450/100000 [00:48<00:44, 1083.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  50%|█████     | 50433/100000 [00:48<00:47, 1047.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 50811/100000 [00:48<00:48, 1019.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 50563/100000 [00:48<00:48, 1024.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 51560/100000 [00:48<00:44, 1084.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 50924/100000 [00:48<00:46, 1044.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 50672/100000 [00:48<00:47, 1038.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 50590/100000 [00:48<00:47, 1041.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 51670/100000 [00:48<00:44, 1084.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 51031/100000 [00:48<00:46, 1047.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 50695/100000 [00:48<00:47, 1041.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 51780/100000 [00:49<00:44, 1082.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 50815/100000 [00:48<00:48, 1007.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 51193/100000 [00:49<00:46, 1054.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 50928/100000 [00:48<00:47, 1032.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 50846/100000 [00:48<00:47, 1026.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 51940/100000 [00:49<00:44, 1075.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████▏    | 51300/100000 [00:49<00:46, 1053.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 51033/100000 [00:48<00:47, 1034.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 50955/100000 [00:48<00:47, 1036.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 52097/100000 [00:49<00:45, 1063.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████▏    | 51417/100000 [00:49<00:44, 1082.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 51193/100000 [00:48<00:46, 1042.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 51110/100000 [00:49<00:47, 1034.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 51526/100000 [00:49<00:44, 1081.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 52256/100000 [00:49<00:45, 1054.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████     | 51223/100000 [00:49<00:46, 1053.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████▏    | 51358/100000 [00:49<00:45, 1058.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 51688/100000 [00:49<00:44, 1079.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████▏    | 51334/100000 [00:49<00:45, 1064.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 52418/100000 [00:49<00:44, 1059.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 51520/100000 [00:49<00:45, 1060.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  51%|█████▏    | 51445/100000 [00:49<00:45, 1074.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 51847/100000 [00:49<00:45, 1062.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 52570/100000 [00:49<00:45, 1038.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 51555/100000 [00:49<00:44, 1077.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 51957/100000 [00:49<00:44, 1068.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 51677/100000 [00:49<00:45, 1054.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 52683/100000 [00:49<00:44, 1053.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 51717/100000 [00:49<00:44, 1076.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 52108/100000 [00:49<00:45, 1044.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 51826/100000 [00:49<00:46, 1033.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 52843/100000 [00:50<00:44, 1053.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 51938/100000 [00:49<00:45, 1051.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 51871/100000 [00:49<00:45, 1054.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 52952/100000 [00:50<00:44, 1060.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 52268/100000 [00:50<00:45, 1049.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 51980/100000 [00:49<00:45, 1059.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 53066/100000 [00:50<00:43, 1080.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 52374/100000 [00:50<00:45, 1042.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 52090/100000 [00:49<00:46, 1034.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 53185/100000 [00:50<00:42, 1104.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 52133/100000 [00:49<00:46, 1037.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 52522/100000 [00:50<00:46, 1019.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 52246/100000 [00:49<00:46, 1031.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 52240/100000 [00:50<00:45, 1043.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 53337/100000 [00:50<00:43, 1067.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 52641/100000 [00:50<00:44, 1054.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 52401/100000 [00:50<00:46, 1029.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 52752/100000 [00:50<00:44, 1065.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  52%|█████▏    | 52400/100000 [00:50<00:45, 1045.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 53498/100000 [00:50<00:43, 1065.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 52547/100000 [00:50<00:47, 1008.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▎    | 53609/100000 [00:50<00:43, 1074.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 52910/100000 [00:50<00:44, 1054.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 52550/100000 [00:50<00:46, 1024.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 52656/100000 [00:50<00:46, 1024.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 53019/100000 [00:50<00:44, 1062.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 52661/100000 [00:50<00:45, 1043.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 53769/100000 [00:50<00:43, 1067.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 52764/100000 [00:50<00:45, 1036.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 53130/100000 [00:50<00:43, 1071.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 52770/100000 [00:50<00:45, 1047.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 53881/100000 [00:51<00:42, 1078.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 52919/100000 [00:50<00:45, 1029.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 52876/100000 [00:50<00:44, 1047.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 53289/100000 [00:51<00:43, 1061.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 54037/100000 [00:51<00:43, 1059.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 53030/100000 [00:50<00:44, 1047.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 52988/100000 [00:50<00:44, 1061.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 53445/100000 [00:51<00:44, 1049.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 53140/100000 [00:50<00:44, 1058.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 53096/100000 [00:50<00:43, 1066.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 54188/100000 [00:51<00:44, 1037.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▎    | 53557/100000 [00:51<00:43, 1063.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 53205/100000 [00:51<00:43, 1070.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 53293/100000 [00:50<00:44, 1039.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 54348/100000 [00:51<00:43, 1042.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▎    | 53713/100000 [00:51<00:44, 1048.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 53360/100000 [00:51<00:44, 1051.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 54467/100000 [00:51<00:42, 1074.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  53%|█████▎    | 53446/100000 [00:51<00:45, 1028.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 53825/100000 [00:51<00:43, 1062.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▍    | 54578/100000 [00:51<00:42, 1079.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▎    | 53555/100000 [00:51<00:44, 1040.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▎    | 53519/100000 [00:51<00:44, 1048.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 53934/100000 [00:51<00:43, 1068.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▎    | 53629/100000 [00:51<00:43, 1058.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▍    | 54738/100000 [00:51<00:42, 1072.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▎    | 53708/100000 [00:51<00:45, 1027.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 54080/100000 [00:51<00:44, 1026.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 53817/100000 [00:51<00:44, 1040.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 53788/100000 [00:51<00:43, 1055.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▍    | 54893/100000 [00:52<00:42, 1054.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 54186/100000 [00:51<00:44, 1030.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 53923/100000 [00:51<00:44, 1042.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 53900/100000 [00:51<00:43, 1069.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▌    | 55002/100000 [00:52<00:42, 1060.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 54291/100000 [00:51<00:44, 1031.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 54008/100000 [00:51<00:43, 1061.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▌    | 55110/100000 [00:52<00:42, 1062.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 54407/100000 [00:52<00:42, 1063.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 54071/100000 [00:51<00:45, 1016.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▌    | 55221/100000 [00:52<00:41, 1071.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▍    | 54520/100000 [00:52<00:42, 1078.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 54174/100000 [00:51<00:45, 1017.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 54160/100000 [00:51<00:44, 1038.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▌    | 55334/100000 [00:52<00:41, 1085.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▍    | 54679/100000 [00:52<00:42, 1066.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 54324/100000 [00:51<00:45, 1009.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 54311/100000 [00:52<00:44, 1023.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▌    | 55490/100000 [00:52<00:41, 1064.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 54437/100000 [00:52<00:43, 1038.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  54%|█████▍    | 54424/100000 [00:52<00:43, 1049.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▍    | 54836/100000 [00:52<00:42, 1054.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▍    | 54544/100000 [00:52<00:43, 1042.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▍    | 54537/100000 [00:52<00:42, 1064.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 55652/100000 [00:52<00:41, 1067.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▍    | 54992/100000 [00:52<00:42, 1049.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▍    | 54652/100000 [00:52<00:43, 1049.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▍    | 54696/100000 [00:52<00:42, 1060.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 55805/100000 [00:52<00:42, 1050.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▌    | 55098/100000 [00:52<00:42, 1050.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▍    | 54809/100000 [00:52<00:43, 1039.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▌    | 55208/100000 [00:52<00:42, 1061.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▍    | 54850/100000 [00:52<00:43, 1045.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 55960/100000 [00:53<00:42, 1038.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▌    | 55320/100000 [00:52<00:41, 1070.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▍    | 54962/100000 [00:52<00:43, 1026.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 56066/100000 [00:53<00:42, 1040.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▌    | 55013/100000 [00:52<00:42, 1054.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▌    | 55067/100000 [00:52<00:43, 1030.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 56172/100000 [00:53<00:42, 1041.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▌    | 55479/100000 [00:53<00:41, 1063.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▌    | 55178/100000 [00:52<00:42, 1047.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▌    | 55174/100000 [00:52<00:42, 1058.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▋    | 56339/100000 [00:53<00:41, 1064.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 55639/100000 [00:53<00:41, 1062.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▌    | 55285/100000 [00:52<00:42, 1051.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▌    | 55284/100000 [00:52<00:41, 1064.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▋    | 56497/100000 [00:53<00:41, 1054.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 55791/100000 [00:53<00:42, 1043.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▌    | 55440/100000 [00:53<00:42, 1039.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  55%|█████▌    | 55441/100000 [00:53<00:42, 1055.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 56610/100000 [00:53<00:40, 1070.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 55547/100000 [00:53<00:42, 1052.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 55941/100000 [00:53<00:42, 1028.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 55596/100000 [00:53<00:42, 1037.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 56720/100000 [00:53<00:40, 1072.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 55656/100000 [00:53<00:41, 1056.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 56047/100000 [00:53<00:42, 1029.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 55701/100000 [00:53<00:42, 1036.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 56830/100000 [00:53<00:40, 1074.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 55810/100000 [00:53<00:42, 1036.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 56941/100000 [00:53<00:39, 1083.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 56200/100000 [00:53<00:42, 1022.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 55845/100000 [00:53<00:43, 1008.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 57053/100000 [00:54<00:39, 1083.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▋    | 56310/100000 [00:53<00:42, 1036.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 55948/100000 [00:53<00:43, 1010.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 55962/100000 [00:53<00:42, 1025.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 57164/100000 [00:54<00:39, 1088.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 56050/100000 [00:53<00:43, 1005.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 56066/100000 [00:53<00:42, 1027.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▋    | 56468/100000 [00:54<00:41, 1038.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 56155/100000 [00:53<00:43, 1012.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 57313/100000 [00:54<00:40, 1050.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 56581/100000 [00:54<00:41, 1058.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▌    | 56220/100000 [00:53<00:42, 1024.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 57420/100000 [00:54<00:40, 1051.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 56694/100000 [00:54<00:40, 1074.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▋    | 56310/100000 [00:53<00:43, 1014.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▋    | 56335/100000 [00:53<00:41, 1053.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▋    | 56413/100000 [00:53<00:42, 1017.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 57566/100000 [00:54<00:41, 1021.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 56852/100000 [00:54<00:40, 1064.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  56%|█████▋    | 56489/100000 [00:54<00:41, 1040.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 56518/100000 [00:54<00:42, 1025.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 57670/100000 [00:54<00:41, 1020.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 56966/100000 [00:54<00:39, 1080.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 56601/100000 [00:54<00:41, 1056.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 56632/100000 [00:54<00:41, 1054.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 57778/100000 [00:54<00:40, 1033.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 56710/100000 [00:54<00:40, 1060.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 57125/100000 [00:54<00:40, 1069.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 56791/100000 [00:54<00:41, 1052.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 56819/100000 [00:54<00:40, 1067.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 57930/100000 [00:54<00:41, 1020.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 57276/100000 [00:54<00:40, 1045.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 56899/100000 [00:54<00:40, 1057.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 56927/100000 [00:54<00:40, 1064.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 58039/100000 [00:55<00:40, 1035.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 57038/100000 [00:54<00:39, 1074.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 58146/100000 [00:55<00:40, 1042.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 57430/100000 [00:54<00:41, 1032.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 57057/100000 [00:54<00:40, 1052.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 57148/100000 [00:54<00:39, 1073.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 58259/100000 [00:55<00:39, 1063.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 57166/100000 [00:54<00:40, 1058.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 57582/100000 [00:55<00:41, 1023.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 57296/100000 [00:54<00:41, 1038.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 58405/100000 [00:55<00:40, 1026.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 57312/100000 [00:54<00:41, 1023.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 57733/100000 [00:55<00:41, 1016.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▊    | 58516/100000 [00:55<00:39, 1044.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 57416/100000 [00:54<00:41, 1024.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  57%|█████▋    | 57444/100000 [00:55<00:41, 1015.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 57839/100000 [00:55<00:41, 1021.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▊    | 58634/100000 [00:55<00:38, 1078.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 57550/100000 [00:55<00:41, 1025.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 57562/100000 [00:55<00:42, 998.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 57997/100000 [00:55<00:40, 1027.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 58792/100000 [00:55<00:38, 1063.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 57664/100000 [00:55<00:42, 999.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 57703/100000 [00:55<00:41, 1020.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 58103/100000 [00:55<00:40, 1031.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 57766/100000 [00:55<00:42, 997.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 58958/100000 [00:55<00:38, 1072.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 57853/100000 [00:55<00:41, 1010.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 58263/100000 [00:55<00:40, 1038.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 57911/100000 [00:55<00:42, 984.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 57960/100000 [00:55<00:41, 1018.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 59118/100000 [00:56<00:38, 1069.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 58409/100000 [00:55<00:40, 1015.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 58019/100000 [00:55<00:41, 1003.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 59227/100000 [00:56<00:38, 1072.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 58114/100000 [00:55<00:41, 1019.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▊    | 58519/100000 [00:56<00:40, 1032.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 58124/100000 [00:55<00:41, 1012.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 59341/100000 [00:56<00:37, 1085.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 58217/100000 [00:55<00:41, 1017.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▊    | 58636/100000 [00:56<00:39, 1060.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 58227/100000 [00:55<00:41, 1016.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 59457/100000 [00:56<00:36, 1102.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 58320/100000 [00:55<00:41, 1010.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 58795/100000 [00:56<00:39, 1056.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 58376/100000 [00:55<00:41, 1000.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|█████▉    | 59622/100000 [00:56<00:36, 1096.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 58479/100000 [00:56<00:40, 1022.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  58%|█████▊    | 58480/100000 [00:56<00:41, 1005.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|█████▉    | 59737/100000 [00:56<00:36, 1101.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 58958/100000 [00:56<00:38, 1059.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▊    | 58596/100000 [00:56<00:39, 1056.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▊    | 58594/100000 [00:56<00:39, 1039.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|█████▉    | 59902/100000 [00:56<00:36, 1092.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 59116/100000 [00:56<00:38, 1055.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 58751/100000 [00:56<00:39, 1044.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▊    | 58749/100000 [00:56<00:39, 1032.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 59223/100000 [00:56<00:38, 1058.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 58860/100000 [00:56<00:39, 1049.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|██████    | 60056/100000 [00:56<00:37, 1066.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 58857/100000 [00:56<00:39, 1041.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 59338/100000 [00:56<00:37, 1077.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 58970/100000 [00:56<00:38, 1056.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 58963/100000 [00:56<00:39, 1041.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|██████    | 60217/100000 [00:57<00:37, 1066.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 59452/100000 [00:56<00:37, 1091.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 59129/100000 [00:56<00:38, 1054.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 59118/100000 [00:56<00:39, 1037.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|██████    | 60386/100000 [00:57<00:36, 1082.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|█████▉    | 59617/100000 [00:57<00:37, 1088.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 59235/100000 [00:56<00:38, 1054.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 59225/100000 [00:56<00:39, 1041.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|█████▉    | 59731/100000 [00:57<00:36, 1098.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 59350/100000 [00:56<00:37, 1074.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 60552/100000 [00:57<00:36, 1087.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 59336/100000 [00:56<00:38, 1059.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 59466/100000 [00:56<00:37, 1093.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|█████▉    | 59896/100000 [00:57<00:36, 1090.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  59%|█████▉    | 59450/100000 [00:56<00:37, 1074.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 60705/100000 [00:57<00:36, 1063.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|█████▉    | 59628/100000 [00:57<00:37, 1084.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|██████    | 60046/100000 [00:57<00:37, 1055.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|█████▉    | 59610/100000 [00:57<00:37, 1068.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 60858/100000 [00:57<00:37, 1048.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|█████▉    | 59740/100000 [00:57<00:36, 1089.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|█████▉    | 59721/100000 [00:57<00:37, 1077.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 60970/100000 [00:57<00:36, 1058.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|██████    | 60202/100000 [00:57<00:38, 1046.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 61082/100000 [00:57<00:36, 1066.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|█████▉    | 59901/100000 [00:57<00:37, 1072.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|██████    | 60310/100000 [00:57<00:37, 1048.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|█████▉    | 59876/100000 [00:57<00:37, 1060.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|██████    | 60431/100000 [00:57<00:36, 1088.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|██████    | 60052/100000 [00:57<00:38, 1047.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 61237/100000 [00:57<00:36, 1048.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|██████    | 60030/100000 [00:57<00:38, 1048.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 60587/100000 [00:57<00:36, 1065.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|██████    | 60209/100000 [00:57<00:38, 1044.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████▏   | 61396/100000 [00:58<00:36, 1047.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|██████    | 60183/100000 [00:57<00:38, 1033.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|██████    | 60316/100000 [00:57<00:37, 1049.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 61505/100000 [00:58<00:36, 1054.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 60740/100000 [00:58<00:37, 1043.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|██████    | 60344/100000 [00:57<00:37, 1044.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|██████    | 60435/100000 [00:57<00:36, 1082.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 61662/100000 [00:58<00:36, 1048.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 60899/100000 [00:58<00:37, 1046.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  60%|██████    | 60460/100000 [00:57<00:37, 1068.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 60591/100000 [00:58<00:37, 1060.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 61770/100000 [00:58<00:36, 1049.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 61008/100000 [00:58<00:36, 1054.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 60610/100000 [00:58<00:37, 1043.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 61884/100000 [00:58<00:35, 1069.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 61116/100000 [00:58<00:36, 1058.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 60743/100000 [00:58<00:37, 1041.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 60766/100000 [00:58<00:37, 1034.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 62046/100000 [00:58<00:35, 1065.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████▏   | 61267/100000 [00:58<00:37, 1038.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 60900/100000 [00:58<00:37, 1040.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 62154/100000 [00:58<00:35, 1067.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 60922/100000 [00:58<00:37, 1033.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 61009/100000 [00:58<00:37, 1051.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████▏   | 61418/100000 [00:58<00:37, 1024.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 62262/100000 [00:58<00:35, 1068.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 61028/100000 [00:58<00:37, 1038.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 61116/100000 [00:58<00:36, 1053.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 61529/100000 [00:58<00:37, 1038.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 62374/100000 [00:59<00:34, 1076.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████    | 61179/100000 [00:58<00:37, 1023.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████▏   | 61267/100000 [00:58<00:37, 1034.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 61687/100000 [00:59<00:36, 1038.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 62521/100000 [00:59<00:36, 1039.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████▏   | 61283/100000 [00:58<00:37, 1025.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████▏   | 61419/100000 [00:58<00:37, 1022.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 62634/100000 [00:59<00:35, 1057.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 61843/100000 [00:59<00:36, 1035.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  61%|██████▏   | 61432/100000 [00:58<00:38, 1012.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 61529/100000 [00:58<00:37, 1037.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 62741/100000 [00:59<00:35, 1059.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 61956/100000 [00:59<00:36, 1056.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 61540/100000 [00:58<00:37, 1023.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 61687/100000 [00:59<00:36, 1039.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 62890/100000 [00:59<00:35, 1031.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 62115/100000 [00:59<00:36, 1051.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 61643/100000 [00:59<00:37, 1022.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 63007/100000 [00:59<00:34, 1064.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 62221/100000 [00:59<00:36, 1048.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 61746/100000 [00:59<00:37, 1015.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 61843/100000 [00:59<00:36, 1037.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 63123/100000 [00:59<00:33, 1089.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 62330/100000 [00:59<00:35, 1057.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 61855/100000 [00:59<00:36, 1033.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 61956/100000 [00:59<00:35, 1057.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 62437/100000 [00:59<00:35, 1057.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 61963/100000 [00:59<00:36, 1044.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 63285/100000 [00:59<00:33, 1080.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 62113/100000 [00:59<00:36, 1051.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 62591/100000 [00:59<00:36, 1038.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 62117/100000 [00:59<00:36, 1035.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 62220/100000 [00:59<00:35, 1050.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 63442/100000 [01:00<00:34, 1065.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 62698/100000 [00:59<00:35, 1042.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 62329/100000 [00:59<00:35, 1058.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 62272/100000 [00:59<00:36, 1029.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▎   | 63603/100000 [01:00<00:34, 1062.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 62437/100000 [00:59<00:35, 1058.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 62845/100000 [01:00<00:36, 1016.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  62%|██████▏   | 62382/100000 [00:59<00:36, 1044.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 63752/100000 [01:00<00:35, 1034.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 62960/100000 [01:00<00:35, 1043.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 62591/100000 [00:59<00:36, 1038.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 62527/100000 [00:59<00:36, 1013.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 63865/100000 [01:00<00:34, 1053.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 63072/100000 [01:00<00:34, 1063.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 62698/100000 [01:00<00:35, 1043.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 62633/100000 [01:00<00:36, 1021.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 63974/100000 [01:00<00:34, 1059.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 63185/100000 [01:00<00:34, 1079.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 62740/100000 [01:00<00:36, 1028.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 62845/100000 [01:00<00:36, 1016.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 64128/100000 [01:00<00:34, 1041.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 63345/100000 [01:00<00:34, 1069.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 62960/100000 [01:00<00:35, 1042.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 62883/100000 [01:00<00:37, 997.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 64238/100000 [01:00<00:33, 1053.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 63072/100000 [01:00<00:34, 1061.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▎   | 63500/100000 [01:00<00:34, 1054.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 62999/100000 [01:00<00:35, 1034.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 64347/100000 [01:00<00:33, 1059.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 63185/100000 [01:00<00:34, 1077.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 63110/100000 [01:00<00:35, 1049.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▎   | 63646/100000 [01:00<00:35, 1023.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▍   | 64507/100000 [01:01<00:33, 1055.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 63346/100000 [01:00<00:34, 1069.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 63750/100000 [01:00<00:35, 1023.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 63268/100000 [01:00<00:35, 1046.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▍   | 64625/100000 [01:01<00:32, 1079.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 63860/100000 [01:01<00:34, 1038.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  63%|██████▎   | 63375/100000 [01:00<00:35, 1046.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▎   | 63501/100000 [01:00<00:34, 1056.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▍   | 64783/100000 [01:01<00:32, 1067.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 63970/100000 [01:01<00:34, 1050.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▎   | 63524/100000 [01:00<00:35, 1023.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▎   | 63648/100000 [01:00<00:35, 1026.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▍   | 64938/100000 [01:01<00:33, 1053.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 64120/100000 [01:01<00:34, 1028.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 63752/100000 [01:01<00:35, 1024.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▎   | 63673/100000 [01:01<00:35, 1009.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 64231/100000 [01:01<00:34, 1046.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 63862/100000 [01:01<00:34, 1040.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▌   | 65089/100000 [01:01<00:33, 1033.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 64339/100000 [01:01<00:33, 1053.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 63829/100000 [01:01<00:35, 1016.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 63971/100000 [01:01<00:34, 1051.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▌   | 65201/100000 [01:01<00:33, 1050.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 63933/100000 [01:01<00:35, 1018.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▌   | 65311/100000 [01:01<00:32, 1060.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 64499/100000 [01:01<00:33, 1053.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 64120/100000 [01:01<00:34, 1027.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▍   | 64610/100000 [01:01<00:33, 1065.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 64082/100000 [01:01<00:35, 1006.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 64230/100000 [01:01<00:34, 1044.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▌   | 65466/100000 [01:01<00:32, 1049.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 64186/100000 [01:01<00:35, 1012.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 64339/100000 [01:01<00:33, 1053.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65580/100000 [01:02<00:32, 1069.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▍   | 64770/100000 [01:01<00:33, 1060.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 64298/100000 [01:01<00:34, 1033.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65694/100000 [01:02<00:31, 1082.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 64500/100000 [01:01<00:33, 1054.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▍   | 64926/100000 [01:02<00:33, 1045.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65818/100000 [01:02<00:30, 1122.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  64%|██████▍   | 64452/100000 [01:01<00:34, 1023.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▍   | 64612/100000 [01:01<00:33, 1068.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▌   | 65074/100000 [01:02<00:34, 1023.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▍   | 64564/100000 [01:01<00:34, 1041.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65970/100000 [01:02<00:31, 1077.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▍   | 64771/100000 [01:02<00:33, 1062.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▌   | 65189/100000 [01:02<00:33, 1048.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▍   | 64670/100000 [01:01<00:33, 1040.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 66128/100000 [01:02<00:31, 1066.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▍   | 64926/100000 [01:02<00:33, 1045.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▌   | 65350/100000 [01:02<00:32, 1052.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▍   | 64826/100000 [01:02<00:33, 1036.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 66241/100000 [01:02<00:31, 1076.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▌   | 65074/100000 [01:02<00:34, 1023.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65505/100000 [01:02<00:33, 1043.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▍   | 64980/100000 [01:02<00:34, 1028.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▋   | 66398/100000 [01:02<00:31, 1064.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▌   | 65189/100000 [01:02<00:33, 1048.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65618/100000 [01:02<00:32, 1060.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▌   | 65129/100000 [01:02<00:34, 1011.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66546/100000 [01:03<00:32, 1034.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65735/100000 [01:02<00:31, 1084.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▌   | 65350/100000 [01:02<00:32, 1053.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▌   | 65240/100000 [01:02<00:33, 1031.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66660/100000 [01:03<00:31, 1054.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65847/100000 [01:02<00:31, 1092.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▌   | 65347/100000 [01:02<00:33, 1038.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65505/100000 [01:02<00:33, 1043.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66815/100000 [01:03<00:31, 1042.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65997/100000 [01:03<00:32, 1053.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65618/100000 [01:02<00:32, 1060.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  65%|██████▌   | 65498/100000 [01:02<00:33, 1025.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 66107/100000 [01:03<00:31, 1063.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65735/100000 [01:02<00:31, 1084.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66970/100000 [01:03<00:31, 1034.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65609/100000 [01:02<00:32, 1044.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 66219/100000 [01:03<00:31, 1077.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65847/100000 [01:03<00:31, 1092.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 67086/100000 [01:03<00:30, 1063.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65719/100000 [01:03<00:32, 1056.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▋   | 66375/100000 [01:03<00:31, 1060.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65834/100000 [01:03<00:31, 1073.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65997/100000 [01:03<00:32, 1051.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 67249/100000 [01:03<00:30, 1066.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 66107/100000 [01:03<00:31, 1061.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 67360/100000 [01:03<00:30, 1075.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66522/100000 [01:03<00:32, 1029.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 65986/100000 [01:03<00:32, 1048.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 66219/100000 [01:03<00:31, 1075.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66630/100000 [01:03<00:32, 1040.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 67512/100000 [01:03<00:31, 1047.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▌   | 66144/100000 [01:03<00:32, 1047.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66737/100000 [01:03<00:31, 1045.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▋   | 66369/100000 [01:03<00:32, 1045.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 67618/100000 [01:04<00:30, 1048.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▋   | 66298/100000 [01:03<00:32, 1034.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66881/100000 [01:03<00:32, 1011.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66519/100000 [01:03<00:32, 1025.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 67775/100000 [01:04<00:30, 1046.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66984/100000 [01:04<00:32, 1014.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  66%|██████▋   | 66454/100000 [01:03<00:32, 1027.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66623/100000 [01:03<00:32, 1028.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 67930/100000 [01:04<00:30, 1040.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 67099/100000 [01:04<00:31, 1046.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66731/100000 [01:03<00:32, 1037.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66600/100000 [01:03<00:33, 1005.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 67207/100000 [01:04<00:31, 1054.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 68081/100000 [01:04<00:31, 1017.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66709/100000 [01:03<00:32, 1023.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66879/100000 [01:04<00:32, 1016.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 67315/100000 [01:04<00:30, 1060.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 68190/100000 [01:04<00:30, 1028.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66983/100000 [01:04<00:32, 1018.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 67422/100000 [01:04<00:30, 1061.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66852/100000 [01:04<00:33, 997.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 67098/100000 [01:04<00:31, 1049.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 68346/100000 [01:04<00:30, 1026.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 66960/100000 [01:04<00:32, 1011.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 67573/100000 [01:04<00:31, 1035.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 67205/100000 [01:04<00:31, 1051.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 68455/100000 [01:04<00:30, 1040.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 67072/100000 [01:04<00:31, 1037.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 67684/100000 [01:04<00:30, 1052.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 67315/100000 [01:04<00:30, 1062.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▊   | 68571/100000 [01:04<00:29, 1065.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 67179/100000 [01:04<00:31, 1042.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 67422/100000 [01:04<00:30, 1061.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▊   | 68690/100000 [01:05<00:28, 1095.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 67830/100000 [01:04<00:31, 1018.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 67340/100000 [01:04<00:31, 1047.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 67938/100000 [01:04<00:31, 1033.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 67573/100000 [01:04<00:31, 1036.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 68850/100000 [01:05<00:28, 1082.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  67%|██████▋   | 67491/100000 [01:04<00:31, 1030.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 67684/100000 [01:04<00:30, 1053.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 68084/100000 [01:05<00:31, 1008.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 69009/100000 [01:05<00:28, 1074.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 68190/100000 [01:05<00:31, 1018.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 67645/100000 [01:04<00:31, 1027.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 67830/100000 [01:04<00:31, 1020.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 69171/100000 [01:05<00:28, 1070.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 68293/100000 [01:05<00:31, 1015.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 67939/100000 [01:05<00:30, 1036.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 67798/100000 [01:05<00:31, 1017.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 68400/100000 [01:05<00:30, 1024.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 69330/100000 [01:05<00:28, 1061.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 68086/100000 [01:05<00:31, 1005.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▊   | 68510/100000 [01:05<00:30, 1037.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 67950/100000 [01:05<00:31, 1012.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 69442/100000 [01:05<00:28, 1070.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 68195/100000 [01:05<00:31, 1024.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▊   | 68629/100000 [01:05<00:29, 1078.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|██████▉   | 69550/100000 [01:05<00:28, 1065.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 68098/100000 [01:05<00:31, 998.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▊   | 68740/100000 [01:05<00:28, 1082.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 68349/100000 [01:05<00:31, 1012.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|██████▉   | 69666/100000 [01:05<00:27, 1086.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 68201/100000 [01:05<00:31, 1001.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 68850/100000 [01:05<00:28, 1077.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 68461/100000 [01:05<00:30, 1036.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|██████▉   | 69776/100000 [01:06<00:27, 1088.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 68350/100000 [01:05<00:31, 995.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▊   | 68575/100000 [01:05<00:29, 1062.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 69008/100000 [01:05<00:29, 1065.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|██████▉   | 69935/100000 [01:06<00:27, 1075.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  68%|██████▊   | 68461/100000 [01:05<00:30, 1021.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▊   | 68692/100000 [01:05<00:28, 1089.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|███████   | 70045/100000 [01:06<00:27, 1079.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 69167/100000 [01:06<00:29, 1062.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▊   | 68574/100000 [01:05<00:30, 1044.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 68852/100000 [01:05<00:29, 1073.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▊   | 68689/100000 [01:05<00:29, 1070.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|███████   | 70205/100000 [01:06<00:27, 1070.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 69322/100000 [01:06<00:29, 1049.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 69010/100000 [01:06<00:29, 1062.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|███████   | 70325/100000 [01:06<00:26, 1100.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 69435/100000 [01:06<00:28, 1067.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 68845/100000 [01:06<00:29, 1057.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|███████   | 70440/100000 [01:06<00:26, 1112.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 69170/100000 [01:06<00:29, 1058.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|██████▉   | 69596/100000 [01:06<00:28, 1065.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 68999/100000 [01:06<00:29, 1045.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 70605/100000 [01:06<00:26, 1103.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|██████▉   | 69705/100000 [01:06<00:28, 1069.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 69329/100000 [01:06<00:29, 1053.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 69152/100000 [01:06<00:29, 1033.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|██████▉   | 69819/100000 [01:06<00:27, 1084.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 69439/100000 [01:06<00:28, 1063.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 70766/100000 [01:06<00:26, 1086.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 69302/100000 [01:06<00:30, 1021.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|██████▉   | 69980/100000 [01:06<00:27, 1075.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|██████▉   | 69601/100000 [01:06<00:28, 1064.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 70930/100000 [01:07<00:26, 1086.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  69%|██████▉   | 69419/100000 [01:06<00:29, 1053.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|██████▉   | 69709/100000 [01:06<00:28, 1067.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 71049/100000 [01:07<00:26, 1104.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|███████   | 70140/100000 [01:07<00:27, 1066.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|██████▉   | 69570/100000 [01:06<00:29, 1033.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|██████▉   | 69825/100000 [01:06<00:27, 1085.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 71204/100000 [01:07<00:26, 1072.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|███████   | 70306/100000 [01:07<00:27, 1077.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|██████▉   | 69680/100000 [01:06<00:28, 1048.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|██████▉   | 69985/100000 [01:07<00:27, 1073.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|███████   | 70424/100000 [01:07<00:26, 1099.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|██████▉   | 69790/100000 [01:06<00:28, 1057.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████▏  | 71363/100000 [01:07<00:26, 1064.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|███████   | 70093/100000 [01:07<00:27, 1073.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 70587/100000 [01:07<00:26, 1090.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|██████▉   | 69944/100000 [01:07<00:28, 1041.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 71517/100000 [01:07<00:27, 1050.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|███████   | 70256/100000 [01:07<00:27, 1069.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|███████   | 70050/100000 [01:07<00:28, 1042.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 71630/100000 [01:07<00:26, 1063.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 70748/100000 [01:07<00:27, 1082.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|███████   | 70376/100000 [01:07<00:26, 1099.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|███████   | 70155/100000 [01:07<00:28, 1043.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 71744/100000 [01:07<00:26, 1080.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 70911/100000 [01:07<00:27, 1076.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|███████   | 70265/100000 [01:07<00:28, 1053.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 70538/100000 [01:07<00:27, 1090.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 71904/100000 [01:08<00:26, 1071.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 71028/100000 [01:07<00:26, 1091.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  70%|███████   | 70386/100000 [01:07<00:27, 1095.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 70650/100000 [01:07<00:26, 1095.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 72013/100000 [01:08<00:26, 1073.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 71187/100000 [01:08<00:26, 1075.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 70544/100000 [01:07<00:27, 1071.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 70810/100000 [01:07<00:27, 1075.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 72122/100000 [01:08<00:25, 1073.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 70657/100000 [01:07<00:27, 1085.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 70919/100000 [01:07<00:27, 1076.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 72231/100000 [01:08<00:25, 1078.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████▏  | 71336/100000 [01:08<00:27, 1042.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 71036/100000 [01:07<00:26, 1095.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████▏  | 71446/100000 [01:08<00:27, 1049.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 70809/100000 [01:07<00:27, 1057.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 72381/100000 [01:08<00:26, 1035.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 71552/100000 [01:08<00:27, 1049.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 71193/100000 [01:08<00:26, 1073.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 72490/100000 [01:08<00:26, 1047.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 70972/100000 [01:08<00:27, 1061.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 71658/100000 [01:08<00:27, 1049.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 71081/100000 [01:08<00:27, 1065.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████▏  | 71344/100000 [01:08<00:27, 1045.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 72649/100000 [01:08<00:26, 1049.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 71768/100000 [01:08<00:26, 1060.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████▏  | 71450/100000 [01:08<00:27, 1042.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 71876/100000 [01:08<00:26, 1062.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████   | 71232/100000 [01:08<00:27, 1040.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 72809/100000 [01:08<00:25, 1051.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 71559/100000 [01:08<00:27, 1051.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 71990/100000 [01:08<00:25, 1081.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 72918/100000 [01:08<00:25, 1054.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  71%|███████▏  | 71390/100000 [01:08<00:27, 1033.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 71667/100000 [01:08<00:26, 1053.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 73026/100000 [01:09<00:25, 1058.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 72146/100000 [01:08<00:26, 1063.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 71777/100000 [01:08<00:26, 1060.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 71544/100000 [01:08<00:27, 1027.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 73139/100000 [01:09<00:24, 1074.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 72257/100000 [01:09<00:26, 1063.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 71889/100000 [01:08<00:26, 1072.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 71651/100000 [01:08<00:27, 1034.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 73250/100000 [01:09<00:24, 1080.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 72002/100000 [01:08<00:25, 1082.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 72402/100000 [01:09<00:26, 1025.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 71758/100000 [01:08<00:27, 1041.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 73360/100000 [01:09<00:24, 1083.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 72510/100000 [01:09<00:26, 1037.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 72160/100000 [01:09<00:26, 1065.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 71918/100000 [01:08<00:26, 1047.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▎  | 73513/100000 [01:09<00:25, 1054.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 72666/100000 [01:09<00:26, 1033.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 72314/100000 [01:09<00:26, 1049.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 72070/100000 [01:09<00:27, 1032.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▎  | 73671/100000 [01:09<00:25, 1049.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 72771/100000 [01:09<00:26, 1033.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 72182/100000 [01:09<00:26, 1052.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 72466/100000 [01:09<00:26, 1033.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 73827/100000 [01:09<00:25, 1041.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 72933/100000 [01:09<00:25, 1046.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 72289/100000 [01:09<00:26, 1051.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 72570/100000 [01:09<00:26, 1029.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 73939/100000 [01:09<00:24, 1058.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 73041/100000 [01:09<00:25, 1050.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 72677/100000 [01:09<00:26, 1038.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  72%|███████▏  | 72428/100000 [01:09<00:27, 1004.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 74052/100000 [01:10<00:24, 1072.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 73150/100000 [01:09<00:25, 1055.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 72785/100000 [01:09<00:26, 1043.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 72538/100000 [01:09<00:26, 1027.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 74168/100000 [01:10<00:23, 1091.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 73264/100000 [01:09<00:24, 1077.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 72945/100000 [01:09<00:25, 1049.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 72689/100000 [01:09<00:26, 1015.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 74328/100000 [01:10<00:23, 1077.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 73427/100000 [01:10<00:24, 1071.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 73051/100000 [01:09<00:25, 1050.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 72796/100000 [01:09<00:26, 1027.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 74438/100000 [01:10<00:23, 1082.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 73161/100000 [01:10<00:25, 1058.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▎  | 73574/100000 [01:10<00:25, 1036.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 72953/100000 [01:09<00:26, 1030.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 73277/100000 [01:10<00:24, 1083.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▍  | 74600/100000 [01:10<00:23, 1076.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▎  | 73731/100000 [01:10<00:25, 1034.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 73058/100000 [01:10<00:26, 1031.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 73434/100000 [01:10<00:24, 1063.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▍  | 74759/100000 [01:10<00:23, 1066.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 73837/100000 [01:10<00:25, 1035.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 73165/100000 [01:10<00:25, 1038.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 73947/100000 [01:10<00:24, 1050.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 73278/100000 [01:10<00:25, 1061.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▎  | 73585/100000 [01:10<00:25, 1038.11 examples/s]#015Tokenizing train dataset:  75%|███████▍  | 74919/100000 [01:10<00:23, 1064.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 74059/100000 [01:10<00:24, 1066.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▎  | 73691/100000 [01:10<00:25, 1043.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▌  | 75029/100000 [01:10<00:23, 1067.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  73%|███████▎  | 73433/100000 [01:10<00:25, 1044.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 74170/100000 [01:10<00:24, 1074.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▌  | 75140/100000 [01:11<00:23, 1076.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 73844/100000 [01:10<00:25, 1033.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▎  | 73580/100000 [01:10<00:25, 1019.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 74330/100000 [01:11<00:24, 1064.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 73955/100000 [01:10<00:24, 1049.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▌  | 75303/100000 [01:11<00:22, 1076.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▎  | 73684/100000 [01:10<00:25, 1021.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 74440/100000 [01:11<00:23, 1071.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 74068/100000 [01:10<00:24, 1063.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▌  | 75411/100000 [01:11<00:23, 1061.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 73835/100000 [01:10<00:25, 1014.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 74181/100000 [01:10<00:23, 1078.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▍  | 74600/100000 [01:11<00:23, 1067.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 75565/100000 [01:11<00:23, 1048.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 73941/100000 [01:10<00:25, 1024.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 74341/100000 [01:11<00:23, 1069.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 75677/100000 [01:11<00:22, 1062.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▍  | 74758/100000 [01:11<00:23, 1059.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 74051/100000 [01:11<00:24, 1041.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 74451/100000 [01:11<00:23, 1073.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 74164/100000 [01:11<00:24, 1060.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 75830/100000 [01:11<00:23, 1040.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▍  | 74918/100000 [01:11<00:23, 1053.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▍  | 74613/100000 [01:11<00:23, 1072.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▌  | 75024/100000 [01:11<00:23, 1053.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 74319/100000 [01:11<00:24, 1047.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 75988/100000 [01:11<00:23, 1039.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▌  | 75135/100000 [01:11<00:23, 1066.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  74%|███████▍  | 74426/100000 [01:11<00:24, 1052.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▍  | 74770/100000 [01:11<00:23, 1056.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 76097/100000 [01:11<00:22, 1050.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▌  | 75242/100000 [01:11<00:23, 1062.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▍  | 74580/100000 [01:11<00:24, 1037.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▍  | 74930/100000 [01:11<00:23, 1057.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▋  | 76257/100000 [01:12<00:22, 1049.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▌  | 75350/100000 [01:11<00:23, 1063.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▌  | 75037/100000 [01:11<00:23, 1059.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▋  | 76363/100000 [01:12<00:22, 1046.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▍  | 74740/100000 [01:11<00:24, 1038.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 75503/100000 [01:12<00:23, 1041.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▌  | 75150/100000 [01:11<00:23, 1075.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 76509/100000 [01:12<00:23, 1019.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▍  | 74895/100000 [01:11<00:24, 1032.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 75664/100000 [01:12<00:23, 1051.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▌  | 75310/100000 [01:12<00:23, 1068.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▌  | 75002/100000 [01:11<00:24, 1036.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 76669/100000 [01:12<00:22, 1033.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 75812/100000 [01:12<00:23, 1027.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▌  | 75110/100000 [01:12<00:23, 1041.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▌  | 75464/100000 [01:12<00:23, 1050.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 76780/100000 [01:12<00:22, 1050.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▌  | 75220/100000 [01:12<00:23, 1053.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 75964/100000 [01:12<00:23, 1021.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 75625/100000 [01:12<00:23, 1049.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 76930/100000 [01:12<00:22, 1028.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▌  | 75328/100000 [01:12<00:23, 1057.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 76075/100000 [01:12<00:23, 1037.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 75732/100000 [01:12<00:23, 1053.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 77037/100000 [01:12<00:22, 1037.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  75%|███████▌  | 75476/100000 [01:12<00:23, 1030.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 77150/100000 [01:12<00:21, 1057.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 76228/100000 [01:12<00:23, 1026.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 75884/100000 [01:12<00:23, 1032.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▋  | 76335/100000 [01:12<00:22, 1035.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 75636/100000 [01:12<00:23, 1037.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 77301/100000 [01:13<00:21, 1035.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 76045/100000 [01:12<00:22, 1042.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 75741/100000 [01:12<00:23, 1035.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 77411/100000 [01:13<00:21, 1047.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▋  | 76480/100000 [01:13<00:23, 1008.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 76198/100000 [01:12<00:23, 1026.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 75890/100000 [01:12<00:23, 1016.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 77565/100000 [01:13<00:21, 1038.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 76636/100000 [01:13<00:23, 1015.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▋  | 76308/100000 [01:12<00:22, 1042.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 77670/100000 [01:13<00:21, 1039.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 76744/100000 [01:13<00:22, 1027.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 76051/100000 [01:12<00:23, 1029.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▋  | 76457/100000 [01:13<00:23, 1022.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 77826/100000 [01:13<00:21, 1037.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 76900/100000 [01:13<00:22, 1025.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▌  | 76200/100000 [01:13<00:23, 1009.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 77933/100000 [01:13<00:21, 1041.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 76609/100000 [01:13<00:23, 1012.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 77010/100000 [01:13<00:22, 1040.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▋  | 76312/100000 [01:13<00:22, 1030.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 78039/100000 [01:13<00:21, 1042.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 76722/100000 [01:13<00:22, 1035.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 77115/100000 [01:13<00:21, 1042.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  76%|███████▋  | 76458/100000 [01:13<00:23, 1009.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 76827/100000 [01:13<00:22, 1036.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 78192/100000 [01:13<00:21, 1033.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 77265/100000 [01:13<00:22, 1026.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 76609/100000 [01:13<00:23, 1003.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 76985/100000 [01:13<00:22, 1037.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 77375/100000 [01:13<00:21, 1037.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 78351/100000 [01:14<00:20, 1040.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 76720/100000 [01:13<00:22, 1026.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 77094/100000 [01:13<00:21, 1048.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 77481/100000 [01:14<00:21, 1037.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 78467/100000 [01:14<00:20, 1060.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 76871/100000 [01:13<00:22, 1017.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▊  | 78579/100000 [01:14<00:19, 1073.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 77247/100000 [01:13<00:21, 1035.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 77639/100000 [01:14<00:21, 1038.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 76980/100000 [01:13<00:22, 1032.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 77353/100000 [01:14<00:21, 1037.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▊  | 78739/100000 [01:14<00:19, 1066.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 77789/100000 [01:14<00:21, 1020.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 77088/100000 [01:13<00:22, 1040.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 77458/100000 [01:14<00:21, 1034.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 78853/100000 [01:14<00:19, 1082.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 77895/100000 [01:14<00:21, 1027.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 77243/100000 [01:14<00:22, 1029.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 78962/100000 [01:14<00:19, 1080.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 77615/100000 [01:14<00:21, 1037.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 78000/100000 [01:14<00:21, 1028.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  77%|███████▋  | 77349/100000 [01:14<00:21, 1032.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 79075/100000 [01:14<00:19, 1091.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 77769/100000 [01:14<00:21, 1030.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 78150/100000 [01:14<00:21, 1014.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 77502/100000 [01:14<00:22, 1017.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 79237/100000 [01:14<00:19, 1077.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 77874/100000 [01:14<00:21, 1025.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 78252/100000 [01:14<00:21, 1014.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 77610/100000 [01:14<00:21, 1030.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 79347/100000 [01:15<00:19, 1080.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 77979/100000 [01:14<00:21, 1029.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 78362/100000 [01:14<00:20, 1032.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 78084/100000 [01:14<00:21, 1031.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 78478/100000 [01:15<00:20, 1058.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 77760/100000 [01:14<00:21, 1016.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|███████▉  | 79503/100000 [01:15<00:19, 1065.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▊  | 78590/100000 [01:15<00:19, 1072.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|███████▉  | 79612/100000 [01:15<00:19, 1064.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 78232/100000 [01:14<00:21, 1011.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 77921/100000 [01:14<00:21, 1029.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|███████▉  | 79724/100000 [01:15<00:18, 1074.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 78348/100000 [01:14<00:20, 1044.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 78750/100000 [01:15<00:20, 1061.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 78070/100000 [01:14<00:21, 1011.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 78459/100000 [01:15<00:20, 1060.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 78867/100000 [01:15<00:19, 1088.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|███████▉  | 79896/100000 [01:15<00:18, 1088.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 78174/100000 [01:15<00:21, 1014.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▊  | 78571/100000 [01:15<00:19, 1074.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|████████  | 80005/100000 [01:15<00:18, 1085.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 79028/100000 [01:15<00:19, 1075.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 78282/100000 [01:15<00:21, 1014.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▊  | 78732/100000 [01:15<00:19, 1068.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|████████  | 80122/100000 [01:15<00:18, 1103.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 79138/100000 [01:15<00:19, 1078.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  78%|███████▊  | 78396/100000 [01:15<00:20, 1043.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 78845/100000 [01:15<00:19, 1079.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|████████  | 80236/100000 [01:15<00:17, 1109.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▊  | 78508/100000 [01:15<00:20, 1061.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 79300/100000 [01:15<00:19, 1075.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▊  | 78615/100000 [01:15<00:20, 1061.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|████████  | 80395/100000 [01:16<00:18, 1083.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 79011/100000 [01:15<00:19, 1077.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 79456/100000 [01:15<00:19, 1060.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▊  | 78723/100000 [01:15<00:19, 1064.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 80506/100000 [01:16<00:17, 1087.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 79171/100000 [01:15<00:19, 1071.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|███████▉  | 79563/100000 [01:16<00:19, 1059.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 78831/100000 [01:15<00:19, 1066.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 79281/100000 [01:15<00:19, 1077.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 80667/100000 [01:16<00:17, 1079.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|███████▉  | 79670/100000 [01:16<00:19, 1056.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 78941/100000 [01:15<00:19, 1070.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 80778/100000 [01:16<00:17, 1082.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|███████▉  | 79782/100000 [01:16<00:18, 1068.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 79050/100000 [01:15<00:19, 1070.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 79440/100000 [01:15<00:19, 1061.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 80887/100000 [01:16<00:17, 1080.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|███████▉  | 79896/100000 [01:16<00:18, 1085.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|███████▉  | 79548/100000 [01:16<00:19, 1063.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 79211/100000 [01:16<00:19, 1068.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 80996/100000 [01:16<00:17, 1079.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|████████  | 80005/100000 [01:16<00:18, 1081.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|███████▉  | 79710/100000 [01:16<00:19, 1063.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|████████  | 80120/100000 [01:16<00:18, 1098.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  79%|███████▉  | 79365/100000 [01:16<00:19, 1050.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 81154/100000 [01:16<00:17, 1066.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|███████▉  | 79817/100000 [01:16<00:19, 1061.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|████████  | 80232/100000 [01:16<00:17, 1100.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████▏ | 81261/100000 [01:16<00:17, 1064.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|███████▉  | 79525/100000 [01:16<00:19, 1052.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|███████▉  | 79932/100000 [01:16<00:18, 1081.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|████████  | 80390/100000 [01:16<00:18, 1080.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████▏ | 81418/100000 [01:16<00:17, 1054.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|████████  | 80046/100000 [01:16<00:18, 1095.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|███████▉  | 79683/100000 [01:16<00:19, 1050.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 80545/100000 [01:16<00:18, 1055.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|███████▉  | 79792/100000 [01:16<00:19, 1058.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 81583/100000 [01:17<00:17, 1065.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|████████  | 80210/100000 [01:16<00:18, 1089.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 80656/100000 [01:17<00:18, 1064.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|███████▉  | 79904/100000 [01:16<00:18, 1071.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 81695/100000 [01:17<00:17, 1075.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|████████  | 80323/100000 [01:16<00:18, 1092.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 80765/100000 [01:17<00:18, 1065.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|████████  | 80014/100000 [01:16<00:18, 1070.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 81855/100000 [01:17<00:17, 1066.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|████████  | 80480/100000 [01:16<00:18, 1071.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|████████  | 80130/100000 [01:16<00:18, 1090.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 80930/100000 [01:17<00:17, 1074.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 80592/100000 [01:17<00:17, 1080.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|████████  | 80241/100000 [01:16<00:18, 1092.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 82017/100000 [01:17<00:16, 1062.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 81038/100000 [01:17<00:17, 1072.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 82134/100000 [01:17<00:16, 1085.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 80749/100000 [01:17<00:18, 1067.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  80%|████████  | 80396/100000 [01:17<00:18, 1065.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 81191/100000 [01:17<00:18, 1044.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 80506/100000 [01:17<00:18, 1071.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 80913/100000 [01:17<00:17, 1074.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 82296/100000 [01:17<00:16, 1079.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████▏ | 81298/100000 [01:17<00:17, 1047.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 80614/100000 [01:17<00:18, 1068.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 81021/100000 [01:17<00:17, 1074.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 82447/100000 [01:17<00:16, 1050.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████▏ | 81455/100000 [01:17<00:17, 1042.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 80774/100000 [01:17<00:18, 1062.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 81177/100000 [01:17<00:17, 1057.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 82555/100000 [01:18<00:16, 1053.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 81565/100000 [01:17<00:17, 1052.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 81673/100000 [01:17<00:17, 1057.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 80939/100000 [01:17<00:17, 1072.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████▏ | 81339/100000 [01:17<00:17, 1060.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 82717/100000 [01:18<00:16, 1056.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 81780/100000 [01:18<00:17, 1054.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 82825/100000 [01:18<00:16, 1061.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████  | 81090/100000 [01:17<00:18, 1048.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████▏ | 81492/100000 [01:17<00:17, 1043.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 81891/100000 [01:18<00:16, 1068.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 81604/100000 [01:18<00:17, 1060.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 82990/100000 [01:18<00:15, 1066.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████▏ | 81250/100000 [01:17<00:17, 1049.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 82050/100000 [01:18<00:16, 1062.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 81711/100000 [01:18<00:17, 1060.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 83151/100000 [01:18<00:15, 1065.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 82160/100000 [01:18<00:16, 1067.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  81%|████████▏ | 81403/100000 [01:18<00:18, 1032.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 81870/100000 [01:18<00:17, 1054.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 83261/100000 [01:18<00:15, 1073.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 81510/100000 [01:18<00:17, 1040.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 82321/100000 [01:18<00:16, 1061.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 83370/100000 [01:18<00:15, 1069.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 81623/100000 [01:18<00:17, 1059.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 82029/100000 [01:18<00:17, 1053.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 82471/100000 [01:18<00:16, 1033.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 81732/100000 [01:18<00:17, 1063.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 82141/100000 [01:18<00:16, 1068.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▎ | 83533/100000 [01:18<00:15, 1071.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 82577/100000 [01:18<00:16, 1036.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▎ | 83650/100000 [01:19<00:15, 1088.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 81893/100000 [01:18<00:17, 1063.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 82301/100000 [01:18<00:16, 1062.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 82741/100000 [01:19<00:16, 1048.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 83802/100000 [01:19<00:15, 1059.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 82052/100000 [01:18<00:16, 1055.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 82452/100000 [01:18<00:16, 1039.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 83919/100000 [01:19<00:14, 1082.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 82892/100000 [01:19<00:16, 1031.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 82161/100000 [01:18<00:16, 1063.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 82560/100000 [01:18<00:16, 1044.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 83008/100000 [01:19<00:16, 1058.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 84076/100000 [01:19<00:14, 1069.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 82321/100000 [01:18<00:16, 1057.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 82720/100000 [01:19<00:16, 1047.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 83171/100000 [01:19<00:15, 1063.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 82828/100000 [01:19<00:16, 1053.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 84240/100000 [01:19<00:14, 1074.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  82%|████████▏ | 82471/100000 [01:19<00:16, 1031.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 83280/100000 [01:19<00:15, 1067.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 84350/100000 [01:19<00:14, 1077.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 82576/100000 [01:19<00:16, 1034.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 82990/100000 [01:19<00:16, 1055.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 84461/100000 [01:19<00:14, 1082.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 83437/100000 [01:19<00:15, 1055.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 82738/100000 [01:19<00:16, 1047.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 83150/100000 [01:19<00:15, 1056.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▍ | 84575/100000 [01:19<00:14, 1095.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▎ | 83548/100000 [01:19<00:15, 1064.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 83261/100000 [01:19<00:15, 1062.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▍ | 84689/100000 [01:20<00:13, 1103.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▎ | 83660/100000 [01:19<00:15, 1076.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 82889/100000 [01:19<00:16, 1033.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 83368/100000 [01:19<00:15, 1063.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 83003/100000 [01:19<00:16, 1055.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▍ | 84849/100000 [01:20<00:13, 1087.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 83810/100000 [01:20<00:15, 1046.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▎ | 83529/100000 [01:19<00:15, 1062.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 83925/100000 [01:20<00:15, 1071.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 83170/100000 [01:19<00:15, 1062.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▍ | 84999/100000 [01:20<00:14, 1051.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▎ | 83643/100000 [01:19<00:15, 1079.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 84036/100000 [01:20<00:14, 1074.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 83280/100000 [01:19<00:15, 1065.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▌ | 85150/100000 [01:20<00:14, 1030.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 83797/100000 [01:20<00:15, 1053.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 84194/100000 [01:20<00:14, 1062.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  83%|████████▎ | 83437/100000 [01:20<00:15, 1053.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 83910/100000 [01:20<00:15, 1068.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▌ | 85310/100000 [01:20<00:14, 1037.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▎ | 83547/100000 [01:20<00:15, 1061.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 84360/100000 [01:20<00:14, 1074.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▌ | 85418/100000 [01:20<00:13, 1044.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▎ | 83660/100000 [01:20<00:15, 1072.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 84069/100000 [01:20<00:14, 1062.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 84468/100000 [01:20<00:14, 1073.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 84177/100000 [01:20<00:14, 1065.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 85563/100000 [01:20<00:14, 1005.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▍ | 84580/100000 [01:20<00:14, 1083.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 83810/100000 [01:20<00:15, 1042.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 85671/100000 [01:20<00:14, 1022.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▍ | 84697/100000 [01:20<00:13, 1100.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 83927/100000 [01:20<00:14, 1072.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 84342/100000 [01:20<00:14, 1068.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 84036/100000 [01:20<00:14, 1072.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 84454/100000 [01:20<00:14, 1079.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 85828/100000 [01:21<00:13, 1028.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▍ | 84854/100000 [01:20<00:14, 1077.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▍ | 84568/100000 [01:20<00:14, 1092.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 84192/100000 [01:20<00:14, 1058.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 85987/100000 [01:21<00:13, 1031.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▍ | 84999/100000 [01:21<00:14, 1036.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▍ | 84679/100000 [01:20<00:14, 1093.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 86099/100000 [01:21<00:13, 1045.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 84357/100000 [01:20<00:14, 1070.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▌ | 85150/100000 [01:21<00:14, 1018.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▍ | 84840/100000 [01:21<00:14, 1080.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 86205/100000 [01:21<00:13, 1046.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  84%|████████▍ | 84465/100000 [01:20<00:14, 1069.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▌ | 85308/100000 [01:21<00:14, 1027.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▍ | 84580/100000 [01:21<00:14, 1082.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▍ | 84988/100000 [01:21<00:14, 1043.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▋ | 86355/100000 [01:21<00:13, 1023.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▌ | 85415/100000 [01:21<00:14, 1033.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▍ | 84693/100000 [01:21<00:14, 1092.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▋ | 86467/100000 [01:21<00:12, 1043.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▌ | 85139/100000 [01:21<00:14, 1025.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 86575/100000 [01:21<00:12, 1052.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 85560/100000 [01:21<00:14, 1010.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▍ | 84852/100000 [01:21<00:14, 1077.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▌ | 85296/100000 [01:21<00:14, 1028.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 85666/100000 [01:21<00:14, 1020.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 86731/100000 [01:21<00:12, 1039.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▍ | 84997/100000 [01:21<00:14, 1036.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▌ | 85403/100000 [01:21<00:14, 1036.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 85769/100000 [01:21<00:14, 1014.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 86836/100000 [01:22<00:12, 1027.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 85508/100000 [01:21<00:13, 1035.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 85877/100000 [01:21<00:13, 1027.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▌ | 85146/100000 [01:21<00:14, 1018.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 86940/100000 [01:22<00:12, 1028.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 85981/100000 [01:22<00:13, 1028.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 85652/100000 [01:21<00:14, 1008.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 87047/100000 [01:22<00:12, 1036.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▌ | 85302/100000 [01:21<00:14, 1023.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 86092/100000 [01:22<00:13, 1048.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 85760/100000 [01:21<00:13, 1021.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  85%|████████▌ | 85410/100000 [01:21<00:14, 1031.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 87199/100000 [01:22<00:12, 1025.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 86200/100000 [01:22<00:13, 1052.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 85869/100000 [01:22<00:13, 1037.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 87309/100000 [01:22<00:12, 1041.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 85554/100000 [01:22<00:14, 1003.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▋ | 86348/100000 [01:22<00:13, 1025.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 86028/100000 [01:22<00:13, 1039.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 87422/100000 [01:22<00:11, 1061.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 85658/100000 [01:22<00:14, 1011.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▋ | 86455/100000 [01:22<00:13, 1034.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 86136/100000 [01:22<00:13, 1048.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 85762/100000 [01:22<00:14, 1015.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 87576/100000 [01:22<00:11, 1045.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 86563/100000 [01:22<00:12, 1046.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 85870/100000 [01:22<00:13, 1031.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▋ | 86286/100000 [01:22<00:13, 1020.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 87689/100000 [01:22<00:11, 1060.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 86670/100000 [01:22<00:12, 1044.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▋ | 86394/100000 [01:22<00:13, 1032.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 87797/100000 [01:23<00:11, 1063.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 86027/100000 [01:22<00:13, 1034.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 86824/100000 [01:22<00:12, 1031.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 86505/100000 [01:22<00:12, 1050.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 87910/100000 [01:23<00:11, 1077.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▌ | 86136/100000 [01:22<00:13, 1045.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 86616/100000 [01:22<00:12, 1062.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 86981/100000 [01:23<00:12, 1032.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 88065/100000 [01:23<00:11, 1058.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▋ | 86285/100000 [01:22<00:13, 1020.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 87086/100000 [01:23<00:12, 1029.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 86764/100000 [01:22<00:12, 1031.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▋ | 86390/100000 [01:22<00:13, 1023.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 88220/100000 [01:23<00:11, 1044.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 86870/100000 [01:23<00:12, 1035.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 87240/100000 [01:23<00:12, 1025.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  86%|████████▋ | 86500/100000 [01:22<00:12, 1040.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 88376/100000 [01:23<00:11, 1039.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 87356/100000 [01:23<00:11, 1057.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 86612/100000 [01:23<00:12, 1060.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 87028/100000 [01:23<00:12, 1034.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 88488/100000 [01:23<00:10, 1056.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 87510/100000 [01:23<00:11, 1041.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 86760/100000 [01:23<00:12, 1027.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 87176/100000 [01:23<00:12, 1016.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▊ | 88638/100000 [01:23<00:11, 1030.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 87620/100000 [01:23<00:11, 1051.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 86867/100000 [01:23<00:12, 1029.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 87288/100000 [01:23<00:12, 1040.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 87727/100000 [01:23<00:11, 1051.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 86971/100000 [01:23<00:12, 1025.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 87405/100000 [01:23<00:11, 1071.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 88793/100000 [01:23<00:10, 1019.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 87840/100000 [01:23<00:11, 1067.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 88904/100000 [01:24<00:10, 1040.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 87125/100000 [01:23<00:12, 1023.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 87554/100000 [01:23<00:11, 1040.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 89011/100000 [01:24<00:10, 1044.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 87995/100000 [01:24<00:11, 1048.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 87664/100000 [01:23<00:11, 1054.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 87280/100000 [01:23<00:12, 1023.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 88105/100000 [01:24<00:11, 1057.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 87773/100000 [01:23<00:11, 1059.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 89169/100000 [01:24<00:10, 1037.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  87%|████████▋ | 87400/100000 [01:23<00:11, 1063.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 87884/100000 [01:23<00:11, 1071.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 89280/100000 [01:24<00:10, 1049.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 88259/100000 [01:24<00:11, 1039.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 87549/100000 [01:23<00:12, 1035.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 88040/100000 [01:24<00:11, 1049.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 89430/100000 [01:24<00:10, 1030.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 88415/100000 [01:24<00:11, 1033.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 87659/100000 [01:24<00:11, 1047.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|████████▉ | 89536/100000 [01:24<00:10, 1035.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▊ | 88523/100000 [01:24<00:11, 1043.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 87767/100000 [01:24<00:11, 1053.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 88191/100000 [01:24<00:11, 1031.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 87878/100000 [01:24<00:11, 1066.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 88298/100000 [01:24<00:11, 1036.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|████████▉ | 89684/100000 [01:24<00:10, 1013.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▊ | 88675/100000 [01:24<00:10, 1030.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 88404/100000 [01:24<00:11, 1040.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 88029/100000 [01:24<00:11, 1042.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|████████▉ | 89835/100000 [01:24<00:10, 1010.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 88826/100000 [01:24<00:10, 1018.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▊ | 88513/100000 [01:24<00:10, 1050.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|████████▉ | 89942/100000 [01:25<00:09, 1019.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 88935/100000 [01:24<00:10, 1033.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 88182/100000 [01:24<00:11, 1024.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▊ | 88663/100000 [01:24<00:11, 1029.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 88290/100000 [01:24<00:11, 1036.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|█████████ | 90087/100000 [01:25<00:09, 996.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 89088/100000 [01:25<00:10, 1023.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 88814/100000 [01:24<00:11, 1016.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|█████████ | 90199/100000 [01:25<00:09, 1025.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 89197/100000 [01:25<00:10, 1035.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  88%|████████▊ | 88446/100000 [01:24<00:11, 1031.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 88927/100000 [01:24<00:10, 1039.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|█████████ | 90312/100000 [01:25<00:09, 1048.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▊ | 88551/100000 [01:24<00:11, 1034.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 89347/100000 [01:25<00:10, 1019.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|█████████ | 90422/100000 [01:25<00:09, 1060.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 89080/100000 [01:25<00:10, 1026.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 89450/100000 [01:25<00:10, 1021.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▊ | 88701/100000 [01:25<00:11, 1017.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 89189/100000 [01:25<00:10, 1039.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 90574/100000 [01:25<00:09, 1036.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|████████▉ | 89607/100000 [01:25<00:10, 1028.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 88862/100000 [01:25<00:10, 1027.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 89294/100000 [01:25<00:10, 1040.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 90691/100000 [01:25<00:08, 1062.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 88970/100000 [01:25<00:10, 1037.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|████████▉ | 89748/100000 [01:25<00:10, 996.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 89449/100000 [01:25<00:10, 1027.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 90856/100000 [01:25<00:08, 1071.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|████████▉ | 89852/100000 [01:25<00:10, 1004.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 89117/100000 [01:25<00:10, 1013.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|████████▉ | 89604/100000 [01:25<00:10, 1027.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 91010/100000 [01:26<00:08, 1054.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|████████▉ | 89957/100000 [01:25<00:09, 1010.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 89227/100000 [01:25<00:10, 1031.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|████████▉ | 89746/100000 [01:25<00:10, 998.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 91167/100000 [01:26<00:08, 1049.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|█████████ | 90100/100000 [01:26<00:10, 989.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  89%|████████▉ | 89380/100000 [01:25<00:10, 1024.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|████████▉ | 89850/100000 [01:25<00:10, 1005.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|█████████ | 90210/100000 [01:26<00:09, 1012.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████▏| 91325/100000 [01:26<00:08, 1048.68 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|████████▉ | 89537/100000 [01:25<00:10, 1029.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|████████▉ | 89955/100000 [01:26<00:09, 1012.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|█████████ | 90327/100000 [01:26<00:09, 1051.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████▏| 91436/100000 [01:26<00:08, 1058.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|█████████ | 90438/100000 [01:26<00:08, 1063.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|████████▉ | 89683/100000 [01:26<00:10, 1009.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|█████████ | 90102/100000 [01:26<00:09, 998.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 91544/100000 [01:26<00:07, 1061.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|█████████ | 90211/100000 [01:26<00:09, 1018.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 91659/100000 [01:26<00:07, 1081.11 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 90584/100000 [01:26<00:09, 1026.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|████████▉ | 89834/100000 [01:26<00:10, 1002.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|█████████ | 90327/100000 [01:26<00:09, 1054.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 90703/100000 [01:26<00:08, 1065.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|████████▉ | 89941/100000 [01:26<00:09, 1011.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 91799/100000 [01:26<00:08, 1024.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|█████████ | 90438/100000 [01:26<00:08, 1065.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 90865/100000 [01:26<00:08, 1065.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|█████████ | 90083/100000 [01:26<00:10, 984.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 91953/100000 [01:27<00:07, 1012.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 90584/100000 [01:26<00:09, 1027.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|█████████ | 90194/100000 [01:26<00:09, 1013.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92060/100000 [01:27<00:07, 1024.99 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 91021/100000 [01:26<00:08, 1049.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 90703/100000 [01:26<00:08, 1066.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|█████████ | 90309/100000 [01:26<00:09, 1042.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92213/100000 [01:27<00:07, 1020.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 91177/100000 [01:27<00:08, 1041.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  90%|█████████ | 90416/100000 [01:26<00:09, 1049.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 90865/100000 [01:26<00:08, 1065.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92373/100000 [01:27<00:07, 1031.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████▏| 91335/100000 [01:27<00:08, 1044.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 90560/100000 [01:26<00:09, 1011.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 91018/100000 [01:27<00:08, 1047.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92480/100000 [01:27<00:07, 1038.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████▏| 91445/100000 [01:27<00:08, 1053.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 90670/100000 [01:27<00:09, 1029.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 91175/100000 [01:27<00:08, 1041.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 91553/100000 [01:27<00:07, 1058.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 90777/100000 [01:27<00:08, 1036.69 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 92638/100000 [01:27<00:07, 1042.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████▏| 91280/100000 [01:27<00:08, 1037.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 91665/100000 [01:27<00:07, 1066.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 90886/100000 [01:27<00:08, 1048.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 92752/100000 [01:27<00:06, 1060.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████▏| 91390/100000 [01:27<00:08, 1050.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 92863/100000 [01:27<00:06, 1069.29 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 91809/100000 [01:27<00:08, 1020.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 91040/100000 [01:27<00:08, 1037.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████▏| 91497/100000 [01:27<00:08, 1052.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 92972/100000 [01:27<00:06, 1073.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 91610/100000 [01:27<00:07, 1073.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 91959/100000 [01:27<00:07, 1011.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████ | 91195/100000 [01:27<00:08, 1032.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 93085/100000 [01:28<00:06, 1087.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92065/100000 [01:27<00:07, 1019.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████▏| 91300/100000 [01:27<00:08, 1032.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 91760/100000 [01:27<00:07, 1040.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 93248/100000 [01:28<00:06, 1084.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  91%|█████████▏| 91410/100000 [01:27<00:08, 1044.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92215/100000 [01:28<00:07, 1010.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 91905/100000 [01:27<00:08, 1011.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 91517/100000 [01:27<00:08, 1050.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 93402/100000 [01:28<00:06, 1061.13 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92318/100000 [01:28<00:07, 1011.07 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92011/100000 [01:27<00:07, 1022.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 91630/100000 [01:27<00:07, 1068.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▎| 93515/100000 [01:28<00:06, 1073.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92430/100000 [01:28<00:07, 1036.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92161/100000 [01:28<00:07, 1010.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▎| 93631/100000 [01:28<00:05, 1091.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 92536/100000 [01:28<00:07, 1036.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 91778/100000 [01:28<00:07, 1030.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92263/100000 [01:28<00:07, 1009.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▎| 93745/100000 [01:28<00:05, 1102.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 92700/100000 [01:28<00:06, 1053.94 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 91921/100000 [01:28<00:08, 1000.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92372/100000 [01:28<00:07, 1027.09 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 93911/100000 [01:28<00:05, 1101.40 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 92810/100000 [01:28<00:06, 1063.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92031/100000 [01:28<00:07, 1022.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92479/100000 [01:28<00:07, 1034.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 94068/100000 [01:28<00:05, 1080.34 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 92970/100000 [01:28<00:06, 1062.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92174/100000 [01:28<00:07, 997.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 92635/100000 [01:28<00:07, 1033.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 93085/100000 [01:28<00:06, 1078.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92280/100000 [01:28<00:07, 1004.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 92750/100000 [01:28<00:06, 1060.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 94226/100000 [01:29<00:05, 1066.90 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92391/100000 [01:28<00:07, 1029.79 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 92860/100000 [01:28<00:06, 1064.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 94334/100000 [01:29<00:05, 1065.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 93247/100000 [01:29<00:06, 1074.97 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  92%|█████████▏| 92498/100000 [01:28<00:07, 1038.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 92969/100000 [01:28<00:06, 1067.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 94490/100000 [01:29<00:05, 1053.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 93400/100000 [01:29<00:06, 1051.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 93083/100000 [01:28<00:06, 1086.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 92653/100000 [01:28<00:07, 1033.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▎| 93511/100000 [01:29<00:06, 1062.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▍| 94642/100000 [01:29<00:05, 1038.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 92762/100000 [01:29<00:06, 1044.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 93242/100000 [01:29<00:06, 1065.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▎| 93627/100000 [01:29<00:05, 1085.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▍| 94747/100000 [01:29<00:05, 1037.05 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 92874/100000 [01:29<00:06, 1063.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▎| 93738/100000 [01:29<00:05, 1091.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 93392/100000 [01:29<00:06, 1038.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 92982/100000 [01:29<00:06, 1064.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▍| 94893/100000 [01:29<00:05, 1013.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▎| 93501/100000 [01:29<00:06, 1049.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 93903/100000 [01:29<00:05, 1091.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 93094/100000 [01:29<00:06, 1075.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▌| 95001/100000 [01:29<00:04, 1027.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▎| 93615/100000 [01:29<00:05, 1070.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▌| 95113/100000 [01:29<00:04, 1048.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 94059/100000 [01:29<00:05, 1071.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 93255/100000 [01:29<00:06, 1073.04 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▎| 93728/100000 [01:29<00:05, 1084.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▌| 95221/100000 [01:30<00:04, 1054.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 94218/100000 [01:29<00:05, 1061.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  93%|█████████▎| 93409/100000 [01:29<00:06, 1050.22 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 93896/100000 [01:29<00:05, 1088.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▌| 95329/100000 [01:30<00:04, 1058.18 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 94325/100000 [01:30<00:05, 1057.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▎| 93520/100000 [01:29<00:06, 1056.93 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▌| 95438/100000 [01:30<00:04, 1064.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 94050/100000 [01:29<00:05, 1062.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▎| 93636/100000 [01:29<00:05, 1080.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 95550/100000 [01:30<00:04, 1078.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 94480/100000 [01:30<00:05, 1045.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 93750/100000 [01:29<00:05, 1090.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 94208/100000 [01:30<00:05, 1048.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 95665/100000 [01:30<00:03, 1092.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▍| 94633/100000 [01:30<00:05, 1034.27 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 93913/100000 [01:30<00:05, 1086.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 94365/100000 [01:30<00:05, 1041.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 95827/100000 [01:30<00:03, 1082.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▍| 94787/100000 [01:30<00:05, 1023.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 95940/100000 [01:30<00:03, 1090.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 94070/100000 [01:30<00:05, 1062.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▍| 94521/100000 [01:30<00:05, 1037.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 96052/100000 [01:30<00:03, 1091.81 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▍| 94934/100000 [01:30<00:05, 1008.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 94226/100000 [01:30<00:05, 1052.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▍| 94671/100000 [01:30<00:05, 1020.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▌| 95040/100000 [01:30<00:04, 1012.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 96210/100000 [01:31<00:03, 1073.98 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▌| 95152/100000 [01:30<00:04, 1036.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 94380/100000 [01:30<00:05, 1037.89 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▍| 94818/100000 [01:30<00:05, 1002.03 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▋| 96367/100000 [01:31<00:03, 1060.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▌| 95262/100000 [01:31<00:04, 1052.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  94%|█████████▍| 94486/100000 [01:30<00:05, 1042.19 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▍| 94972/100000 [01:30<00:05, 1003.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▋| 96477/100000 [01:31<00:03, 1068.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▌| 95372/100000 [01:31<00:04, 1063.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▍| 94637/100000 [01:30<00:05, 1026.31 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 96589/100000 [01:31<00:03, 1077.41 examples/s]#015Tokenizing train dataset:  95%|█████████▌| 95080/100000 [01:30<00:04, 1010.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 95534/100000 [01:31<00:04, 1064.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▌| 95191/100000 [01:31<00:04, 1033.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 96698/100000 [01:31<00:03, 1078.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▍| 94789/100000 [01:30<00:05, 1016.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 95647/100000 [01:31<00:04, 1076.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▌| 95297/100000 [01:31<00:04, 1038.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 96809/100000 [01:31<00:02, 1081.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 95758/100000 [01:31<00:03, 1074.51 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▍| 94938/100000 [01:31<00:05, 1005.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▌| 95405/100000 [01:31<00:04, 1044.96 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 96963/100000 [01:31<00:02, 1058.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 95869/100000 [01:31<00:03, 1077.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▌| 95041/100000 [01:31<00:04, 1009.92 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 95512/100000 [01:31<00:04, 1048.57 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 97070/100000 [01:31<00:02, 1054.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▌| 95152/100000 [01:31<00:04, 1032.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 95625/100000 [01:31<00:04, 1070.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 96032/100000 [01:31<00:03, 1079.16 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 97179/100000 [01:31<00:02, 1061.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▌| 95265/100000 [01:31<00:04, 1055.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 95733/100000 [01:31<00:03, 1071.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 96188/100000 [01:31<00:03, 1061.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  95%|█████████▌| 95375/100000 [01:31<00:04, 1066.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 97337/100000 [01:32<00:02, 1055.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 95898/100000 [01:31<00:03, 1078.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 97454/100000 [01:32<00:02, 1084.33 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▋| 96347/100000 [01:32<00:03, 1053.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 95535/100000 [01:31<00:04, 1063.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 96057/100000 [01:31<00:03, 1070.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▋| 96460/100000 [01:32<00:03, 1065.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 95647/100000 [01:31<00:04, 1073.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 97615/100000 [01:32<00:02, 1077.08 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 96569/100000 [01:32<00:03, 1066.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 95758/100000 [01:31<00:03, 1071.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 96212/100000 [01:31<00:03, 1053.67 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 97772/100000 [01:32<00:02, 1061.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 96680/100000 [01:32<00:03, 1073.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 95869/100000 [01:31<00:03, 1077.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▋| 96368/100000 [01:32<00:03, 1044.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 96790/100000 [01:32<00:02, 1075.53 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 97936/100000 [01:32<00:01, 1068.87 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 96030/100000 [01:32<00:03, 1071.76 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▋| 96477/100000 [01:32<00:03, 1052.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 96943/100000 [01:32<00:02, 1049.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 96586/100000 [01:32<00:03, 1058.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 98091/100000 [01:32<00:01, 1054.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▌| 96186/100000 [01:32<00:03, 1057.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 96695/100000 [01:32<00:03, 1063.58 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 97100/100000 [01:32<00:02, 1041.75 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 98238/100000 [01:32<00:01, 1026.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▋| 96343/100000 [01:32<00:03, 1049.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 97210/100000 [01:32<00:02, 1051.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 96858/100000 [01:32<00:02, 1070.24 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  96%|█████████▋| 96454/100000 [01:32<00:03, 1059.60 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 98383/100000 [01:33<00:01, 1002.23 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 97371/100000 [01:32<00:02, 1056.28 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 97003/100000 [01:32<00:02, 1031.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 98491/100000 [01:33<00:01, 1017.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 96618/100000 [01:32<00:03, 1066.82 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 97489/100000 [01:33<00:02, 1081.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 97110/100000 [01:32<00:02, 1036.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▊| 98603/100000 [01:33<00:01, 1035.55 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 96780/100000 [01:32<00:03, 1067.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 97217/100000 [01:32<00:02, 1043.56 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▊| 98711/100000 [01:33<00:01, 1045.12 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 97647/100000 [01:33<00:02, 1064.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 98818/100000 [01:33<00:01, 1046.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 96933/100000 [01:32<00:02, 1048.02 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 97377/100000 [01:33<00:02, 1047.50 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 97804/100000 [01:33<00:02, 1052.95 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 98930/100000 [01:33<00:01, 1061.01 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 97490/100000 [01:33<00:02, 1066.63 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 97910/100000 [01:33<00:01, 1051.37 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 97090/100000 [01:33<00:02, 1038.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 99038/100000 [01:33<00:00, 1065.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 97197/100000 [01:33<00:02, 1045.06 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 97647/100000 [01:33<00:02, 1052.49 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 98068/100000 [01:33<00:01, 1049.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 99192/100000 [01:33<00:00, 1046.46 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 97304/100000 [01:33<00:02, 1047.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 97801/100000 [01:33<00:02, 1035.38 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 99298/100000 [01:33<00:00, 1046.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 98214/100000 [01:33<00:01, 1019.84 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  97%|█████████▋| 97417/100000 [01:33<00:02, 1067.72 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 97909/100000 [01:33<00:02, 1041.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 97526/100000 [01:33<00:02, 1072.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 99455/100000 [01:34<00:00, 1044.00 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 98364/100000 [01:33<00:01, 1012.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 98068/100000 [01:33<00:01, 1042.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|█████████▉| 99562/100000 [01:34<00:00, 1048.32 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 97679/100000 [01:33<00:02, 1049.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▊| 98518/100000 [01:34<00:01, 1015.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|█████████▉| 99671/100000 [01:34<00:00, 1058.62 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 97785/100000 [01:33<00:02, 1048.80 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 98214/100000 [01:33<00:01, 1015.64 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▊| 98629/100000 [01:34<00:01, 1034.78 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|█████████▉| 99814/100000 [01:34<00:00, 1012.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 97947/100000 [01:33<00:01, 1056.77 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 98365/100000 [01:34<00:01, 1009.30 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 98785/100000 [01:34<00:01, 1034.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|█████████▉| 99918/100000 [01:34<00:00, 1017.85 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 98467/100000 [01:34<00:01, 1009.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 98895/100000 [01:34<00:01, 1045.54 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 98104/100000 [01:34<00:01, 1040.35 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▊| 98576/100000 [01:34<00:01, 1026.14 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 99005/100000 [01:34<00:00, 1054.10 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|██████████| 100000/100000 [01:34<00:00, 1055.45 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 98253/100000 [01:34<00:01, 1022.39 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▊| 98688/100000 [01:34<00:01, 1044.49 examples/s]\u001b[0m\n",
      "\u001b[35m[2025-09-09 17:58:32,561] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 99160/100000 [01:34<00:00, 1040.52 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  98%|█████████▊| 98394/100000 [01:34<00:01, 990.74 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 98846/100000 [01:34<00:01, 1039.48 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 99265/100000 [01:34<00:00, 1040.92 examples/s]\u001b[0m\n",
      "\u001b[35mdf: /root/.triton/autotune: No such file or directory\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▊| 98506/100000 [01:34<00:01, 1015.65 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 98953/100000 [01:34<00:01, 1043.66 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 99377/100000 [01:34<00:00, 1059.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▊| 98617/100000 [01:34<00:01, 1034.91 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 99066/100000 [01:34<00:00, 1060.25 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|█████████▉| 99538/100000 [01:35<00:00, 1061.47 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 98774/100000 [01:34<00:01, 1037.42 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 99219/100000 [01:34<00:00, 1041.17 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|█████████▉| 99647/100000 [01:35<00:00, 1064.15 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 98884/100000 [01:34<00:01, 1051.43 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 99328/100000 [01:34<00:00, 1049.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|█████████▉| 99792/100000 [01:35<00:00, 1026.59 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 98992/100000 [01:34<00:00, 1056.83 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 99434/100000 [01:35<00:00, 1050.88 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|█████████▉| 99541/100000 [01:35<00:00, 1055.26 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|█████████▉| 99940/100000 [01:35<00:00, 1010.73 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 99145/100000 [01:35<00:00, 1041.71 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|█████████▉| 99652/100000 [01:35<00:00, 1067.44 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|██████████| 100000/100000 [01:35<00:00, 1045.61 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 99306/100000 [01:35<00:00, 1050.49 examples/s]\u001b[0m\n",
      "\u001b[35m[2025-09-09 17:58:33,616] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|█████████▉| 99798/100000 [01:35<00:00, 1026.20 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset:  99%|█████████▉| 99412/100000 [01:35<00:00, 1050.61 examples/s]\u001b[0m\n",
      "\u001b[35m[2025-09-09 17:58:33,672] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|█████████▉| 99522/100000 [01:35<00:00, 1060.36 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|█████████▉| 99946/100000 [01:35<00:00, 1009.21 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|█████████▉| 99680/100000 [01:35<00:00, 1051.86 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|██████████| 100000/100000 [01:35<00:00, 1044.45 examples/s]\u001b[0m\n",
      "\u001b[35m[2025-09-09 17:58:33,999] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|█████████▉| 99822/100000 [01:35<00:00, 1012.41 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|█████████▉| 99925/100000 [01:35<00:00, 1014.70 examples/s]\u001b[0m\n",
      "\u001b[35mTokenizing train dataset: 100%|██████████| 100000/100000 [01:36<00:00, 1039.52 examples/s]\u001b[0m\n",
      "\u001b[35m[2025-09-09 17:58:34,531] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[35m[2025-09-09 17:58:34,717] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\u001b[0m\n",
      "\u001b[35m[2025-09-09 17:58:35,095] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\u001b[0m\n",
      "\u001b[34m2025-09-09 17:58:35 - __main__ - INFO - Beginning distributed DPO training...\u001b[0m\n",
      "\u001b[34m0%|          | 0/782 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35m[2025-09-09 17:58:35,497] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\u001b[0m\n",
      "\u001b[34m0%|          | 1/782 [00:03<43:14,  3.32s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 2/782 [00:06<38:25,  2.96s/it]\u001b[0m\n",
      "\u001b[34m0%|          | 3/782 [00:08<35:10,  2.71s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/782 [00:11<34:46,  2.68s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 5/782 [00:13<33:34,  2.59s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 6/782 [00:15<32:20,  2.50s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 7/782 [00:18<33:26,  2.59s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 8/782 [00:20<31:53,  2.47s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 9/782 [00:23<32:01,  2.49s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 10/782 [00:25<32:27,  2.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6931, 'grad_norm': 1.940722942352295, 'learning_rate': 3.5000000000000004e-06, 'rewards/chosen': 0.00013354004477150738, 'rewards/rejected': 7.334367546718568e-05, 'rewards/accuracies': 0.35234373807907104, 'rewards/margins': 6.0196394770173356e-05, 'logps/chosen': -174.50363159179688, 'logps/rejected': -176.9784393310547, 'logits/chosen': -1.2474920749664307, 'logits/rejected': -1.1634594202041626, 'epoch': 0.01}\u001b[0m\n",
      "\u001b[34m1%|▏         | 10/782 [00:25<32:27,  2.52s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 11/782 [00:28<31:05,  2.42s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 12/782 [00:30<30:43,  2.39s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 13/782 [00:33<31:26,  2.45s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 14/782 [00:35<31:35,  2.47s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 15/782 [00:38<31:55,  2.50s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 16/782 [00:40<31:06,  2.44s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 17/782 [00:42<30:37,  2.40s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 18/782 [00:45<31:31,  2.48s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 19/782 [00:47<31:59,  2.52s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 20/782 [00:50<31:49,  2.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6931, 'grad_norm': 1.802454948425293, 'learning_rate': 8.000000000000001e-06, 'rewards/chosen': -0.001083556329831481, 'rewards/rejected': -0.0012680359650403261, 'rewards/accuracies': 0.4984374940395355, 'rewards/margins': 0.0001844796643126756, 'logps/chosen': -179.98548889160156, 'logps/rejected': -179.24465942382812, 'logits/chosen': -1.3112142086029053, 'logits/rejected': -1.189011812210083, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 20/782 [00:50<31:49,  2.51s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 21/782 [00:52<31:06,  2.45s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 22/782 [00:55<31:24,  2.48s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 23/782 [00:57<31:08,  2.46s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 24/782 [01:00<30:55,  2.45s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 25/782 [01:02<30:44,  2.44s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 26/782 [01:05<30:50,  2.45s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 27/782 [01:07<31:03,  2.47s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 28/782 [01:10<31:17,  2.49s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 29/782 [01:12<31:16,  2.49s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 30/782 [01:15<31:05,  2.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.692, 'grad_norm': 1.766672134399414, 'learning_rate': 1.3000000000000001e-05, 'rewards/chosen': -0.010064692236483097, 'rewards/rejected': -0.012574811466038227, 'rewards/accuracies': 0.5414062738418579, 'rewards/margins': 0.002510118531063199, 'logps/chosen': -182.88217163085938, 'logps/rejected': -172.7987518310547, 'logits/chosen': -1.2522683143615723, 'logits/rejected': -1.1718034744262695, 'epoch': 0.04}\u001b[0m\n",
      "\u001b[34m4%|▍         | 30/782 [01:15<31:05,  2.48s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 31/782 [01:17<30:36,  2.45s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 32/782 [01:19<30:17,  2.42s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 33/782 [01:22<30:39,  2.46s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 34/782 [01:24<30:50,  2.47s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 35/782 [01:27<30:45,  2.47s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 36/782 [01:29<30:50,  2.48s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 37/782 [01:32<31:29,  2.54s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 38/782 [01:35<31:34,  2.55s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 39/782 [01:37<31:00,  2.50s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 40/782 [01:39<30:38,  2.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6892, 'grad_norm': 1.873327374458313, 'learning_rate': 1.75e-05, 'rewards/chosen': -0.022916480898857117, 'rewards/rejected': -0.03144998103380203, 'rewards/accuracies': 0.555468738079071, 'rewards/margins': 0.008533499203622341, 'logps/chosen': -173.1060028076172, 'logps/rejected': -174.3029022216797, 'logits/chosen': -1.2816646099090576, 'logits/rejected': -1.2032711505889893, 'epoch': 0.05}\u001b[0m\n",
      "\u001b[34m5%|▌         | 40/782 [01:39<30:38,  2.48s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 41/782 [01:42<29:56,  2.42s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 42/782 [01:44<29:26,  2.39s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 43/782 [01:47<30:28,  2.47s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 44/782 [01:49<30:26,  2.47s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 45/782 [01:52<30:26,  2.48s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 46/782 [01:54<29:56,  2.44s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 47/782 [01:57<30:24,  2.48s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 48/782 [01:59<30:47,  2.52s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 49/782 [02:01<29:46,  2.44s/it]\u001b[0m\n",
      "\u001b[34m6%|▋         | 50/782 [02:04<29:55,  2.45s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6877, 'grad_norm': 1.7884538173675537, 'learning_rate': 2.25e-05, 'rewards/chosen': -0.046364087611436844, 'rewards/rejected': -0.059968434274196625, 'rewards/accuracies': 0.5445312261581421, 'rewards/margins': 0.013604352250695229, 'logps/chosen': -180.63162231445312, 'logps/rejected': -179.77084350585938, 'logits/chosen': -1.31731379032135, 'logits/rejected': -1.2181565761566162, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▋         | 50/782 [02:04<29:55,  2.45s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 51/782 [02:06<29:54,  2.45s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 52/782 [02:09<30:08,  2.48s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 53/782 [02:11<29:42,  2.45s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 54/782 [02:14<29:59,  2.47s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 55/782 [02:16<29:30,  2.44s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 56/782 [02:19<29:36,  2.45s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 57/782 [02:21<29:41,  2.46s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 58/782 [02:24<29:27,  2.44s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 59/782 [02:26<29:31,  2.45s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 60/782 [02:29<30:02,  2.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6869, 'grad_norm': 1.8614912033081055, 'learning_rate': 2.7000000000000002e-05, 'rewards/chosen': -0.08936665207147598, 'rewards/rejected': -0.10828325897455215, 'rewards/accuracies': 0.563281238079071, 'rewards/margins': 0.018916601315140724, 'logps/chosen': -175.40907287597656, 'logps/rejected': -170.97879028320312, 'logits/chosen': -1.3612314462661743, 'logits/rejected': -1.2586946487426758, 'epoch': 0.08}\u001b[0m\n",
      "\u001b[34m8%|▊         | 60/782 [02:29<30:02,  2.50s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 61/782 [02:31<30:08,  2.51s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 62/782 [02:34<30:29,  2.54s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 63/782 [02:36<29:56,  2.50s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 64/782 [02:38<29:18,  2.45s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 65/782 [02:41<29:26,  2.46s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 66/782 [02:43<29:39,  2.49s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 67/782 [02:46<28:48,  2.42s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 68/782 [02:48<28:29,  2.39s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 69/782 [02:51<28:55,  2.43s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 70/782 [02:53<29:05,  2.45s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6908, 'grad_norm': 2.0924408435821533, 'learning_rate': 3.2000000000000005e-05, 'rewards/chosen': -0.11108219623565674, 'rewards/rejected': -0.12390150874853134, 'rewards/accuracies': 0.535937488079071, 'rewards/margins': 0.01281932182610035, 'logps/chosen': -179.03907775878906, 'logps/rejected': -172.24368286132812, 'logits/chosen': -1.389448881149292, 'logits/rejected': -1.3066364526748657, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 70/782 [02:53<29:05,  2.45s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 71/782 [02:55<28:16,  2.39s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 72/782 [02:58<27:43,  2.34s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 73/782 [03:00<27:22,  2.32s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 74/782 [03:02<28:03,  2.38s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 75/782 [03:05<27:58,  2.37s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 76/782 [03:07<27:44,  2.36s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 77/782 [03:09<27:45,  2.36s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 78/782 [03:12<28:21,  2.42s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 79/782 [03:14<28:38,  2.44s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 80/782 [03:17<28:39,  2.45s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6847, 'grad_norm': 1.8230061531066895, 'learning_rate': 3.7e-05, 'rewards/chosen': -0.025386523455381393, 'rewards/rejected': -0.0487591028213501, 'rewards/accuracies': 0.5484374761581421, 'rewards/margins': 0.023372579365968704, 'logps/chosen': -181.3466033935547, 'logps/rejected': -171.31056213378906, 'logits/chosen': -1.3537476062774658, 'logits/rejected': -1.2615065574645996, 'epoch': 0.1}\u001b[0m\n",
      "\u001b[34m10%|█         | 80/782 [03:17<28:39,  2.45s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 81/782 [03:19<28:13,  2.42s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 82/782 [03:22<28:07,  2.41s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 83/782 [03:24<28:17,  2.43s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 84/782 [03:27<28:02,  2.41s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 85/782 [03:29<27:19,  2.35s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 86/782 [03:31<27:33,  2.38s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 87/782 [03:34<27:31,  2.38s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 88/782 [03:36<28:04,  2.43s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 89/782 [03:39<28:35,  2.48s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 90/782 [03:41<27:43,  2.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6835, 'grad_norm': 1.925787091255188, 'learning_rate': 4.2e-05, 'rewards/chosen': 0.030065879225730896, 'rewards/rejected': 0.0050734467804431915, 'rewards/accuracies': 0.550000011920929, 'rewards/margins': 0.024992430582642555, 'logps/chosen': -171.46734619140625, 'logps/rejected': -170.75823974609375, 'logits/chosen': -1.2600853443145752, 'logits/rejected': -1.2048053741455078, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 90/782 [03:41<27:43,  2.40s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 91/782 [03:43<28:19,  2.46s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 92/782 [03:46<28:43,  2.50s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 93/782 [03:49<28:46,  2.51s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 94/782 [03:51<29:05,  2.54s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 95/782 [03:54<29:04,  2.54s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 96/782 [03:56<28:31,  2.49s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 97/782 [03:59<28:43,  2.52s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 98/782 [04:01<28:18,  2.48s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 99/782 [04:03<27:44,  2.44s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 100/782 [04:06<27:30,  2.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6794, 'grad_norm': 1.8718538284301758, 'learning_rate': 4.7e-05, 'rewards/chosen': 0.03625015541911125, 'rewards/rejected': -0.002874956699088216, 'rewards/accuracies': 0.567187488079071, 'rewards/margins': 0.039125118404626846, 'logps/chosen': -183.49020385742188, 'logps/rejected': -177.54910278320312, 'logits/chosen': -1.2613022327423096, 'logits/rejected': -1.1836698055267334, 'epoch': 0.13}\u001b[0m\n",
      "\u001b[34m13%|█▎        | 100/782 [04:06<27:30,  2.42s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 101/782 [04:09<28:44,  2.53s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 102/782 [04:11<28:19,  2.50s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 103/782 [04:14<28:10,  2.49s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 104/782 [04:16<27:37,  2.45s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 105/782 [04:18<27:04,  2.40s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 106/782 [04:21<27:19,  2.43s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 107/782 [04:23<27:47,  2.47s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 108/782 [04:26<27:46,  2.47s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 109/782 [04:28<27:17,  2.43s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 110/782 [04:31<27:22,  2.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6749, 'grad_norm': 1.9782824516296387, 'learning_rate': 4.9995756260623194e-05, 'rewards/chosen': 0.005439312197268009, 'rewards/rejected': -0.050480373203754425, 'rewards/accuracies': 0.557812511920929, 'rewards/margins': 0.05591968819499016, 'logps/chosen': -177.0453338623047, 'logps/rejected': -168.96070861816406, 'logits/chosen': -1.286859154701233, 'logits/rejected': -1.225923776626587, 'epoch': 0.14}\u001b[0m\n",
      "\u001b[34m14%|█▍        | 110/782 [04:31<27:22,  2.44s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 111/782 [04:33<27:48,  2.49s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 112/782 [04:36<27:38,  2.48s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 113/782 [04:38<27:33,  2.47s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 114/782 [04:40<27:24,  2.46s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 115/782 [04:43<27:15,  2.45s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 116/782 [04:45<27:23,  2.47s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 117/782 [04:48<27:25,  2.47s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 118/782 [04:50<27:22,  2.47s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 119/782 [04:53<27:06,  2.45s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 120/782 [04:55<27:06,  2.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6785, 'grad_norm': 2.7591264247894287, 'learning_rate': 4.99480307371557e-05, 'rewards/chosen': -0.0747162252664566, 'rewards/rejected': -0.1324475258588791, 'rewards/accuracies': 0.5601562261581421, 'rewards/margins': 0.05773131921887398, 'logps/chosen': -182.90658569335938, 'logps/rejected': -178.83187866210938, 'logits/chosen': -1.3585550785064697, 'logits/rejected': -1.3021667003631592, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▌        | 120/782 [04:55<27:06,  2.46s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 121/782 [04:58<27:41,  2.51s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 122/782 [05:00<27:52,  2.53s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 123/782 [05:03<28:07,  2.56s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 124/782 [05:06<28:03,  2.56s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 125/782 [05:08<27:01,  2.47s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 126/782 [05:10<27:09,  2.48s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 127/782 [05:13<26:58,  2.47s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 128/782 [05:15<27:25,  2.52s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 129/782 [05:18<26:38,  2.45s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 130/782 [05:20<26:31,  2.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6727, 'grad_norm': 2.012089967727661, 'learning_rate': 4.9847376605981866e-05, 'rewards/chosen': -0.0988091379404068, 'rewards/rejected': -0.1766899824142456, 'rewards/accuracies': 0.5625, 'rewards/margins': 0.0778808742761612, 'logps/chosen': -184.111572265625, 'logps/rejected': -179.52410888671875, 'logits/chosen': -1.4508178234100342, 'logits/rejected': -1.3472976684570312, 'epoch': 0.17}\u001b[0m\n",
      "\u001b[34m17%|█▋        | 130/782 [05:20<26:31,  2.44s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 131/782 [05:23<26:33,  2.45s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 132/782 [05:25<25:31,  2.36s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 133/782 [05:27<25:46,  2.38s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 134/782 [05:30<25:46,  2.39s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 135/782 [05:32<25:13,  2.34s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 136/782 [05:34<25:54,  2.41s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 137/782 [05:37<25:34,  2.38s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 138/782 [05:39<25:36,  2.39s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 139/782 [05:42<25:42,  2.40s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 140/782 [05:44<25:42,  2.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6727, 'grad_norm': 2.114039182662964, 'learning_rate': 4.969400741032999e-05, 'rewards/chosen': -0.027588138356804848, 'rewards/rejected': -0.10035945475101471, 'rewards/accuracies': 0.58203125, 'rewards/margins': 0.07277131080627441, 'logps/chosen': -174.10269165039062, 'logps/rejected': -171.28329467773438, 'logits/chosen': -1.3107030391693115, 'logits/rejected': -1.2428001165390015, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 140/782 [05:44<25:42,  2.40s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 141/782 [05:47<26:30,  2.48s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 142/782 [05:49<26:38,  2.50s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 143/782 [05:52<26:32,  2.49s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 144/782 [05:54<25:26,  2.39s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 145/782 [05:56<25:41,  2.42s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 146/782 [05:59<26:11,  2.47s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 147/782 [06:01<26:02,  2.46s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 148/782 [06:04<26:34,  2.52s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 149/782 [06:06<26:31,  2.52s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 150/782 [06:09<26:21,  2.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6764, 'grad_norm': 2.017446756362915, 'learning_rate': 4.948824853131236e-05, 'rewards/chosen': 0.009160919114947319, 'rewards/rejected': -0.055823106318712234, 'rewards/accuracies': 0.57421875, 'rewards/margins': 0.064984031021595, 'logps/chosen': -178.1321258544922, 'logps/rejected': -180.36026000976562, 'logits/chosen': -1.218431830406189, 'logits/rejected': -1.1373533010482788, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[34m19%|█▉        | 150/782 [06:09<26:21,  2.50s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 151/782 [06:11<25:57,  2.47s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 152/782 [06:14<26:13,  2.50s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 153/782 [06:17<26:42,  2.55s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 154/782 [06:19<26:56,  2.57s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 155/782 [06:21<25:55,  2.48s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 156/782 [06:24<25:52,  2.48s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 157/782 [06:26<25:26,  2.44s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 158/782 [06:29<25:23,  2.44s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 159/782 [06:31<25:41,  2.47s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 160/782 [06:34<25:39,  2.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6697, 'grad_norm': 1.9590259790420532, 'learning_rate': 4.923053649761152e-05, 'rewards/chosen': 0.0019894989673048258, 'rewards/rejected': -0.07655742764472961, 'rewards/accuracies': 0.5679687261581421, 'rewards/margins': 0.0785469263792038, 'logps/chosen': -181.90570068359375, 'logps/rejected': -178.19754028320312, 'logits/chosen': -1.2449184656143188, 'logits/rejected': -1.1889607906341553, 'epoch': 0.2}\u001b[0m\n",
      "\u001b[34m20%|██        | 160/782 [06:34<25:39,  2.48s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 161/782 [06:36<25:50,  2.50s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 162/782 [06:39<25:16,  2.45s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 163/782 [06:41<25:01,  2.43s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 164/782 [06:43<24:44,  2.40s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 165/782 [06:46<24:47,  2.41s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 166/782 [06:48<25:25,  2.48s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 167/782 [06:51<24:52,  2.43s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 168/782 [06:53<24:34,  2.40s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 169/782 [06:55<23:57,  2.34s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 170/782 [06:58<24:06,  2.36s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6843, 'grad_norm': 1.9720110893249512, 'learning_rate': 4.8921418059360846e-05, 'rewards/chosen': -0.01689869910478592, 'rewards/rejected': -0.07380108535289764, 'rewards/accuracies': 0.5406249761581421, 'rewards/margins': 0.05690238997340202, 'logps/chosen': -170.97305297851562, 'logps/rejected': -170.25772094726562, 'logits/chosen': -1.1834571361541748, 'logits/rejected': -1.1195149421691895, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[34m22%|██▏       | 170/782 [06:58<24:06,  2.36s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 171/782 [07:00<24:34,  2.41s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 172/782 [07:03<25:15,  2.48s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 173/782 [07:05<25:08,  2.48s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 174/782 [07:08<24:33,  2.42s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 175/782 [07:10<23:56,  2.37s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 176/782 [07:12<23:59,  2.37s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 177/782 [07:15<24:12,  2.40s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 178/782 [07:17<24:27,  2.43s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 179/782 [07:20<25:10,  2.51s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 180/782 [07:22<24:46,  2.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6722, 'grad_norm': 2.0528624057769775, 'learning_rate': 4.8561549028184316e-05, 'rewards/chosen': 0.09294506162405014, 'rewards/rejected': 0.01901308074593544, 'rewards/accuracies': 0.5687500238418579, 'rewards/margins': 0.0739319771528244, 'logps/chosen': -177.43809509277344, 'logps/rejected': -174.54934692382812, 'logits/chosen': -1.2355965375900269, 'logits/rejected': -1.156317949295044, 'epoch': 0.23}\u001b[0m\n",
      "\u001b[34m23%|██▎       | 180/782 [07:22<24:46,  2.47s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 181/782 [07:25<25:02,  2.50s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 182/782 [07:27<24:52,  2.49s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 183/782 [07:30<24:42,  2.47s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 184/782 [07:32<24:31,  2.46s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 185/782 [07:35<24:50,  2.50s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 186/782 [07:37<24:24,  2.46s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 187/782 [07:40<24:16,  2.45s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 188/782 [07:42<24:54,  2.52s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 189/782 [07:45<25:08,  2.54s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 190/782 [07:47<24:54,  2.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6756, 'grad_norm': 1.94681715965271, 'learning_rate': 4.815169288585641e-05, 'rewards/chosen': 0.04334985092282295, 'rewards/rejected': -0.02968698740005493, 'rewards/accuracies': 0.55078125, 'rewards/margins': 0.07303683459758759, 'logps/chosen': -182.96505737304688, 'logps/rejected': -185.0191192626953, 'logits/chosen': -1.3008991479873657, 'logits/rejected': -1.2108210325241089, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▍       | 190/782 [07:47<24:54,  2.52s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 191/782 [07:50<24:18,  2.47s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 192/782 [07:52<24:11,  2.46s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 193/782 [07:55<24:30,  2.50s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 194/782 [07:57<24:29,  2.50s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 195/782 [08:00<24:24,  2.49s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 196/782 [08:02<23:41,  2.43s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 197/782 [08:04<23:41,  2.43s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 198/782 [08:07<23:08,  2.38s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 199/782 [08:09<22:45,  2.34s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 200/782 [08:11<23:16,  2.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6746, 'grad_norm': 2.070117235183716, 'learning_rate': 4.769271916453386e-05, 'rewards/chosen': 0.025567537173628807, 'rewards/rejected': -0.04559202119708061, 'rewards/accuracies': 0.5703125, 'rewards/margins': 0.07115955650806427, 'logps/chosen': -175.04147338867188, 'logps/rejected': -169.6757049560547, 'logits/chosen': -1.2880470752716064, 'logits/rejected': -1.230995535850525, 'epoch': 0.26}\u001b[0m\n",
      "\u001b[34m26%|██▌       | 200/782 [08:11<23:16,  2.40s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 201/782 [08:14<24:03,  2.48s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 202/782 [08:16<23:32,  2.44s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 203/782 [08:19<23:21,  2.42s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 204/782 [08:21<23:18,  2.42s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 205/782 [08:23<22:50,  2.37s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 206/782 [08:26<23:00,  2.40s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 207/782 [08:29<23:27,  2.45s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 208/782 [08:31<23:21,  2.44s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 209/782 [08:33<23:27,  2.46s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 210/782 [08:36<22:59,  2.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6728, 'grad_norm': 1.952705979347229, 'learning_rate': 4.718560160199579e-05, 'rewards/chosen': -0.012422295287251472, 'rewards/rejected': -0.09615439176559448, 'rewards/accuracies': 0.553906261920929, 'rewards/margins': 0.08373209834098816, 'logps/chosen': -171.83901977539062, 'logps/rejected': -166.32925415039062, 'logits/chosen': -1.347609281539917, 'logits/rejected': -1.295588493347168, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 210/782 [08:36<22:59,  2.41s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 211/782 [08:38<23:15,  2.44s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 212/782 [08:41<23:37,  2.49s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 213/782 [08:43<23:17,  2.46s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 214/782 [08:46<23:07,  2.44s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 215/782 [08:48<22:25,  2.37s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 216/782 [08:50<22:29,  2.38s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 217/782 [08:53<22:49,  2.42s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 218/782 [08:55<23:01,  2.45s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 219/782 [08:58<22:49,  2.43s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 220/782 [09:00<22:10,  2.37s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6734, 'grad_norm': 1.9563318490982056, 'learning_rate': 4.663141607580589e-05, 'rewards/chosen': 0.07296161353588104, 'rewards/rejected': -0.0009059328585863113, 'rewards/accuracies': 0.585156261920929, 'rewards/margins': 0.0738675445318222, 'logps/chosen': -167.6881866455078, 'logps/rejected': -167.51913452148438, 'logits/chosen': -1.3057541847229004, 'logits/rejected': -1.2696529626846313, 'epoch': 0.28}\u001b[0m\n",
      "\u001b[34m28%|██▊       | 220/782 [09:00<22:10,  2.37s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 221/782 [09:02<22:31,  2.41s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 222/782 [09:05<22:14,  2.38s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 223/782 [09:07<22:35,  2.43s/it]\u001b[0m\n",
      "\u001b[34m29%|██▊       | 224/782 [09:10<23:00,  2.47s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 225/782 [09:12<22:48,  2.46s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 226/782 [09:15<23:04,  2.49s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 227/782 [09:17<22:54,  2.48s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 228/782 [09:20<22:42,  2.46s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 229/782 [09:22<23:12,  2.52s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 230/782 [09:25<23:02,  2.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6799, 'grad_norm': 1.9400829076766968, 'learning_rate': 4.6031338320779534e-05, 'rewards/chosen': 0.08386192470788956, 'rewards/rejected': 0.020102977752685547, 'rewards/accuracies': 0.569531261920929, 'rewards/margins': 0.0637589544057846, 'logps/chosen': -169.79409790039062, 'logps/rejected': -174.01515197753906, 'logits/chosen': -1.3038030862808228, 'logits/rejected': -1.1996023654937744, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[34m29%|██▉       | 230/782 [09:25<23:02,  2.51s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 231/782 [09:27<23:11,  2.53s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 232/782 [09:30<22:27,  2.45s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 233/782 [09:32<22:32,  2.46s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 234/782 [09:35<22:20,  2.45s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 235/782 [09:37<22:27,  2.46s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 236/782 [09:40<22:56,  2.52s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 237/782 [09:42<22:32,  2.48s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 238/782 [09:45<22:26,  2.47s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 239/782 [09:47<22:17,  2.46s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 240/782 [09:49<22:03,  2.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6698, 'grad_norm': 1.9255740642547607, 'learning_rate': 4.538664143459819e-05, 'rewards/chosen': 0.04781452938914299, 'rewards/rejected': -0.03573823720216751, 'rewards/accuracies': 0.58984375, 'rewards/margins': 0.0835527703166008, 'logps/chosen': -174.8519287109375, 'logps/rejected': -168.48611450195312, 'logits/chosen': -1.2946836948394775, 'logits/rejected': -1.2441439628601074, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[34m31%|███       | 240/782 [09:49<22:03,  2.44s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 241/782 [09:52<22:00,  2.44s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 242/782 [09:54<22:12,  2.47s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 243/782 [09:57<22:09,  2.47s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 244/782 [09:59<21:55,  2.45s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 245/782 [10:02<22:01,  2.46s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 246/782 [10:04<22:00,  2.46s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 247/782 [10:07<21:48,  2.45s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 248/782 [10:09<22:04,  2.48s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 249/782 [10:11<21:31,  2.42s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 250/782 [10:14<21:37,  2.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6531, 'grad_norm': 1.992404818534851, 'learning_rate': 4.469869317686332e-05, 'rewards/chosen': 0.0964711531996727, 'rewards/rejected': -0.02483990602195263, 'rewards/accuracies': 0.616406261920929, 'rewards/margins': 0.12131105363368988, 'logps/chosen': -177.867919921875, 'logps/rejected': -180.3314666748047, 'logits/chosen': -1.2767255306243896, 'logits/rejected': -1.1821397542953491, 'epoch': 0.32}\u001b[0m\n",
      "\u001b[34m32%|███▏      | 250/782 [10:14<21:37,  2.44s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 251/782 [10:17<22:04,  2.49s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 252/782 [10:19<22:38,  2.56s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 253/782 [10:22<22:14,  2.52s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 254/782 [10:24<22:21,  2.54s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 255/782 [10:27<22:09,  2.52s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 256/782 [10:29<22:28,  2.56s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 257/782 [10:32<21:56,  2.51s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 258/782 [10:34<21:29,  2.46s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 259/782 [10:37<21:41,  2.49s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 260/782 [10:39<21:55,  2.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6693, 'grad_norm': 2.1449809074401855, 'learning_rate': 4.3968953067319777e-05, 'rewards/chosen': 0.11896656453609467, 'rewards/rejected': 0.02374548837542534, 'rewards/accuracies': 0.5765625238418579, 'rewards/margins': 0.09522107243537903, 'logps/chosen': -180.3020782470703, 'logps/rejected': -177.64468383789062, 'logits/chosen': -1.2766517400741577, 'logits/rejected': -1.2327113151550293, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 260/782 [10:39<21:55,  2.52s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 261/782 [10:42<21:25,  2.47s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 262/782 [10:44<21:41,  2.50s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 263/782 [10:47<21:52,  2.53s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 264/782 [10:49<21:22,  2.48s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 265/782 [10:52<21:24,  2.48s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 266/782 [10:54<20:26,  2.38s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 267/782 [10:56<20:54,  2.44s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 268/782 [10:59<21:12,  2.47s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 269/782 [11:01<20:37,  2.41s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 270/782 [11:04<20:19,  2.38s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6633, 'grad_norm': 1.9822360277175903, 'learning_rate': 4.3198969289405055e-05, 'rewards/chosen': 0.10384942591190338, 'rewards/rejected': -0.014647385105490685, 'rewards/accuracies': 0.5921875238418579, 'rewards/margins': 0.11849681288003922, 'logps/chosen': -177.31787109375, 'logps/rejected': -169.41793823242188, 'logits/chosen': -1.3209583759307861, 'logits/rejected': -1.2695859670639038, 'epoch': 0.35}\u001b[0m\n",
      "\u001b[34m35%|███▍      | 270/782 [11:04<20:19,  2.38s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 271/782 [11:06<20:07,  2.36s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 272/782 [11:08<20:11,  2.38s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 273/782 [11:11<20:29,  2.42s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 274/782 [11:13<20:49,  2.46s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 275/782 [11:16<20:52,  2.47s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 276/782 [11:18<20:41,  2.45s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 277/782 [11:21<20:39,  2.45s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 278/782 [11:23<20:54,  2.49s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 279/782 [11:26<20:48,  2.48s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 280/782 [11:28<21:01,  2.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6659, 'grad_norm': 1.993337869644165, 'learning_rate': 4.2390375405693726e-05, 'rewards/chosen': 0.10075177252292633, 'rewards/rejected': -0.019008873030543327, 'rewards/accuracies': 0.582812488079071, 'rewards/margins': 0.1197606548666954, 'logps/chosen': -169.89279174804688, 'logps/rejected': -170.54737854003906, 'logits/chosen': -1.3386210203170776, 'logits/rejected': -1.269540786743164, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 280/782 [11:28<21:01,  2.51s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 281/782 [11:31<21:15,  2.55s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 282/782 [11:33<21:05,  2.53s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 283/782 [11:36<20:36,  2.48s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 284/782 [11:38<20:29,  2.47s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 285/782 [11:41<20:31,  2.48s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 286/782 [11:43<20:02,  2.42s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 287/782 [11:45<20:06,  2.44s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 288/782 [11:48<20:29,  2.49s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 289/782 [11:51<20:32,  2.50s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 290/782 [11:53<20:14,  2.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6767, 'grad_norm': 2.230504035949707, 'learning_rate': 4.154488689220536e-05, 'rewards/chosen': 0.09265555441379547, 'rewards/rejected': 0.0003241088124923408, 'rewards/accuracies': 0.585156261920929, 'rewards/margins': 0.09233144670724869, 'logps/chosen': -180.0294647216797, 'logps/rejected': -183.99755859375, 'logits/chosen': -1.360361099243164, 'logits/rejected': -1.2765628099441528, 'epoch': 0.37}\u001b[0m\n",
      "\u001b[34m37%|███▋      | 290/782 [11:53<20:14,  2.47s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 291/782 [11:55<20:03,  2.45s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 292/782 [11:58<19:24,  2.38s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 293/782 [12:00<19:21,  2.38s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 294/782 [12:02<19:30,  2.40s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 295/782 [12:05<19:47,  2.44s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 296/782 [12:07<19:23,  2.39s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 297/782 [12:10<19:12,  2.38s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 298/782 [12:12<19:33,  2.42s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 299/782 [12:15<19:28,  2.42s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 300/782 [12:17<19:34,  2.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6603, 'grad_norm': 2.096552610397339, 'learning_rate': 4.066429749892854e-05, 'rewards/chosen': 0.14553841948509216, 'rewards/rejected': 0.026662886142730713, 'rewards/accuracies': 0.5914062261581421, 'rewards/margins': 0.11887552589178085, 'logps/chosen': -180.73947143554688, 'logps/rejected': -179.41981506347656, 'logits/chosen': -1.3509050607681274, 'logits/rejected': -1.2334034442901611, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[34m38%|███▊      | 300/782 [12:17<19:34,  2.44s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 301/782 [12:20<20:23,  2.54s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 302/782 [12:22<20:10,  2.52s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 303/782 [12:24<19:18,  2.42s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 304/782 [12:27<19:28,  2.44s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 305/782 [12:30<19:41,  2.48s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 306/782 [12:32<19:30,  2.46s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 307/782 [12:34<19:13,  2.43s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 308/782 [12:37<19:15,  2.44s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 309/782 [12:39<19:03,  2.42s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 310/782 [12:42<19:19,  2.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6636, 'grad_norm': 2.016263484954834, 'learning_rate': 3.975047544428254e-05, 'rewards/chosen': 0.12972882390022278, 'rewards/rejected': 0.013692943379282951, 'rewards/accuracies': 0.58203125, 'rewards/margins': 0.11603586375713348, 'logps/chosen': -179.02798461914062, 'logps/rejected': -169.90867614746094, 'logits/chosen': -1.288948655128479, 'logits/rejected': -1.2244186401367188, 'epoch': 0.4}\u001b[0m\n",
      "\u001b[34m40%|███▉      | 310/782 [12:42<19:19,  2.46s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 311/782 [12:44<19:18,  2.46s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 312/782 [12:46<18:42,  2.39s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 313/782 [12:49<19:00,  2.43s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 314/782 [12:51<19:12,  2.46s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 315/782 [12:54<19:34,  2.51s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 316/782 [12:57<19:23,  2.50s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 317/782 [12:59<18:52,  2.43s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 318/782 [13:01<18:22,  2.38s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 319/782 [13:03<18:22,  2.38s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 320/782 [13:06<18:18,  2.38s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6516, 'grad_norm': 1.9789669513702393, 'learning_rate': 3.880535945158997e-05, 'rewards/chosen': 0.14685693383216858, 'rewards/rejected': 0.010516973212361336, 'rewards/accuracies': 0.598437488079071, 'rewards/margins': 0.1363399475812912, 'logps/chosen': -178.4857177734375, 'logps/rejected': -176.27645874023438, 'logits/chosen': -1.2519887685775757, 'logits/rejected': -1.2004790306091309, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m41%|████      | 320/782 [13:06<18:18,  2.38s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 321/782 [13:08<18:48,  2.45s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 322/782 [13:11<18:32,  2.42s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 323/782 [13:13<18:42,  2.45s/it]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 324/782 [13:16<18:36,  2.44s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 325/782 [13:18<18:13,  2.39s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 326/782 [13:20<18:14,  2.40s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 327/782 [13:23<18:12,  2.40s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 328/782 [13:25<18:07,  2.39s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 329/782 [13:28<18:16,  2.42s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 330/782 [13:30<18:07,  2.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6573, 'grad_norm': 2.1887693405151367, 'learning_rate': 3.78309546359696e-05, 'rewards/chosen': 0.14307858049869537, 'rewards/rejected': 0.0028618560172617435, 'rewards/accuracies': 0.59375, 'rewards/margins': 0.14021673798561096, 'logps/chosen': -183.0562286376953, 'logps/rejected': -173.3577117919922, 'logits/chosen': -1.2846072912216187, 'logits/rejected': -1.2209722995758057, 'epoch': 0.42}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 330/782 [13:30<18:07,  2.40s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 331/782 [13:32<17:56,  2.39s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 332/782 [13:35<18:06,  2.41s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 333/782 [13:37<18:01,  2.41s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 334/782 [13:40<18:12,  2.44s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 335/782 [13:42<17:45,  2.38s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 336/782 [13:44<17:53,  2.41s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 337/782 [13:47<17:26,  2.35s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 338/782 [13:49<17:44,  2.40s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 339/782 [13:52<17:58,  2.43s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 340/782 [13:54<17:57,  2.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6685, 'grad_norm': 2.16020131111145, 'learning_rate': 3.682932825037523e-05, 'rewards/chosen': 0.15279321372509003, 'rewards/rejected': 0.0318036712706089, 'rewards/accuracies': 0.5859375, 'rewards/margins': 0.12098954617977142, 'logps/chosen': -172.36830139160156, 'logps/rejected': -173.81570434570312, 'logits/chosen': -1.2768962383270264, 'logits/rejected': -1.2322196960449219, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m43%|████▎     | 340/782 [13:54<17:57,  2.44s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 341/782 [13:56<17:27,  2.38s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 342/782 [13:59<17:01,  2.32s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 343/782 [14:01<17:20,  2.37s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 344/782 [14:04<17:31,  2.40s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 345/782 [14:06<17:36,  2.42s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 346/782 [14:08<17:38,  2.43s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 347/782 [14:11<17:55,  2.47s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 348/782 [14:13<17:40,  2.44s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 349/782 [14:16<17:51,  2.47s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 350/782 [14:18<17:11,  2.39s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6672, 'grad_norm': 1.7660123109817505, 'learning_rate': 3.580260529980584e-05, 'rewards/chosen': 0.2147027999162674, 'rewards/rejected': 0.09599385410547256, 'rewards/accuracies': 0.5921875238418579, 'rewards/margins': 0.11870895326137543, 'logps/chosen': -168.5792694091797, 'logps/rejected': -174.44601440429688, 'logits/chosen': -1.1861947774887085, 'logits/rejected': -1.1154292821884155, 'epoch': 0.45}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 350/782 [14:18<17:11,  2.39s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 351/782 [14:21<17:12,  2.40s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 352/782 [14:23<17:23,  2.43s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 353/782 [14:26<17:41,  2.47s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 354/782 [14:28<17:35,  2.47s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 355/782 [14:31<17:48,  2.50s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 356/782 [14:33<17:55,  2.52s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 357/782 [14:36<17:47,  2.51s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 358/782 [14:38<17:37,  2.49s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 359/782 [14:41<17:34,  2.49s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 360/782 [14:43<17:01,  2.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6677, 'grad_norm': 1.735750675201416, 'learning_rate': 3.4752964032991634e-05, 'rewards/chosen': 0.27990812063217163, 'rewards/rejected': 0.1718486100435257, 'rewards/accuracies': 0.5914062261581421, 'rewards/margins': 0.10805950313806534, 'logps/chosen': -170.1289825439453, 'logps/rejected': -172.8999481201172, 'logits/chosen': -1.1824848651885986, 'logits/rejected': -1.1242835521697998, 'epoch': 0.46}\u001b[0m\n",
      "\u001b[34m46%|████▌     | 360/782 [14:43<17:01,  2.42s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 361/782 [14:46<17:26,  2.49s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 362/782 [14:48<17:16,  2.47s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 363/782 [14:51<17:36,  2.52s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 364/782 [14:53<17:02,  2.45s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 365/782 [14:55<16:51,  2.43s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 366/782 [14:58<16:39,  2.40s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 367/782 [15:00<17:03,  2.47s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 368/782 [15:03<17:17,  2.51s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 369/782 [15:05<16:56,  2.46s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 370/782 [15:08<16:44,  2.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6638, 'grad_norm': 2.0870110988616943, 'learning_rate': 3.3682631321120504e-05, 'rewards/chosen': 0.30062440037727356, 'rewards/rejected': 0.18871566653251648, 'rewards/accuracies': 0.58984375, 'rewards/margins': 0.1119086965918541, 'logps/chosen': -175.36441040039062, 'logps/rejected': -170.91819763183594, 'logits/chosen': -1.2130099534988403, 'logits/rejected': -1.1373940706253052, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 370/782 [15:08<16:44,  2.44s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 371/782 [15:10<16:44,  2.44s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 372/782 [15:12<16:37,  2.43s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 373/782 [15:15<16:39,  2.44s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 374/782 [15:17<16:24,  2.41s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 375/782 [15:20<16:35,  2.45s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 376/782 [15:22<16:15,  2.40s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 377/782 [15:25<16:36,  2.46s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 378/782 [15:27<16:49,  2.50s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 379/782 [15:30<16:31,  2.46s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 380/782 [15:32<16:18,  2.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6661, 'grad_norm': 1.9291225671768188, 'learning_rate': 3.2593877933409434e-05, 'rewards/chosen': 0.27384668588638306, 'rewards/rejected': 0.16773463785648346, 'rewards/accuracies': 0.5882812738418579, 'rewards/margins': 0.10611201822757721, 'logps/chosen': -177.3420867919922, 'logps/rejected': -171.528076171875, 'logits/chosen': -1.2181158065795898, 'logits/rejected': -1.1420252323150635, 'epoch': 0.49}\u001b[0m\n",
      "\u001b[34m49%|████▊     | 380/782 [15:32<16:18,  2.43s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 381/782 [15:35<16:20,  2.45s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 382/782 [15:37<16:21,  2.45s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 383/782 [15:40<16:27,  2.47s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 384/782 [15:42<16:04,  2.42s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 385/782 [15:44<16:04,  2.43s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 386/782 [15:47<15:53,  2.41s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 387/782 [15:49<15:57,  2.42s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 388/782 [15:51<15:46,  2.40s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 389/782 [15:54<15:41,  2.40s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 390/782 [15:56<15:26,  2.36s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6592, 'grad_norm': 1.894286036491394, 'learning_rate': 3.1489013719543706e-05, 'rewards/chosen': 0.2563883364200592, 'rewards/rejected': 0.12915459275245667, 'rewards/accuracies': 0.6031249761581421, 'rewards/margins': 0.12723374366760254, 'logps/chosen': -171.1841583251953, 'logps/rejected': -166.2349090576172, 'logits/chosen': -1.2060647010803223, 'logits/rejected': -1.1615113019943237, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|████▉     | 390/782 [15:56<15:26,  2.36s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 391/782 [15:59<15:43,  2.41s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 392/782 [16:01<15:43,  2.42s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 393/782 [16:04<15:47,  2.44s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 394/782 [16:06<15:36,  2.41s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 395/782 [16:08<15:44,  2.44s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 396/782 [16:11<15:38,  2.43s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 397/782 [16:13<15:45,  2.46s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 398/782 [16:16<15:36,  2.44s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 399/782 [16:18<15:40,  2.45s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 400/782 [16:21<15:42,  2.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6594, 'grad_norm': 2.0102357864379883, 'learning_rate': 3.0370382709204885e-05, 'rewards/chosen': 0.2339547872543335, 'rewards/rejected': 0.10363759845495224, 'rewards/accuracies': 0.592968761920929, 'rewards/margins': 0.13031718134880066, 'logps/chosen': -174.55221557617188, 'logps/rejected': -174.2601318359375, 'logits/chosen': -1.2832982540130615, 'logits/rejected': -1.2317041158676147, 'epoch': 0.51}\u001b[0m\n",
      "\u001b[34m51%|█████     | 400/782 [16:21<15:42,  2.47s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 401/782 [16:24<16:18,  2.57s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 402/782 [16:26<16:05,  2.54s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 403/782 [16:28<15:41,  2.48s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 404/782 [16:31<15:27,  2.45s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 405/782 [16:33<14:55,  2.37s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 406/782 [16:35<14:57,  2.39s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 407/782 [16:38<15:02,  2.41s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 408/782 [16:40<15:08,  2.43s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 409/782 [16:43<15:16,  2.46s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 410/782 [16:45<14:55,  2.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6584, 'grad_norm': 2.205436944961548, 'learning_rate': 2.9240358139084017e-05, 'rewards/chosen': 0.22171755135059357, 'rewards/rejected': 0.08066529035568237, 'rewards/accuracies': 0.5992187261581421, 'rewards/margins': 0.1410522609949112, 'logps/chosen': -178.94541931152344, 'logps/rejected': -176.21524047851562, 'logits/chosen': -1.2630817890167236, 'logits/rejected': -1.193098783493042, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 410/782 [16:45<14:55,  2.41s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 411/782 [16:48<14:59,  2.42s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 412/782 [16:50<15:07,  2.45s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 413/782 [16:53<15:11,  2.47s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 414/782 [16:55<15:03,  2.46s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 415/782 [16:57<14:53,  2.43s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 416/782 [17:00<14:59,  2.46s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 417/782 [17:02<14:37,  2.40s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 418/782 [17:05<14:41,  2.42s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 419/782 [17:07<14:53,  2.46s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 420/782 [17:10<14:54,  2.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6694, 'grad_norm': 1.9615511894226074, 'learning_rate': 2.8101337417930524e-05, 'rewards/chosen': 0.23202160000801086, 'rewards/rejected': 0.11647336184978485, 'rewards/accuracies': 0.5703125, 'rewards/margins': 0.11554821580648422, 'logps/chosen': -179.4777069091797, 'logps/rejected': -177.3743896484375, 'logits/chosen': -1.235626459121704, 'logits/rejected': -1.2076479196548462, 'epoch': 0.54}\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 420/782 [17:10<14:54,  2.47s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 421/782 [17:12<14:49,  2.46s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 422/782 [17:15<14:38,  2.44s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 423/782 [17:17<15:01,  2.51s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 424/782 [17:20<14:49,  2.49s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 425/782 [17:22<14:56,  2.51s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 426/782 [17:25<14:43,  2.48s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 427/782 [17:27<14:38,  2.47s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 428/782 [17:30<14:36,  2.48s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 429/782 [17:32<14:36,  2.48s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 430/782 [17:35<14:40,  2.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6634, 'grad_norm': 2.34116792678833, 'learning_rate': 2.695573704031885e-05, 'rewards/chosen': 0.24488358199596405, 'rewards/rejected': 0.12399639189243317, 'rewards/accuracies': 0.5843750238418579, 'rewards/margins': 0.12088720500469208, 'logps/chosen': -178.8231201171875, 'logps/rejected': -178.88406372070312, 'logits/chosen': -1.243303894996643, 'logits/rejected': -1.1911907196044922, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 430/782 [17:35<14:40,  2.50s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 431/782 [17:37<14:35,  2.49s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 432/782 [17:39<14:25,  2.47s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 433/782 [17:42<14:15,  2.45s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 434/782 [17:44<13:51,  2.39s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 435/782 [17:47<14:02,  2.43s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 436/782 [17:49<14:10,  2.46s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 437/782 [17:52<14:09,  2.46s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 438/782 [17:54<14:09,  2.47s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 439/782 [17:56<13:49,  2.42s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 440/782 [17:59<13:49,  2.42s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6698, 'grad_norm': 2.1490719318389893, 'learning_rate': 2.580598745992342e-05, 'rewards/chosen': 0.213405579328537, 'rewards/rejected': 0.10059022903442383, 'rewards/accuracies': 0.5757812261581421, 'rewards/margins': 0.11281536519527435, 'logps/chosen': -173.33172607421875, 'logps/rejected': -170.39247131347656, 'logits/chosen': -1.2646182775497437, 'logits/rejected': -1.2020642757415771, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 440/782 [17:59<13:49,  2.42s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 441/782 [18:01<14:00,  2.46s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 442/782 [18:04<14:05,  2.49s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 443/782 [18:07<14:10,  2.51s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 444/782 [18:09<13:47,  2.45s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 445/782 [18:11<13:42,  2.44s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 446/782 [18:14<13:35,  2.43s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 447/782 [18:16<13:43,  2.46s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 448/782 [18:19<13:50,  2.49s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 449/782 [18:21<13:48,  2.49s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 450/782 [18:24<13:56,  2.52s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6525, 'grad_norm': 1.9959771633148193, 'learning_rate': 2.4654527933178647e-05, 'rewards/chosen': 0.23017163574695587, 'rewards/rejected': 0.07955445349216461, 'rewards/accuracies': 0.6039062738418579, 'rewards/margins': 0.15061721205711365, 'logps/chosen': -174.38856506347656, 'logps/rejected': -175.69790649414062, 'logits/chosen': -1.2847670316696167, 'logits/rejected': -1.1977016925811768, 'epoch': 0.58}\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 450/782 [18:24<13:56,  2.52s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 451/782 [18:26<13:52,  2.51s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 452/782 [18:29<13:49,  2.51s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 453/782 [18:31<13:41,  2.50s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 454/782 [18:34<13:48,  2.52s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 455/782 [18:36<13:24,  2.46s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 456/782 [18:39<13:22,  2.46s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 457/782 [18:41<13:18,  2.46s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 458/782 [18:44<13:24,  2.48s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 459/782 [18:46<13:26,  2.50s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 460/782 [18:49<13:20,  2.49s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.641, 'grad_norm': 1.9343485832214355, 'learning_rate': 2.3503801344263348e-05, 'rewards/chosen': 0.18658864498138428, 'rewards/rejected': 0.009877556934952736, 'rewards/accuracies': 0.628125011920929, 'rewards/margins': 0.17671111226081848, 'logps/chosen': -174.6220245361328, 'logps/rejected': -179.0773468017578, 'logits/chosen': -1.2403172254562378, 'logits/rejected': -1.193009376525879, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 460/782 [18:49<13:20,  2.49s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 461/782 [18:51<13:24,  2.51s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 462/782 [18:53<13:03,  2.45s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 463/782 [18:56<13:06,  2.47s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 464/782 [18:59<13:26,  2.53s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 465/782 [19:01<12:55,  2.45s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 466/782 [19:04<13:05,  2.49s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 467/782 [19:06<13:20,  2.54s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 468/782 [19:09<13:22,  2.56s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 469/782 [19:11<13:17,  2.55s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 470/782 [19:14<12:51,  2.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.652, 'grad_norm': 1.9746266603469849, 'learning_rate': 2.235624902238879e-05, 'rewards/chosen': 0.1238364577293396, 'rewards/rejected': -0.045052021741867065, 'rewards/accuracies': 0.604687511920929, 'rewards/margins': 0.16888847947120667, 'logps/chosen': -177.93748474121094, 'logps/rejected': -175.62435913085938, 'logits/chosen': -1.2869055271148682, 'logits/rejected': -1.2423222064971924, 'epoch': 0.6}\u001b[0m\n",
      "\u001b[34m60%|██████    | 470/782 [19:14<12:51,  2.47s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 471/782 [19:16<12:54,  2.49s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 472/782 [19:19<12:45,  2.47s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 473/782 [19:21<12:55,  2.51s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 474/782 [19:24<12:41,  2.47s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 475/782 [19:26<12:48,  2.50s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 476/782 [19:28<12:23,  2.43s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 477/782 [19:31<11:58,  2.36s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 478/782 [19:33<12:03,  2.38s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 479/782 [19:36<12:16,  2.43s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 480/782 [19:38<12:13,  2.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6472, 'grad_norm': 2.291252374649048, 'learning_rate': 2.1214305562385592e-05, 'rewards/chosen': 0.1437341272830963, 'rewards/rejected': -0.024413879960775375, 'rewards/accuracies': 0.6390625238418579, 'rewards/margins': 0.1681479960680008, 'logps/chosen': -174.76589965820312, 'logps/rejected': -169.55197143554688, 'logits/chosen': -1.3774769306182861, 'logits/rejected': -1.2950680255889893, 'epoch': 0.61}\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 480/782 [19:38<12:13,  2.43s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 481/782 [19:40<12:16,  2.45s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 482/782 [19:43<12:04,  2.41s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 483/782 [19:45<12:13,  2.45s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 484/782 [19:48<12:19,  2.48s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 485/782 [19:50<12:15,  2.48s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 486/782 [19:53<12:15,  2.49s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 487/782 [19:55<12:05,  2.46s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 488/782 [19:58<11:51,  2.42s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 489/782 [20:00<12:02,  2.46s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 490/782 [20:03<12:04,  2.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6507, 'grad_norm': 2.1674258708953857, 'learning_rate': 2.008039365957804e-05, 'rewards/chosen': 0.1780879944562912, 'rewards/rejected': 0.007824445143342018, 'rewards/accuracies': 0.6156250238418579, 'rewards/margins': 0.17026355862617493, 'logps/chosen': -184.36109924316406, 'logps/rejected': -182.3328399658203, 'logits/chosen': -1.3319690227508545, 'logits/rejected': -1.250218152999878, 'epoch': 0.63}\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 490/782 [20:03<12:04,  2.48s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 491/782 [20:05<12:00,  2.48s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 492/782 [20:07<11:47,  2.44s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 493/782 [20:10<11:45,  2.44s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 494/782 [20:12<11:37,  2.42s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 495/782 [20:15<11:29,  2.40s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 496/782 [20:17<11:24,  2.39s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 497/782 [20:20<11:32,  2.43s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 498/782 [20:22<11:22,  2.40s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 499/782 [20:24<11:34,  2.46s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 500/782 [20:27<11:34,  2.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6569, 'grad_norm': 2.1621997356414795, 'learning_rate': 1.895691896990388e-05, 'rewards/chosen': 0.20675769448280334, 'rewards/rejected': 0.04650575667619705, 'rewards/accuracies': 0.594531238079071, 'rewards/margins': 0.1602519303560257, 'logps/chosen': -175.98684692382812, 'logps/rejected': -177.46865844726562, 'logits/chosen': -1.3523781299591064, 'logits/rejected': -1.2796578407287598, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 500/782 [20:27<11:34,  2.46s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 501/782 [20:30<11:47,  2.52s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 502/782 [20:32<11:30,  2.46s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 503/782 [20:34<11:16,  2.42s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 504/782 [20:37<11:12,  2.42s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 505/782 [20:39<11:12,  2.43s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 506/782 [20:41<11:03,  2.41s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 507/782 [20:44<10:53,  2.38s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 508/782 [20:46<10:56,  2.40s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 509/782 [20:49<11:01,  2.42s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 510/782 [20:51<11:15,  2.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6648, 'grad_norm': 2.1352522373199463, 'learning_rate': 1.7846265006183977e-05, 'rewards/chosen': 0.1979748010635376, 'rewards/rejected': 0.06838320195674896, 'rewards/accuracies': 0.58984375, 'rewards/margins': 0.12959158420562744, 'logps/chosen': -172.70791625976562, 'logps/rejected': -172.67327880859375, 'logits/chosen': -1.3208980560302734, 'logits/rejected': -1.2418776750564575, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 510/782 [20:51<11:15,  2.48s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 511/782 [20:54<11:21,  2.51s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 512/782 [20:56<11:13,  2.50s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 513/782 [20:59<10:59,  2.45s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 514/782 [21:01<10:54,  2.44s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 515/782 [21:04<10:56,  2.46s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 516/782 [21:06<11:01,  2.49s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 517/782 [21:09<10:49,  2.45s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 518/782 [21:11<10:47,  2.45s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 519/782 [21:14<11:02,  2.52s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 520/782 [21:16<10:50,  2.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6555, 'grad_norm': 2.0970866680145264, 'learning_rate': 1.6750788081369952e-05, 'rewards/chosen': 0.22459609806537628, 'rewards/rejected': 0.07622547447681427, 'rewards/accuracies': 0.61328125, 'rewards/margins': 0.148370623588562, 'logps/chosen': -182.48074340820312, 'logps/rejected': -179.33966064453125, 'logits/chosen': -1.2940412759780884, 'logits/rejected': -1.2192097902297974, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 520/782 [21:16<10:50,  2.48s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 521/782 [21:19<10:45,  2.47s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 522/782 [21:21<10:26,  2.41s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 523/782 [21:23<10:18,  2.39s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 524/782 [21:26<10:17,  2.39s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 525/782 [21:28<10:31,  2.46s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 526/782 [21:30<10:07,  2.37s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 527/782 [21:33<10:10,  2.39s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 528/782 [21:35<10:03,  2.38s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 529/782 [21:38<10:16,  2.44s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 530/782 [21:40<10:20,  2.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6535, 'grad_norm': 2.0319716930389404, 'learning_rate': 1.5672812309497724e-05, 'rewards/chosen': 0.20462314784526825, 'rewards/rejected': 0.05152306705713272, 'rewards/accuracies': 0.5960937738418579, 'rewards/margins': 0.15310007333755493, 'logps/chosen': -167.81484985351562, 'logps/rejected': -165.76791381835938, 'logits/chosen': -1.3124456405639648, 'logits/rejected': -1.2393895387649536, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 530/782 [21:40<10:20,  2.46s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 531/782 [21:43<10:22,  2.48s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 532/782 [21:45<10:02,  2.41s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 533/782 [21:47<09:57,  2.40s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 534/782 [21:50<10:07,  2.45s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 535/782 [21:52<10:07,  2.46s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 536/782 [21:55<09:54,  2.42s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 537/782 [21:57<10:00,  2.45s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 538/782 [22:00<10:10,  2.50s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 539/782 [22:03<10:21,  2.56s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 540/782 [22:05<10:08,  2.51s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6499, 'grad_norm': 2.0990965366363525, 'learning_rate': 1.4614624674952842e-05, 'rewards/chosen': 0.19406922161579132, 'rewards/rejected': 0.024481484666466713, 'rewards/accuracies': 0.6078125238418579, 'rewards/margins': 0.16958774626255035, 'logps/chosen': -183.17816162109375, 'logps/rejected': -180.4830322265625, 'logits/chosen': -1.3120214939117432, 'logits/rejected': -1.2275261878967285, 'epoch': 0.69}\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 540/782 [22:05<10:08,  2.51s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 541/782 [22:07<10:05,  2.51s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 542/782 [22:10<09:54,  2.48s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 543/782 [22:12<09:44,  2.45s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 544/782 [22:15<10:01,  2.53s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 545/782 [22:17<09:43,  2.46s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 546/782 [22:20<09:43,  2.47s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 547/782 [22:22<09:41,  2.47s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 548/782 [22:25<09:47,  2.51s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 549/782 [22:27<09:26,  2.43s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 550/782 [22:29<09:17,  2.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.655, 'grad_norm': 1.9263451099395752, 'learning_rate': 1.3578470180508432e-05, 'rewards/chosen': 0.1727231740951538, 'rewards/rejected': 0.015712393447756767, 'rewards/accuracies': 0.6039062738418579, 'rewards/margins': 0.1570107787847519, 'logps/chosen': -180.2021026611328, 'logps/rejected': -177.9386749267578, 'logits/chosen': -1.3118642568588257, 'logits/rejected': -1.2550926208496094, 'epoch': 0.7}\u001b[0m\n",
      "\u001b[34m70%|███████   | 550/782 [22:29<09:17,  2.41s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 551/782 [22:32<09:10,  2.38s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 552/782 [22:34<09:11,  2.40s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 553/782 [22:37<09:10,  2.41s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 554/782 [22:39<09:18,  2.45s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 555/782 [22:42<09:23,  2.48s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 556/782 [22:44<09:10,  2.43s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 557/782 [22:47<09:10,  2.45s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 558/782 [22:49<09:06,  2.44s/it]\u001b[0m\n",
      "\u001b[34m71%|███████▏  | 559/782 [22:51<08:58,  2.41s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 560/782 [22:54<09:03,  2.45s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6515, 'grad_norm': 1.9911978244781494, 'learning_rate': 1.2566547084429325e-05, 'rewards/chosen': 0.17083339393138885, 'rewards/rejected': 0.005813335068523884, 'rewards/accuracies': 0.612500011920929, 'rewards/margins': 0.16502006351947784, 'logps/chosen': -175.26870727539062, 'logps/rejected': -175.18679809570312, 'logits/chosen': -1.2788500785827637, 'logits/rejected': -1.2045022249221802, 'epoch': 0.72}\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 560/782 [22:54<09:03,  2.45s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 561/782 [22:56<08:58,  2.44s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 562/782 [22:59<09:07,  2.49s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 563/782 [23:01<09:00,  2.47s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 564/782 [23:04<08:56,  2.46s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 565/782 [23:06<08:46,  2.42s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 566/782 [23:08<08:43,  2.42s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 567/782 [23:11<08:45,  2.44s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 568/782 [23:13<08:45,  2.46s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 569/782 [23:16<08:54,  2.51s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 570/782 [23:18<08:43,  2.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6567, 'grad_norm': 1.9317783117294312, 'learning_rate': 1.1581002236747329e-05, 'rewards/chosen': 0.1336485594511032, 'rewards/rejected': -0.02524658478796482, 'rewards/accuracies': 0.586718738079071, 'rewards/margins': 0.15889516472816467, 'logps/chosen': -174.77406311035156, 'logps/rejected': -173.37750244140625, 'logits/chosen': -1.2982275485992432, 'logits/rejected': -1.2206813097000122, 'epoch': 0.73}\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 570/782 [23:18<08:43,  2.47s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 571/782 [23:21<08:57,  2.55s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 572/782 [23:24<08:41,  2.48s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 573/782 [23:26<08:41,  2.50s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 574/782 [23:29<08:45,  2.53s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 575/782 [23:31<08:32,  2.48s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 576/782 [23:33<08:28,  2.47s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 577/782 [23:36<08:33,  2.50s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 578/782 [23:38<08:24,  2.47s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 579/782 [23:41<08:11,  2.42s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 580/782 [23:43<08:25,  2.50s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6586, 'grad_norm': 2.013068914413452, 'learning_rate': 1.0623926524601771e-05, 'rewards/chosen': 0.1384669542312622, 'rewards/rejected': -0.010153022594749928, 'rewards/accuracies': 0.5992187261581421, 'rewards/margins': 0.14861997961997986, 'logps/chosen': -178.03173828125, 'logps/rejected': -176.72628784179688, 'logits/chosen': -1.2890945672988892, 'logits/rejected': -1.2333532571792603, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 580/782 [23:43<08:25,  2.50s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 581/782 [23:46<08:19,  2.49s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 582/782 [23:48<08:21,  2.51s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 583/782 [23:51<08:18,  2.51s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 584/782 [23:53<08:12,  2.48s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 585/782 [23:56<08:03,  2.46s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 586/782 [23:58<07:52,  2.41s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 587/782 [24:01<08:02,  2.47s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 588/782 [24:03<07:59,  2.47s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 589/782 [24:06<07:53,  2.45s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 590/782 [24:08<07:56,  2.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.651, 'grad_norm': 2.07871150970459, 'learning_rate': 9.697350436308427e-06, 'rewards/chosen': 0.1611165702342987, 'rewards/rejected': -0.009841259568929672, 'rewards/accuracies': 0.5992187261581421, 'rewards/margins': 0.17095783352851868, 'logps/chosen': -178.49208068847656, 'logps/rejected': -173.5032196044922, 'logits/chosen': -1.2803866863250732, 'logits/rejected': -1.202717900276184, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 590/782 [24:08<07:56,  2.48s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 591/782 [24:11<07:59,  2.51s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 592/782 [24:13<07:52,  2.49s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 593/782 [24:16<07:56,  2.52s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 594/782 [24:18<07:42,  2.46s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 595/782 [24:20<07:38,  2.45s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 596/782 [24:23<07:31,  2.42s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 597/782 [24:25<07:28,  2.42s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 598/782 [24:28<07:31,  2.45s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 599/782 [24:30<07:25,  2.44s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 600/782 [24:32<07:16,  2.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6489, 'grad_norm': 2.132211446762085, 'learning_rate': 8.80323975356783e-06, 'rewards/chosen': 0.19411182403564453, 'rewards/rejected': 0.019719485193490982, 'rewards/accuracies': 0.616406261920929, 'rewards/margins': 0.17439231276512146, 'logps/chosen': -175.07461547851562, 'logps/rejected': -173.33895874023438, 'logits/chosen': -1.2885732650756836, 'logits/rejected': -1.2511249780654907, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 600/782 [24:32<07:16,  2.40s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 601/782 [24:35<07:34,  2.51s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 602/782 [24:38<07:27,  2.49s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 603/782 [24:40<07:28,  2.51s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 604/782 [24:43<07:14,  2.44s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 605/782 [24:45<07:21,  2.49s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 606/782 [24:48<07:25,  2.53s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 607/782 [24:50<07:17,  2.50s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 608/782 [24:53<07:18,  2.52s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 609/782 [24:55<07:20,  2.55s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 610/782 [24:58<07:17,  2.54s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6504, 'grad_norm': 2.0431180000305176, 'learning_rate': 7.943491380952189e-06, 'rewards/chosen': 0.18539288640022278, 'rewards/rejected': 0.018373902887105942, 'rewards/accuracies': 0.6039062738418579, 'rewards/margins': 0.16701899468898773, 'logps/chosen': -178.0685577392578, 'logps/rejected': -173.0338134765625, 'logits/chosen': -1.2803618907928467, 'logits/rejected': -1.2137079238891602, 'epoch': 0.78}\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 610/782 [24:58<07:17,  2.54s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 611/782 [25:00<07:14,  2.54s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 612/782 [25:03<06:55,  2.44s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 613/782 [25:05<06:59,  2.48s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 614/782 [25:08<07:01,  2.51s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 615/782 [25:10<06:57,  2.50s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 616/782 [25:13<06:50,  2.47s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 617/782 [25:15<06:37,  2.41s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 618/782 [25:18<06:42,  2.46s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 619/782 [25:20<06:42,  2.47s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 620/782 [25:23<06:41,  2.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.641, 'grad_norm': 2.018103837966919, 'learning_rate': 7.119929321518876e-06, 'rewards/chosen': 0.19457827508449554, 'rewards/rejected': 0.008721021004021168, 'rewards/accuracies': 0.6265624761581421, 'rewards/margins': 0.18585726618766785, 'logps/chosen': -177.38363647460938, 'logps/rejected': -182.6400909423828, 'logits/chosen': -1.2631146907806396, 'logits/rejected': -1.1772626638412476, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 620/782 [25:23<06:41,  2.48s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 621/782 [25:25<06:31,  2.43s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 622/782 [25:27<06:36,  2.48s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 623/782 [25:30<06:28,  2.44s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 624/782 [25:32<06:27,  2.46s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 625/782 [25:35<06:25,  2.46s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 626/782 [25:37<06:23,  2.46s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 627/782 [25:40<06:21,  2.46s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 628/782 [25:42<06:21,  2.48s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 629/782 [25:44<06:09,  2.42s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 630/782 [25:47<06:15,  2.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6526, 'grad_norm': 1.8929349184036255, 'learning_rate': 6.334300807088509e-06, 'rewards/chosen': 0.17142558097839355, 'rewards/rejected': 0.013900834135711193, 'rewards/accuracies': 0.6171875, 'rewards/margins': 0.15752477943897247, 'logps/chosen': -172.52134704589844, 'logps/rejected': -168.3699951171875, 'logits/chosen': -1.2610597610473633, 'logits/rejected': -1.212826132774353, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[34m81%|████████  | 630/782 [25:47<06:15,  2.47s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 631/782 [25:49<06:02,  2.40s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 632/782 [25:52<06:06,  2.44s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 633/782 [25:54<05:53,  2.37s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 634/782 [25:56<05:54,  2.39s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 635/782 [25:59<05:51,  2.39s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 636/782 [26:01<05:46,  2.37s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 637/782 [26:04<05:55,  2.45s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 638/782 [26:06<05:55,  2.47s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 639/782 [26:09<05:46,  2.42s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 640/782 [26:11<05:36,  2.37s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.648, 'grad_norm': 2.2492165565490723, 'learning_rate': 5.588272591397337e-06, 'rewards/chosen': 0.1760028600692749, 'rewards/rejected': -0.004243074916303158, 'rewards/accuracies': 0.6195312738418579, 'rewards/margins': 0.18024595081806183, 'logps/chosen': -174.42733764648438, 'logps/rejected': -168.462646484375, 'logits/chosen': -1.2692959308624268, 'logits/rejected': -1.2211391925811768, 'epoch': 0.82}\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 640/782 [26:11<05:36,  2.37s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 641/782 [26:14<05:45,  2.45s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 642/782 [26:16<05:44,  2.46s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 643/782 [26:18<05:39,  2.44s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 644/782 [26:21<05:35,  2.43s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 645/782 [26:23<05:37,  2.47s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 646/782 [26:26<05:35,  2.46s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 647/782 [26:28<05:32,  2.47s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 648/782 [26:31<05:27,  2.44s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 649/782 [26:33<05:25,  2.45s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 650/782 [26:36<05:22,  2.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6589, 'grad_norm': 1.9480828046798706, 'learning_rate': 4.883427413988309e-06, 'rewards/chosen': 0.19729086756706238, 'rewards/rejected': 0.037076763808727264, 'rewards/accuracies': 0.614062488079071, 'rewards/margins': 0.16021409630775452, 'logps/chosen': -176.55638122558594, 'logps/rejected': -177.37167358398438, 'logits/chosen': -1.3210759162902832, 'logits/rejected': -1.2442878484725952, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 650/782 [26:36<05:22,  2.44s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 651/782 [26:38<05:25,  2.48s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 652/782 [26:41<05:26,  2.51s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 653/782 [26:43<05:24,  2.52s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 654/782 [26:46<05:19,  2.49s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 655/782 [26:48<05:17,  2.50s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 656/782 [26:51<05:17,  2.52s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 657/782 [26:53<05:14,  2.52s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 658/782 [26:56<05:11,  2.51s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 659/782 [26:58<05:05,  2.48s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 660/782 [27:01<05:02,  2.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.654, 'grad_norm': 2.1864702701568604, 'learning_rate': 4.2212606423427865e-06, 'rewards/chosen': 0.15640142560005188, 'rewards/rejected': -0.0018049937207251787, 'rewards/accuracies': 0.610156238079071, 'rewards/margins': 0.15820643305778503, 'logps/chosen': -174.4680938720703, 'logps/rejected': -176.25790405273438, 'logits/chosen': -1.2582242488861084, 'logits/rejected': -1.193665623664856, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 660/782 [27:01<05:02,  2.48s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 661/782 [27:03<04:52,  2.42s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 662/782 [27:05<04:51,  2.43s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 663/782 [27:08<04:49,  2.43s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 664/782 [27:10<04:48,  2.45s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 665/782 [27:13<04:39,  2.38s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 666/782 [27:15<04:38,  2.40s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 667/782 [27:18<04:39,  2.43s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 668/782 [27:20<04:40,  2.46s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 669/782 [27:23<04:41,  2.49s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 670/782 [27:25<04:34,  2.45s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6464, 'grad_norm': 2.0993876457214355, 'learning_rate': 3.603177099376931e-06, 'rewards/chosen': 0.19596174359321594, 'rewards/rejected': 0.016229847446084023, 'rewards/accuracies': 0.6171875, 'rewards/margins': 0.17973190546035767, 'logps/chosen': -178.17788696289062, 'logps/rejected': -177.82696533203125, 'logits/chosen': -1.2435745000839233, 'logits/rejected': -1.1812663078308105, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 670/782 [27:25<04:34,  2.45s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 671/782 [27:27<04:32,  2.45s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 672/782 [27:30<04:35,  2.50s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 673/782 [27:32<04:26,  2.45s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 674/782 [27:35<04:27,  2.47s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 675/782 [27:37<04:12,  2.36s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 676/782 [27:39<04:09,  2.35s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 677/782 [27:42<04:05,  2.33s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 678/782 [27:44<04:06,  2.37s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 679/782 [27:46<04:03,  2.37s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 680/782 [27:49<04:01,  2.37s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.661, 'grad_norm': 2.206332206726074, 'learning_rate': 3.030488083033273e-06, 'rewards/chosen': 0.19321659207344055, 'rewards/rejected': 0.03785472363233566, 'rewards/accuracies': 0.577343761920929, 'rewards/margins': 0.1553618609905243, 'logps/chosen': -170.0057373046875, 'logps/rejected': -165.68637084960938, 'logits/chosen': -1.2670575380325317, 'logits/rejected': -1.2009403705596924, 'epoch': 0.87}\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 680/782 [27:49<04:01,  2.37s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 681/782 [27:51<03:57,  2.35s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 682/782 [27:54<03:57,  2.38s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 683/782 [27:56<03:54,  2.37s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 684/782 [27:58<03:54,  2.40s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 685/782 [28:01<03:50,  2.37s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 686/782 [28:03<03:53,  2.43s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 687/782 [28:06<03:52,  2.45s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 688/782 [28:08<03:46,  2.41s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 689/782 [28:11<03:48,  2.46s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 690/782 [28:13<03:44,  2.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6729, 'grad_norm': 2.158731460571289, 'learning_rate': 2.5044085842905686e-06, 'rewards/chosen': 0.2188209593296051, 'rewards/rejected': 0.09094975888729095, 'rewards/accuracies': 0.596875011920929, 'rewards/margins': 0.12787123024463654, 'logps/chosen': -174.6243133544922, 'logps/rejected': -176.40499877929688, 'logits/chosen': -1.2896416187286377, 'logits/rejected': -1.2077678442001343, 'epoch': 0.88}\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 690/782 [28:13<03:44,  2.44s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 691/782 [28:16<03:44,  2.47s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 692/782 [28:18<03:41,  2.47s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 693/782 [28:20<03:38,  2.46s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 694/782 [28:23<03:36,  2.46s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 695/782 [28:26<03:40,  2.54s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 696/782 [28:28<03:33,  2.48s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 697/782 [28:30<03:30,  2.48s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 698/782 [28:33<03:29,  2.49s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 699/782 [28:35<03:23,  2.45s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 700/782 [28:38<03:19,  2.44s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.647, 'grad_norm': 1.9419488906860352, 'learning_rate': 2.026054709494235e-06, 'rewards/chosen': 0.2188420295715332, 'rewards/rejected': 0.040114857256412506, 'rewards/accuracies': 0.6117187738418579, 'rewards/margins': 0.1787271797657013, 'logps/chosen': -174.05722045898438, 'logps/rejected': -173.4442596435547, 'logits/chosen': -1.2794543504714966, 'logits/rejected': -1.2128700017929077, 'epoch': 0.9}\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 700/782 [28:38<03:19,  2.44s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 701/782 [28:41<03:26,  2.54s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 702/782 [28:43<03:20,  2.50s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 703/782 [28:46<03:19,  2.53s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 704/782 [28:48<03:16,  2.52s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 705/782 [28:50<03:11,  2.48s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 706/782 [28:53<03:11,  2.52s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 707/782 [28:55<03:07,  2.50s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 708/782 [28:58<03:01,  2.46s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 709/782 [29:00<02:57,  2.44s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 710/782 [29:03<02:53,  2.41s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.653, 'grad_norm': 2.1246931552886963, 'learning_rate': 1.5964413124758494e-06, 'rewards/chosen': 0.20872652530670166, 'rewards/rejected': 0.04245644435286522, 'rewards/accuracies': 0.602343738079071, 'rewards/margins': 0.16627006232738495, 'logps/chosen': -176.0843505859375, 'logps/rejected': -174.3927764892578, 'logits/chosen': -1.2356669902801514, 'logits/rejected': -1.150054693222046, 'epoch': 0.91}\u001b[0m\n",
      "\u001b[34m91%|█████████ | 710/782 [29:03<02:53,  2.41s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 711/782 [29:05<02:50,  2.40s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 712/782 [29:07<02:49,  2.42s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 713/782 [29:10<02:48,  2.44s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 714/782 [29:12<02:46,  2.45s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 715/782 [29:15<02:43,  2.43s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 716/782 [29:17<02:40,  2.43s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 717/782 [29:20<02:41,  2.48s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 718/782 [29:22<02:41,  2.52s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 719/782 [29:25<02:37,  2.51s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 720/782 [29:27<02:32,  2.46s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.639, 'grad_norm': 1.977638602256775, 'learning_rate': 1.2522178440776022e-06, 'rewards/chosen': 0.20826569199562073, 'rewards/rejected': 0.008574898354709148, 'rewards/accuracies': 0.62109375, 'rewards/margins': 0.1996908038854599, 'logps/chosen': -171.8989715576172, 'logps/rejected': -175.142333984375, 'logits/chosen': -1.253984808921814, 'logits/rejected': -1.1967369318008423, 'epoch': 0.92}\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 720/782 [29:27<02:32,  2.46s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 721/782 [29:30<02:33,  2.51s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 722/782 [29:32<02:28,  2.47s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 723/782 [29:35<02:22,  2.41s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 724/782 [29:37<02:19,  2.40s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 725/782 [29:40<02:20,  2.47s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 726/782 [29:42<02:19,  2.49s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 727/782 [29:45<02:16,  2.49s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 728/782 [29:47<02:15,  2.51s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 729/782 [29:50<02:13,  2.51s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 730/782 [29:52<02:11,  2.53s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6561, 'grad_norm': 2.1560370922088623, 'learning_rate': 9.176353682245675e-07, 'rewards/chosen': 0.21859721839427948, 'rewards/rejected': 0.058333612978458405, 'rewards/accuracies': 0.594531238079071, 'rewards/margins': 0.16026361286640167, 'logps/chosen': -181.9980010986328, 'logps/rejected': -180.54849243164062, 'logits/chosen': -1.2909624576568604, 'logits/rejected': -1.2256590127944946, 'epoch': 0.93}\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 730/782 [29:52<02:11,  2.53s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 731/782 [29:55<02:09,  2.53s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 732/782 [29:57<02:04,  2.50s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▎| 733/782 [30:00<02:00,  2.46s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 734/782 [30:02<01:57,  2.44s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 735/782 [30:04<01:53,  2.41s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 736/782 [30:07<01:49,  2.37s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 737/782 [30:09<01:47,  2.40s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 738/782 [30:11<01:42,  2.33s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 739/782 [30:14<01:43,  2.41s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 740/782 [30:16<01:42,  2.45s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6542, 'grad_norm': 2.0833163261413574, 'learning_rate': 6.341449422277046e-07, 'rewards/chosen': 0.20307569205760956, 'rewards/rejected': 0.036784783005714417, 'rewards/accuracies': 0.606249988079071, 'rewards/margins': 0.16629090905189514, 'logps/chosen': -172.89027404785156, 'logps/rejected': -170.65635681152344, 'logits/chosen': -1.2642085552215576, 'logits/rejected': -1.216355562210083, 'epoch': 0.95}\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 740/782 [30:16<01:42,  2.45s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 741/782 [30:19<01:40,  2.44s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 742/782 [30:21<01:37,  2.43s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 743/782 [30:23<01:32,  2.38s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 744/782 [30:26<01:29,  2.36s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 745/782 [30:28<01:25,  2.32s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 746/782 [30:30<01:25,  2.36s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 747/782 [30:33<01:24,  2.42s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 748/782 [30:36<01:23,  2.46s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 749/782 [30:38<01:21,  2.47s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 750/782 [30:41<01:19,  2.48s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6523, 'grad_norm': 2.1519057750701904, 'learning_rate': 4.023480064854174e-07, 'rewards/chosen': 0.1908283680677414, 'rewards/rejected': 0.026674926280975342, 'rewards/accuracies': 0.590624988079071, 'rewards/margins': 0.16415342688560486, 'logps/chosen': -171.82113647460938, 'logps/rejected': -167.16567993164062, 'logits/chosen': -1.3023625612258911, 'logits/rejected': -1.2176971435546875, 'epoch': 0.96}\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 750/782 [30:41<01:19,  2.48s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 751/782 [30:43<01:16,  2.47s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 752/782 [30:45<01:13,  2.46s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 753/782 [30:48<01:12,  2.51s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 754/782 [30:50<01:08,  2.46s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 755/782 [30:53<01:06,  2.46s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 756/782 [30:55<01:04,  2.46s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 757/782 [30:58<01:02,  2.50s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 758/782 [31:00<00:59,  2.46s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 759/782 [31:03<00:56,  2.46s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 760/782 [31:05<00:54,  2.47s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6509, 'grad_norm': 2.152796506881714, 'learning_rate': 2.227363308375363e-07, 'rewards/chosen': 0.19681316614151, 'rewards/rejected': 0.024912750348448753, 'rewards/accuracies': 0.629687488079071, 'rewards/margins': 0.1719004064798355, 'logps/chosen': -180.89498901367188, 'logps/rejected': -177.7013397216797, 'logits/chosen': -1.2909576892852783, 'logits/rejected': -1.2060855627059937, 'epoch': 0.97}\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 760/782 [31:05<00:54,  2.47s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 761/782 [31:08<00:52,  2.51s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 762/782 [31:10<00:49,  2.48s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 763/782 [31:13<00:47,  2.50s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 764/782 [31:15<00:45,  2.52s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 765/782 [31:18<00:41,  2.45s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 766/782 [31:20<00:38,  2.43s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 767/782 [31:23<00:36,  2.45s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 768/782 [31:25<00:34,  2.44s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 769/782 [31:27<00:32,  2.46s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 770/782 [31:30<00:29,  2.43s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6579, 'grad_norm': 2.0694968700408936, 'learning_rate': 9.569097124875193e-08, 'rewards/chosen': 0.2024345099925995, 'rewards/rejected': 0.04936673492193222, 'rewards/accuracies': 0.5757812261581421, 'rewards/margins': 0.15306779742240906, 'logps/chosen': -172.5419464111328, 'logps/rejected': -172.09564208984375, 'logits/chosen': -1.3105146884918213, 'logits/rejected': -1.247514247894287, 'epoch': 0.99}\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 770/782 [31:30<00:29,  2.43s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 771/782 [31:32<00:27,  2.46s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 772/782 [31:35<00:24,  2.46s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 773/782 [31:37<00:22,  2.45s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 774/782 [31:40<00:19,  2.45s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 775/782 [31:42<00:17,  2.44s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 776/782 [31:45<00:14,  2.45s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 777/782 [31:47<00:12,  2.46s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 778/782 [31:50<00:10,  2.53s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 779/782 [31:52<00:07,  2.51s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 780/782 [31:54<00:04,  2.40s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6463, 'grad_norm': 2.2015726566314697, 'learning_rate': 2.1481461377634294e-08, 'rewards/chosen': 0.20715299248695374, 'rewards/rejected': 0.02001214399933815, 'rewards/accuracies': 0.6117187738418579, 'rewards/margins': 0.1871408075094223, 'logps/chosen': -176.29617309570312, 'logps/rejected': -176.86215209960938, 'logits/chosen': -1.3053441047668457, 'logits/rejected': -1.2251774072647095, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 780/782 [31:54<00:04,  2.40s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 781/782 [31:57<00:02,  2.45s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 782/782 [31:58<00:00,  1.98s/it]\u001b[0m\n",
      "\u001b[34m{'train_runtime': 1918.763, 'train_samples_per_second': 52.117, 'train_steps_per_second': 0.408, 'train_loss': 0.6638312012033389, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 782/782 [31:58<00:00,  1.98s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 782/782 [31:58<00:00,  2.45s/it]\u001b[0m\n",
      "\u001b[34m2025-09-09 18:30:34 - __main__ - INFO - Saving trained model...\u001b[0m\n",
      "\u001b[34m2025-09-09 18:30:34 - __main__ - INFO - Model saved to /opt/ml/model\u001b[0m\n",
      "\u001b[34m2025-09-09 18:30:34 - __main__ - INFO - ==================================================\u001b[0m\n",
      "\u001b[34m2025-09-09 18:30:34 - __main__ - INFO - Training completed successfully!\u001b[0m\n",
      "\u001b[34m2025-09-09 18:30:34 - __main__ - INFO - Model saved to: /opt/ml/model\u001b[0m\n",
      "\u001b[34m2025-09-09 18:30:34 - __main__ - INFO - ==================================================\u001b[0m\n",
      "\u001b[34m2025-09-09 18:30:35 - __main__ - INFO - Cleaned up distributed training on rank 1\u001b[0m\n",
      "\u001b[34m2025-09-09 18:30:35 - __main__ - INFO - Cleaned up distributed training on rank 2\u001b[0m\n",
      "\u001b[34m2025-09-09 18:30:35 - __main__ - INFO - Cleaned up distributed training on rank 3\u001b[0m\n",
      "\u001b[34m2025-09-09 18:30:35 - __main__ - INFO - Cleaned up distributed training on rank 0\u001b[0m\n",
      "\u001b[35m2025-09-09 18:30:35 - __main__ - INFO - Cleaned up distributed training on rank 5\u001b[0m\n",
      "\u001b[35m2025-09-09 18:30:35 - __main__ - INFO - Cleaned up distributed training on rank 6\u001b[0m\n",
      "\u001b[35m2025-09-09 18:30:35 - __main__ - INFO - Cleaned up distributed training on rank 7\u001b[0m\n",
      "\u001b[35m2025-09-09 18:30:35 - __main__ - INFO - Cleaned up distributed training on rank 4\u001b[0m\n",
      "\u001b[35m2025-09-09 18:30:37,810 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[35m2025-09-09 18:30:37,810 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[35m2025-09-09 18:30:37,810 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-09-09 18:30:41 Uploading - Uploading generated training model\u001b[34m2025-09-09 18:30:37,431 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-09-09 18:30:37,431 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-09-09 18:30:37,431 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2025-09-09 18:30:56 Completed - Resource retained for reuse\n",
      "Training seconds: 4382\n",
      "Billable seconds: 4382\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "from huggingface_hub import HfFolder\n",
    "import time\n",
    "\n",
    "# Define Training Job Name with timestamp for uniqueness\n",
    "job_name = f'qwen-dpo-training-{int(time.time())}'\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='dpo_train.py',              # train script\n",
    "    source_dir='code',                       # directory with all training files\n",
    "    instance_type='ml.g5.12xlarge',          # GPU instance\n",
    "    instance_count=1,                        # instance count\n",
    "    base_job_name=job_name,                  # base name for training job\n",
    "    role=role,                               # IAM role for AWS resources\n",
    "    volume_size=30,                         # EBS volume size in GB\n",
    "    transformers_version='4.49.0',           # transformers version\n",
    "    pytorch_version='2.5.1',                 # pytorch version\n",
    "    py_version='py311',                      # python version\n",
    "    disable_output_compression=True,         # faster training completion\n",
    "    output_path=\"s3://mm-fsi-fix/dpo/\",      # S3 location where training output is stored\n",
    "\n",
    "    # Environment variables\n",
    "    environment={\n",
    "        \"HUGGINGFACE_HUB_CACHE\": \"/tmp/.cache\",  # cache location\n",
    "        \"HF_TOKEN\": HfFolder.get_token(),        # Hf Token ID\n",
    "        \"TOKENIZERS_PARALLELISM\": \"false\",        # Prevent tokenizer warnings\n",
    "        \"PIP_CACHE_DIR\": \"/opt/ml/sagemaker/warmpoolcache/pip\"  # Persisitent dir across warm pool runs used to cache pip installs\n",
    "    },\n",
    "    distribution={\n",
    "        \"torch_distributed\": {  # this uses torchrun command on your script with the following arguments\n",
    "            \"enabled\": True,\n",
    "        }\n",
    "    },\n",
    "    keep_alive_period_in_seconds=1800,       # 30 minutes warm pool, maximum of 60 mins\n",
    ")\n",
    "\n",
    "# Start training\n",
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23217f2f-b506-4939-bdd7-b17db99e6ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
